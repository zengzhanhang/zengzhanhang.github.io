<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Human Pose Estimation Literature Review - 小树的面包店 | Zhanhang Zeng&#039;s Blog</title><meta description="【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。   2D Human Pose EstimationWhat is Human Pose Estimation? Human P"><meta property="og:type" content="blog"><meta property="og:title" content="Human Pose Estimation Literature Review"><meta property="og:url" content="https://zengzhanhang.github.io/2019/12/31/Human-Pose-Estimation-Review/"><meta property="og:site_name" content="小树的面包店 | Zhanhang Zeng&#039;s Blog"><meta property="og:description" content="【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。   2D Human Pose EstimationWhat is Human Pose Estimation? Human P"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/HumanPoseEstimation/pose-example.PNG"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/HumanPoseEstimation/DensePose-example.PNG"><meta property="article:published_time" content="2019-12-31T07:36:02.000Z"><meta property="article:modified_time" content="2020-05-17T09:22:01.994Z"><meta property="article:author" content="Zhanhang (展航) ZENG"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="CV"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/Documents/ReadingNote/HumanPoseEstimation/pose-example.PNG"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zengzhanhang.github.io/2019/12/31/Human-Pose-Estimation-Review/"},"headline":"小树的面包店 | Zhanhang Zeng's Blog","image":[],"datePublished":"2019-12-31T07:36:02.000Z","dateModified":"2020-05-17T09:22:01.994Z","author":{"@type":"Person","name":"Zhanhang (展航) ZENG"},"description":"【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。   2D Human Pose EstimationWhat is Human Pose Estimation? Human P"}</script><link rel="canonical" href="https://zengzhanhang.github.io/2019/12/31/Human-Pose-Estimation-Review/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/img/logo1.png" alt="小树的面包店 | Zhanhang Zeng&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/logo1.png" alt="小树的面包店 | Zhanhang Zeng&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/notes_TOC">Notes - 数据科学知识手册</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile">Human Pose Estimation Literature Review</h1><div class="article-meta size-small is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time class="level-item" dateTime="2019-12-31T07:36:02.000Z" title="2019-12-31T07:36:02.000Z">2019-12-31</time></span><span class="level-item"><i class="far fa-calendar-check"> </i><time class="level-item" dateTime="2020-05-17T09:22:01.994Z" title="2020-05-17T09:22:01.994Z">Edit: 2020-05-17</time></span><span class="level-item"><i class="fas fa-user"> </i>Zhanhang (展航) ZENG</span><span class="level-item"><i class="far fa-clock"></i> 8 minutes read (About 1128 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span> visits</span></div><div class="level-left is-uppercase mt-2"><span class="level-item"><i class="fas fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/Computer-Vision/">Computer Vision</a><span> / </span><a class="link-muted" href="/categories/Computer-Vision/Pose-Estimation/">Pose Estimation</a></span><span class="level-item"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/DeepLearning/">DeepLearning</a><a class="link-muted mr-2" rel="tag" href="/tags/CV/">CV</a></span></div></div><div class="content"><blockquote>
<p>【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。</p>
</blockquote>
<!-- > Declaration：写这个 blog 是因为目前在做 pose estimation 领域的 literature review，并且读到了两篇觉得不错的 blog，分别是：[A 2019 guide to Human Pose Estimation with Deep Learning](https://nanonets.com/blog/human-pose-estimation-2d-guide/) 和 [A 2019 guide to 3D Human Pose Estimation](https://nanonets.com/blog/human-pose-estimation-3d-guide/)。 -->
<h1 id="2D-Human-Pose-Estimation"><a href="#2D-Human-Pose-Estimation" class="headerlink" title="2D Human Pose Estimation"></a>2D Human Pose Estimation</h1><h2 id="What-is-Human-Pose-Estimation"><a href="#What-is-Human-Pose-Estimation" class="headerlink" title="What is Human Pose Estimation?"></a>What is Human Pose Estimation?</h2><ul>
<li><strong>Human Pose Estimation</strong> 主要是在图像或视频中检测估计人体的一些关键点（例如，关节，五官等）的问题。它也可以被定义成在所有关节姿势的空间中搜索特定姿势的问题。<!-- is defined as the problem of localization of human joints (also known as keypoints - elbows, wrists, etc) in images or videos. It is also defined as the search for a specific pose in space of all articulated poses. --></li>
</ul>
<!-- ![](/Documents/ReadingNote/HumanPoseEstimation/pose-example.png) -->
<ul>
<li><strong>2D Pose Estimation</strong> - 从图像中估计2D姿态（关键点）坐标，即 2D pose (x,y) coordinates。</li>
<li><strong>3D Pose Estimation</strong> - 估计出关键点的3D坐标，即Estimate a 3D pose (x,y,z) coordinates a RGB image.</li>
</ul>
<!-- ![Photograph taken from Pexels](/Documents/ReadingNote/HumanPoseEstimation/Screen-Shot-2019-04-11-at-5.17.56-PM.png) -->
<p><img src="/Documents/ReadingNote/HumanPoseEstimation/pose-example.PNG" alt="2D (keypoints input) to 3D pose estimation example"></p>
<ul>
<li><strong>3D mesh (shap) estimation</strong> - 从图像中估计object的shape，例如数据集<a href="http://densepose.org/">DensePose</a>。</li>
</ul>
<p><img src="/Documents/ReadingNote/HumanPoseEstimation/DensePose-example.PNG" alt="DensePose Results: 3D shape estimation"></p>
<a id="more"></a>
<ul>
<li><p><strong>应用</strong> - Human Pose Estimation 有很多应用，主要被用于 Action recognition, Animation, Gaming, Gait recognition 等等。具体的应用场景集中在在智能视频监控，病人监护系统，人机交互，虚拟现实，人体动画，智能家居，智能安防，运动员辅助训练等等。</p>
<ul>
<li>例如：<a href="https://www.homecourt.ai/">HomeCourt</a> 使用 Pose Estimation 去分析篮球运动员的运动。</li>
</ul>
</li>
<li><p><strong>Why is it hard?</strong> - 由于人体具有相当的柔性，会出现各种姿态和形状，人体任何一个部位的微小变化都会产生一种新的姿态，同时其关键点的可见性受穿着、姿态、视角等影响非常大，而且还面临着遮挡、光照、雾等环境的影响，除此之外，2D人体关键点和3D人体关键点在视觉上会有明显的差异，身体不同部位都会有视觉上缩短的效果（foreshortening），使得人体骨骼关键点检测成为计算机视觉领域中一个极具挑战性的课题。</p>
</li>
</ul>
<h2 id="Relevant-Datasets"><a href="#Relevant-Datasets" class="headerlink" title="Relevant Datasets"></a>Relevant Datasets</h2><h3 id="2D-Datasets"><a href="#2D-Datasets" class="headerlink" title="2D Datasets"></a>2D Datasets</h3><ul>
<li><strong>LSP（Leeds Sports Pose Dataset）</strong> - 单人人体关键点检测数据集，关键点个数为14，样本数2K，在目前的研究中基本上被弃用；[<a href="https://sam.johnson.io/research/lsp.html">url</a>]</li>
<li><strong>FLIC（Frames Labeled In Cinema）</strong> - 单人人体关键点检测数据集，关键点个数为9，样本数2W，在目前的研究中基本上被弃用；[<a href="https://bensapp.github.io/flic-dataset.html">url</a>]</li>
<li><strong>MPII Human Pose dataset</strong> - 单人/多人人体关键点检测数据集，关键点个数为16，样本数25K，40K People，410 human activities，全身。 [<a href="http://human-pose.mpi-inf.mpg.de/#overview">url</a>]</li>
<li><strong>MSCOCO</strong> - 多人，全身数据集。关键点个数为17，样本数多于300K，目前的相关研究基本上还需要在该数据集上进行验证；[<a href="http://cocodataset.org/#home">url</a>]</li>
<li><strong>AI Challenger</strong> - 多人，全身数据集。关键点：17，样本数约380K（210K Training, 30K Validation, 30K Testing）.[<a href="https://challenger.ai/competition/keypoint/subject">url</a>]</li>
<li><strong>PoseTrack</strong>[<a href="https://posetrack.net/">url</a>] - 多人，全身视频数据集。关键点：15，主要用于多人姿态估计和<strong>姿态追（Multi-person Pose Tracking）</strong>,数据集:<ul>
<li>$&gt;$ 1356 video sequences，</li>
<li>$&gt;$ 46K annotated video frames</li>
<li>$&gt;$ 276K body pose annotations</li>
</ul>
</li>
<li><strong>VGG Human Pose Estimation datasets</strong> -  单人，上半身视频数据集。[<a href="https://www.robots.ox.ac.uk/~vgg/data/pose/">url</a>]</li>
</ul>
<h3 id="3D-Datasets"><a href="#3D-Datasets" class="headerlink" title="3D Datasets"></a>3D Datasets</h3><ul>
<li><strong>Human 3.6M</strong> - 室内场景，采集人数11人，3D human shape 数据集，3.6 M 图像。[<a href="http://vision.imar.ro/human3.6m/description.php">url</a>]<ul>
<li>Download: <a href="https://drive.google.com/drive/folders/1kgVH-GugrLoc9XyvP6nRoaFpw3TmM5xK?usp=sharing">Google Drive</a></li>
<li>Related Repo: <a href="https://github.com/mks0601/3DMPPE_POSENET_RELEASE">https://github.com/mks0601/3DMPPE_POSENET_RELEASE</a></li>
</ul>
</li>
<li><strong>DensePose</strong> - 用于3D shape研究的数据集。[<a href="http://densepose.org/">url</a>]；长线来讲，3D shape 估计是非常有价值的研究方向。</li>
</ul>
<h2 id="Why-is-it-hard"><a href="#Why-is-it-hard" class="headerlink" title="Why is it hard?"></a>Why is it hard?</h2><p>Strong articulations, small and barely visible joints, occlusions, clothing, and lighting changes make this a difficult problem.</p>
</div><ul class="post-copyright"><li><strong>Title: </strong><a href="/2019/12/31/Human-Pose-Estimation-Review/">Human Pose Estimation Literature Review</a></li><li><strong>Author: </strong><a href="/">Zhanhang (展航) ZENG</a></li><li><strong>Link: </strong><a href="/2019/12/31/Human-Pose-Estimation-Review/">https://zengzhanhang.github.io/2019/12/31/Human-Pose-Estimation-Review/</a></li><li><strong>Released Date: </strong>2019-12-31</li><li><strong>Last update: </strong>2020-05-17</li><li><strong>Statement: </strong>All articles in this blog, unless otherwise stated, are based on the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> license.</li></ul><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex mb-4"><div class="level-start"><div class="article-tags size-small is-uppercase"><span class="mr-2"><i class="fas fa-tags has-text-grey"></i> #</span><a class="link-muted mr-2" rel="tag" href="/tags/DeepLearning/">DeepLearning</a><a class="link-muted mr-2" rel="tag" href="/tags/CV/">CV</a></div></div><div class="level-start"><div style="text-align:center"></div></div></div><div style="text-align:center"><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5ebfed406b62a000122baf21&amp;product=inline-share-buttons" defer></script></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/QRcode/alipay.jpg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/QRcode/WeChatPay.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/01/14/docker_usage_cheat_sheet/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Using Python with Docker</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/07/21/densenet/"><span class="level-item">DenseNet — Dense卷积网络（图像分类）</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '5068a70a3ddc9610095a9362a47c8a66',
            repo: 'zengzhanhang.github.io',
            owner: 'zengzhanhang',
            clientID: '13624a11c95a2d5eed31',
            clientSecret: '67d97a17e7f385cf24a8a24e3bbfaa1e8fcc20c4',
            admin: ["zengzhanhang"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/img/icon(square)-min.png" alt="Zhanhang (Kyle) ZENG"></figure><p class="title is-size-4 is-block line-height-inherit">Zhanhang (Kyle) ZENG</p><p class="is-size-6 is-block">Statistics, Machine Learning &amp; Ai</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Edinburgh, Scotland</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">17</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded follow-button" href="https://github.com/zengzhanhang" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/zhanhang-zeng-801a67185/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/zengzhanhang"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/cengzhanhang"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://www.weibo.com/zengzhanhang"><i class="fab fa-weibo"></i></a></div></div></div><div class="card widget is-sticky" id="toc"><div class="card-content"><div class="menu catalogue-other-setting"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#2D-Human-Pose-Estimation"><span class="mr-2">1</span><span>2D Human Pose Estimation</span></a><ul class="menu-list"><li><a class="is-flex" href="#What-is-Human-Pose-Estimation"><span class="mr-2">1.1</span><span>What is Human Pose Estimation?</span></a></li><li><a class="is-flex" href="#Relevant-Datasets"><span class="mr-2">1.2</span><span>Relevant Datasets</span></a><ul class="menu-list"><li><a class="is-flex" href="#2D-Datasets"><span class="mr-2">1.2.1</span><span>2D Datasets</span></a></li><li><a class="is-flex" href="#3D-Datasets"><span class="mr-2">1.2.2</span><span>3D Datasets</span></a></li></ul></li><li><a class="is-flex" href="#Why-is-it-hard"><span class="mr-2">1.3</span><span>Why is it hard?</span></a></li></ul></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/img/logo1.png" alt="小树的面包店 | Zhanhang Zeng&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/logo1.png" alt="小树的面包店 | Zhanhang Zeng&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2020 Zhanhang (展航) ZENG</span>  All Rights Reserved<br>Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span>    <span id="busuanzi_container_site_pv">Totally, <span id="busuanzi_value_site_pv">0</span> page views</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://zengzhanhang.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/imaegoo/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>