<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>DenseNet — Dense卷积网络（图像分类） - 小树的面包店 | ZhanhangZeng&#039;s Blog</title><meta description="【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。 作者 | Sik-Ho Tsang (原文：Review: DenseNet — Dense Convolutional Network (Image Classification))编译 | Xiaowen"><meta property="og:type" content="blog"><meta property="og:title" content="DenseNet — Dense卷积网络（图像分类）"><meta property="og:url" content="https://zengzhanhang.github.io/2019/07/21/densenet/"><meta property="og:site_name" content="小树的面包店 | ZhanhangZeng&#039;s Blog"><meta property="og:description" content="【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。 作者 | Sik-Ho Tsang (原文：Review: DenseNet — Dense Convolutional Network (Image Classification))编译 | Xiaowen"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/StandardCNN.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/ResNet.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/k_growthDenseBlock.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet.gif"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/compositionlayer.gif"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet_b.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/multipleDenseBlock.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DeepSupervision.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/parameterefficient.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Features.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CNN.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Dense.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CIFAR10_results.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Results.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CIFAR100_results.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Details_results.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/imagenetresults.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/results2.png"><meta property="og:image" content="https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/results3.png"><meta property="article:published_time" content="2019-07-21T06:57:55.000Z"><meta property="article:modified_time" content="2020-05-16T13:21:28.873Z"><meta property="article:author" content="Zhanhang ZENG (展航)"><meta property="article:tag" content="CV"><meta property="article:tag" content="Image Classification"><meta property="article:tag" content="Reading Note"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/Documents/ReadingNote/DenseNet/StandardCNN.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zengzhanhang.github.io/2019/07/21/densenet/"},"headline":"小树的面包店 | ZhanhangZeng's Blog","image":["https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/StandardCNN.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/ResNet.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/k_growthDenseBlock.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet.gif","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/compositionlayer.gif","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DenseNet_b.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/multipleDenseBlock.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/DeepSupervision.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/parameterefficient.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Features.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CNN.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Dense.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CIFAR10_results.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Results.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/CIFAR100_results.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/Details_results.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/imagenetresults.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/results2.png","https://zengzhanhang.github.io/Documents/ReadingNote/DenseNet/results3.png"],"datePublished":"2019-07-21T06:57:55.000Z","dateModified":"2020-05-16T13:21:28.873Z","author":{"@type":"Person","name":"Zhanhang ZENG (展航)"},"description":"【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。 作者 | Sik-Ho Tsang (原文：Review: DenseNet — Dense Convolutional Network (Image Classification))编译 | Xiaowen"}</script><link rel="canonical" href="https://zengzhanhang.github.io/2019/07/21/densenet/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo1.png" alt="小树的面包店 | ZhanhangZeng&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/notes_TOC">Notes - 数据科学知识手册</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile">DenseNet — Dense卷积网络（图像分类）</h1><div class="article-meta size-small is-mobile"><div class="level-left"><i class="far fa-calendar-alt"> Post: </i><time class="level-item" dateTime="2019-07-21T06:57:55.000Z" title="2019-07-21T06:57:55.000Z">2019-07-21</time><i class="far fa-calendar-check"> Edit: </i><time class="level-item" dateTime="2020-05-16T13:21:28.873Z" title="2020-05-16T13:21:28.873Z">Edit: 2020-05-16</time><span class="level-item"><i class="far fa-clock"></i> 12 minutes read (About 1799 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span> visits</span></div><div class="level-left is-uppercase mt-2"><span class="level-item"><i class="fas fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/Computer-Vision/">Computer Vision</a><span> / </span><a class="link-muted" href="/categories/Computer-Vision/Networks-Architecture/">Networks Architecture</a></span><span class="level-item"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/CV/">CV</a><a class="link-muted mr-2" rel="tag" href="/tags/Image-Classification/">Image Classification</a><a class="link-muted mr-2" rel="tag" href="/tags/Reading-Note/">Reading Note</a></span></div></div><div class="content"><blockquote>
<p>【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。</p>
<p>作者 | Sik-Ho Tsang (原文：<a href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803">Review: DenseNet — Dense Convolutional Network (Image Classification)</a>)<br>编译 | Xiaowen</p>
</blockquote>
<a id="more"></a>
<p>该译文转载自：<a href="https://mp.weixin.qq.com/s/F2GRDen0v2zbnHlB-xHioA">https://mp.weixin.qq.com/s/F2GRDen0v2zbnHlB-xHioA</a></p>
<hr>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol>
<li><a href="#1-Dense-Block">Dense Block</a></li>
<li><a href="#2-DenseNet-结构">DenseNet 结构</a></li>
<li><a href="#3-DenseNet-的优势">DenseNet 的优势</a></li>
<li><a href="#4-CIFAR-amp-SVHN-小规模数据集结果">CIFAR &amp; SVHN 小规模数据集结果</a></li>
<li><a href="#5-ImageNet-大规模数据集结果">ImageNet 大规模数据集结果</a></li>
<li><a href="#6-特征复用的进一步分析">特征复用的进一步分析</a></li>
</ol>
<hr>
<h1 id="1-Dense-Block"><a href="#1-Dense-Block" class="headerlink" title="1. Dense Block"></a>1. Dense Block</h1><p><img src="/Documents/ReadingNote/DenseNet/StandardCNN.png" alt="Standard ConvNet Concept"></p>
<p>在 <strong>andard ConvNet</strong> 中，输入图像经过多次卷积，得到高层次特征。</p>
<p><img src="/Documents/ReadingNote/DenseNet/ResNet.png" alt="ResNet Concept"></p>
<p>在 <strong>ResNet</strong> 中，提出了恒等映射（ <strong>identity mapping</strong> ）来促进梯度传播，同时使用使用 <strong>Element-wise addition</strong> 。它可以看作是将状态从一个 ResNet 模块传递到另一个 ResNet 模块的算法。 (It can be viewed as algorithms with a state passed from one ResNet module to another one.)</p>
<p><img src="/Documents/ReadingNote/DenseNet/DenseNet.png" alt="One Dense Block in DenseNet"></p>
<p>在 <strong>DenseNet</strong> 中，每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联(<strong>Concatenation</strong>)方式，每一层都在接受来自前几层的“集体知识（collective knowledge）”。(Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.)</p>
<p><img src="/Documents/ReadingNote/DenseNet/k_growthDenseBlock.png" alt="Dense Block in DenseNet with Growth Rate k"></p>
<p>由于每个层从前面的所有层接收特征映射，所以网络可以更薄、更紧凑，即信道数可以更少。增长速率k是每个层的附加信道数。</p>
<p>因此，它具有较高的计算效率和存储效率。下图显示了前向传播中级联的概念：</p>
<p><img src="/Documents/ReadingNote/DenseNet/DenseNet.gif" alt="Concatenation during Forward Propagation"></p>
<hr>
<h1 id="2-DenseNet-结构"><a href="#2-DenseNet-结构" class="headerlink" title="2. DenseNet 结构"></a>2. DenseNet 结构</h1><h2 id="2-1-基础-DenseNet-组成层"><a href="#2-1-基础-DenseNet-组成层" class="headerlink" title="2.1 基础 DenseNet 组成层"></a>2.1 基础 DenseNet 组成层</h2><p><img src="/Documents/ReadingNote/DenseNet/compositionlayer.gif" alt="Composition Layer"></p>
<p>对于每个组成层使用 <strong>Pre-Activation Batch Norm (BN)</strong> 和 <strong>ReLU</strong>，然后用k通道的输出特征映射进行 $\boldsymbol{3 \times 3}$ <strong>卷积</strong>，例如，将$x_0$、$x_1$、$x_2$、$x_3$ 转换为 $x_4$。这是     <strong><em>Pre-Activation ResNet</em></strong> 的想法。</p>
<h2 id="2-2-DenseNet-B-Bottleneck-层"><a href="#2-2-DenseNet-B-Bottleneck-层" class="headerlink" title="2.2 DenseNet-B (Bottleneck 层)"></a>2.2 DenseNet-B (Bottleneck 层)</h2><p><img src="/Documents/ReadingNote/DenseNet/DenseNet_b.png" alt="DenseNet-B"></p>
<p>为了降低模型的复杂度和规模，在 <strong>BN-ReLU-3×3 conv</strong> 之前进行了 <strong>BN-ReLU-1×1 conv</strong> .</p>
<h2 id="2-3-具有转换层（transition-layer）的多Dense块"><a href="#2-3-具有转换层（transition-layer）的多Dense块" class="headerlink" title="2.3 具有转换层（transition layer）的多Dense块"></a>2.3 具有转换层（transition layer）的多Dense块</h2><p><img src="/Documents/ReadingNote/DenseNet/multipleDenseBlock.png" alt="Multiple Dense Blocks"></p>
<ul>
<li><p>采用1×1 Conv和2×2平均池化作为相邻 dense block 之间的转换层。</p>
</li>
<li><p>特征映射大小在 dense block 中是相同的，因此它们可以很容易地连接在一起。</p>
</li>
<li><p>在最后一个 dense block 的末尾，执行一个全局平均池化，然后附加一个 Softmax 分类器。</p>
</li>
</ul>
<h2 id="2-4-DenseNet-BC-进一步压缩"><a href="#2-4-DenseNet-BC-进一步压缩" class="headerlink" title="2.4 DenseNet-BC (进一步压缩)"></a>2.4 DenseNet-BC (进一步压缩)</h2><ul>
<li><p><strong>如果 Dense Block 包含 $m$ 个特征映射，则转换层（transition layer）生成 $\theta_m$ 输出特征映射</strong>，其中 $0 &lt; \theta \leq 1$ 称为压缩因子。</p>
</li>
<li><p>当 $\theta = 1$ 时，跨转换层的特征映射数保持不变。$\boldsymbol{\theta &lt; 1}$ 的 DenseNet 称为 <strong>DenseNet-C</strong>，在实验中采用 $\theta = 0.5$ 。</p>
</li>
<li><p>当同时使用 <strong>bottleneck</strong> 和 $\boldsymbol{\theta &lt; 1}$ 时的转换层时，该模型称为 <strong>DenseNet-BC</strong> 模型。</p>
</li>
<li><p>最后，训练 <strong>with/without B/C</strong> 和不同 <strong>L 层</strong> 和 <strong>k growth rate</strong> 的 DenseNet。</p>
</li>
</ul>
<hr>
<h1 id="3-DenseNet-的优势"><a href="#3-DenseNet-的优势" class="headerlink" title="3. DenseNet 的优势"></a>3. DenseNet 的优势</h1><h2 id="3-1-Strong-Gradient-Flow"><a href="#3-1-Strong-Gradient-Flow" class="headerlink" title="3.1 Strong Gradient Flow"></a>3.1 Strong Gradient Flow</h2><p><img src="/Documents/ReadingNote/DenseNet/DeepSupervision.png" alt="Implicit “Deep Supervision”"></p>
<p>误差信号可以更直接地传播到早期的层中。这是一种隐含的深度监督，因为早期的层可以从最终的分类层直接获得监督。</p>
<h2 id="3-2-Parameter-amp-Computational-Efficiency"><a href="#3-2-Parameter-amp-Computational-Efficiency" class="headerlink" title="3.2 Parameter &amp; Computational Efficiency"></a>3.2 Parameter &amp; Computational Efficiency</h2><p><img src="/Documents/ReadingNote/DenseNet/parameterefficient.png" alt="Number of Parameters for ResNet and DenseNet"></p>
<p>对于每个层，RetNet 中的参数与c×c成正比，而 DenseNet 中的参数与1×k×k成正比。</p>
<p>由于 k &lt;&lt; C, 所以 DenseNet 比 ResNet 的size更小。</p>
<h2 id="3-3-More-Diversified-Features"><a href="#3-3-More-Diversified-Features" class="headerlink" title="3.3 More Diversified Features"></a>3.3 More Diversified Features</h2><p><img src="/Documents/ReadingNote/DenseNet/Features.png" alt="More Diversified Features in DenseNet"></p>
<p>由于 DenseNet 中的每一层都接收前面的所有层作为输入，因此特征更加多样化，并且倾向于有更丰富的模式。</p>
<h2 id="3-4-Maintains-Low-Complexity-Features"><a href="#3-4-Maintains-Low-Complexity-Features" class="headerlink" title="3.4 Maintains Low Complexity Features"></a>3.4 Maintains Low Complexity Features</h2><p><img src="/Documents/ReadingNote/DenseNet/CNN.png" alt="Standard ConvNet"></p>
<p>在标准ConvNet中，分类器使用最复杂的特征。</p>
<p><img src="/Documents/ReadingNote/DenseNet/Dense.png" alt="DenseNet"></p>
<p>在 DenseNet 中，分类器使用所有复杂级别的特征。它倾向于给出更平滑的决策边界。它还解释了为什么 DenseNet 在训练数据不足时表现良好。</p>
<hr>
<h1 id="4-CIFAR-amp-SVHN-小规模数据集结果"><a href="#4-CIFAR-amp-SVHN-小规模数据集结果" class="headerlink" title="4. CIFAR &amp; SVHN 小规模数据集结果"></a>4. CIFAR &amp; SVHN 小规模数据集结果</h1><h2 id="4-1-CIFAR-10"><a href="#4-1-CIFAR-10" class="headerlink" title="4.1 CIFAR-10"></a>4.1 CIFAR-10</h2><p><img src="/Documents/ReadingNote/DenseNet/CIFAR10_results.png" alt="CIFAR-10 Results"></p>
<p>Pre-Activation ResNet is used in detailed comparison.</p>
<ul>
<li><p>数据增强（C10+），测试误差：</p>
<ol>
<li>Small-size ResNet-110: 6.41%</li>
<li>Large-size ResNet-1001 (10.2M parameters): 4.62%</li>
<li>State-of-the-art (SOTA) 4.2%</li>
<li>Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 4.5%</li>
<li>Large-size DenseNet (L=250, k=24): 3.6%</li>
</ol>
</li>
<li><p>无数据增强（C10），测试误差：</p>
<ol>
<li>Small-size ResNet-110: 11.26%</li>
<li>Large-size ResNet-1001 (10.2M parameters): 10.56%</li>
<li>State-of-the-art (SOTA) 7.3%</li>
<li>Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 5.9%</li>
<li>Large-size DenseNet (L=250, k=24): 4.2%</li>
</ol>
</li>
</ul>
<p>在 <a href="https://towardsdatascience.com/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">Pre-Activation ResNet</a> 中出现严重的过拟合，而 DenseNet 在训练数据不足时表现良好，因为DenseNet 使用了复杂的特征。</p>
<p><img src="/Documents/ReadingNote/DenseNet/Results.png" alt="C10+: Different DenseNet Variants (Left), DenseNet vs ResNet (Middle), Training and Testing Curves of DenseNet and ResNet (Right)"></p>
<ul>
<li>左：DenseNet-BC 获得最佳效果。</li>
<li>中：Pre-Activation ResNet 已经比 alexnet 和 vggnet 获得更少的参数，DenseNet-BC(k=12)的参数比 Pre-Activation ResNet 少3×10，测试误差相同。</li>
<li>右：与 Pre-Activation ResNet-1001有10.2m 参数相比，0.8 参数的 DenseNet-BC-100 具有相似的测试误差。</li>
</ul>
<h2 id="4-2-CIFAR-100"><a href="#4-2-CIFAR-100" class="headerlink" title="4.2 CIFAR-100"></a>4.2 CIFAR-100</h2><p>CIFAR-100类似的趋势如下：</p>
<p><img src="/Documents/ReadingNote/DenseNet/CIFAR100_results.png" alt="CIFAR-100 Results"></p>
<h2 id="4-3-Detailed-Results"><a href="#4-3-Detailed-Results" class="headerlink" title="4.3 Detailed Results"></a>4.3 Detailed Results</h2><p><img src="/Documents/ReadingNote/DenseNet/Details_results.png" alt="Detailed Results, + means data augmentation"></p>
<p>SVHN 是街景房屋编号的数据集。蓝色代表最好的效果。DenseNet-BC 不能得到比基本 DenseNet 更好的结果，作者认为 SVHN 是一项相对容易的任务，非常深的模型可能会过拟合。</p>
<hr>
<h1 id="5-ImageNet-大规模数据集结果"><a href="#5-ImageNet-大规模数据集结果" class="headerlink" title="5. ImageNet 大规模数据集结果"></a>5. ImageNet 大规模数据集结果</h1><p><img src="/Documents/ReadingNote/DenseNet/imagenetresults.png" alt="Different DenseNet Top-1 and Top-5 Error Rates with Single-Crop (10-Crop) Results"></p>
<p><img src="/Documents/ReadingNote/DenseNet/results2.png" alt="ImageNet Validation Set Results Compared with Original ResNet"></p>
<ul>
<li>左：20M参数的DenseNet-201与大于40M参数的ResNet-101产生类似的验证错误。</li>
<li>右：相似的计算次数趋势(GOLOPS)。</li>
<li>底部：DenseNet-264(k=48)最高误差为20.27%，前5误差为5.17%。</li>
</ul>
<hr>
<h1 id="6-特征复用的进一步分析"><a href="#6-特征复用的进一步分析" class="headerlink" title="6. 特征复用的进一步分析"></a>6. 特征复用的进一步分析</h1><p><img src="/Documents/ReadingNote/DenseNet/results3.png" alt="Heat map on the average absolute weights of how Target layer (l) reuses the source layer (s)"></p>
<ul>
<li>从非常早期的层中提取的特征被同一 Dense Block 中的较深层直接使用。</li>
<li>转换层的权重也分布在前面的所有层中。</li>
<li>第二和第三 dense block 内的各层一贯地将最小权重分配给转换层的输出。(第一行)</li>
<li>在最终分类层，权重似乎集中在最终 feature map 上。一些更高级的特性在网络中产生得很晚。</li>
</ul>
<hr>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[2017 CVPR] [DenseNet]<br><a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html">Densely Connected Convolutional Networks</a></p>
<hr>
<p>原文链接：<br><a href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803">https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803</a></p>
<hr>
</div><ul class="post-copyright"><li><strong>Title: </strong><a href="/2019/07/21/densenet/">DenseNet — Dense卷积网络（图像分类）</a></li><li><strong>Author: </strong><a href="/">Zhanhang ZENG (展航)</a></li><li><strong>Link: </strong><a href="/2019/07/21/densenet/">https://zengzhanhang.github.io/2019/07/21/densenet/</a></li><li><strong>Released Date: </strong>2019-07-21</li><li><strong>Last update: </strong>2020-05-16</li><li><strong>Statement: </strong>All articles in this blog, unless otherwise stated, are based on the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> license.</li></ul><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex mb-4"><div class="level-start"><div class="article-tags size-small is-uppercase"><span class="mr-2"><i class="fas fa-tags has-text-grey"></i> #</span><a class="link-muted mr-2" rel="tag" href="/tags/CV/">CV</a><a class="link-muted mr-2" rel="tag" href="/tags/Image-Classification/">Image Classification</a><a class="link-muted mr-2" rel="tag" href="/tags/Reading-Note/">Reading Note</a></div></div><div class="level-start"><div style="text-align:center"></div></div></div><div style="text-align:center"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/QRcode/alipay.jpg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/QRcode/WeChatPay.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/12/31/Human-Pose-Estimation-Review/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Human Pose Estimation Literature Review</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/07/18/deepmedic/"><span class="level-item">DeepMedic - multi-sacle 3D CNN with CRF for brain lesion segmentation</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '7fe54815721e985a9173913f05ce65b7',
            repo: 'zengzhanhang.github.io',
            owner: 'zengzhanhang',
            clientID: '13624a11c95a2d5eed31',
            clientSecret: '67d97a17e7f385cf24a8a24e3bbfaa1e8fcc20c4',
            admin: ["zengzhanhang"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/img/icon(square)-min.png" alt="Zhanhang (Kyle) Zeng"></figure><p class="title is-size-4 is-block line-height-inherit">Zhanhang (Kyle) Zeng</p><p class="is-size-6 is-block">Statistics, Machine Learning &amp; Ai</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Edinburgh, the United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">13</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">17</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded follow-button" href="https://github.com/zengzhanhang" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/zhanhang-zeng-801a67185/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/zengzhanhang"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/cengzhanhang"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://www.weibo.com/zengzhanhang"><i class="fab fa-weibo"></i></a></div></div></div><div class="card widget is-sticky is-active" id="toc"><div class="card-content"><div class="menu catalogue-other-setting"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#目录"><span class="mr-2">1</span><span>目录</span></a></li><li><a class="is-flex" href="#1-Dense-Block"><span class="mr-2">2</span><span>1. Dense Block</span></a></li><li><a class="is-flex" href="#2-DenseNet-结构"><span class="mr-2">3</span><span>2. DenseNet 结构</span></a><ul class="menu-list"><li><a class="is-flex" href="#2-1-基础-DenseNet-组成层"><span class="mr-2">3.1</span><span>2.1 基础 DenseNet 组成层</span></a></li><li><a class="is-flex" href="#2-2-DenseNet-B-Bottleneck-层"><span class="mr-2">3.2</span><span>2.2 DenseNet-B (Bottleneck 层)</span></a></li><li><a class="is-flex" href="#2-3-具有转换层（transition-layer）的多Dense块"><span class="mr-2">3.3</span><span>2.3 具有转换层（transition layer）的多Dense块</span></a></li><li><a class="is-flex" href="#2-4-DenseNet-BC-进一步压缩"><span class="mr-2">3.4</span><span>2.4 DenseNet-BC (进一步压缩)</span></a></li></ul></li><li><a class="is-flex" href="#3-DenseNet-的优势"><span class="mr-2">4</span><span>3. DenseNet 的优势</span></a><ul class="menu-list"><li><a class="is-flex" href="#3-1-Strong-Gradient-Flow"><span class="mr-2">4.1</span><span>3.1 Strong Gradient Flow</span></a></li><li><a class="is-flex" href="#3-2-Parameter-amp-Computational-Efficiency"><span class="mr-2">4.2</span><span>3.2 Parameter &amp; Computational Efficiency</span></a></li><li><a class="is-flex" href="#3-3-More-Diversified-Features"><span class="mr-2">4.3</span><span>3.3 More Diversified Features</span></a></li><li><a class="is-flex" href="#3-4-Maintains-Low-Complexity-Features"><span class="mr-2">4.4</span><span>3.4 Maintains Low Complexity Features</span></a></li></ul></li><li><a class="is-flex" href="#4-CIFAR-amp-SVHN-小规模数据集结果"><span class="mr-2">5</span><span>4. CIFAR &amp; SVHN 小规模数据集结果</span></a><ul class="menu-list"><li><a class="is-flex" href="#4-1-CIFAR-10"><span class="mr-2">5.1</span><span>4.1 CIFAR-10</span></a></li><li><a class="is-flex" href="#4-2-CIFAR-100"><span class="mr-2">5.2</span><span>4.2 CIFAR-100</span></a></li><li><a class="is-flex" href="#4-3-Detailed-Results"><span class="mr-2">5.3</span><span>4.3 Detailed Results</span></a></li></ul></li><li><a class="is-flex" href="#5-ImageNet-大规模数据集结果"><span class="mr-2">6</span><span>5. ImageNet 大规模数据集结果</span></a></li><li><a class="is-flex" href="#6-特征复用的进一步分析"><span class="mr-2">7</span><span>6. 特征复用的进一步分析</span></a></li><li><a class="is-flex" href="#Reference"><span class="mr-2">8</span><span>Reference</span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo1.png" alt="小树的面包店 | ZhanhangZeng&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2020 Zhanhang ZENG (展航)</span>  All Rights Reserved<br>Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span>    <span id="busuanzi_container_site_pv">Totally, <span id="busuanzi_value_site_pv">0</span> page views</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://zengzhanhang.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>