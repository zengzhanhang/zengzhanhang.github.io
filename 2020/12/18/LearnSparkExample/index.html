<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Building K-Means with Spark - Zhanhang Zeng&#039;s Blog | 小树的个人博客</title><meta description="Industry applications of machine learning generally require us to have the ability to deal with massive datasets. Spark provides a machine learning library named mllib allowing us to build machine le"><meta property="og:type" content="blog"><meta property="og:title" content="Building K-Means with Spark"><meta property="og:url" content="https://zengzhanhang.com/2020/12/18/LearnSparkExample/"><meta property="og:site_name" content="Zhanhang Zeng&#039;s Blog | 小树的个人博客"><meta property="og:description" content="Industry applications of machine learning generally require us to have the ability to deal with massive datasets. Spark provides a machine learning library named mllib allowing us to build machine le"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://zengzhanhang.com/notes/Spark/KmeansExample/thumbnail.png"><meta property="article:published_time" content="2020-12-18T09:07:37.000Z"><meta property="article:modified_time" content="2021-01-05T07:28:00.000Z"><meta property="article:author" content="Zhanhang (Matthew) ZENG"><meta property="article:tag" content="Big Data,Spark,Machine Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/notes/Spark/KmeansExample/thumbnail.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zengzhanhang.com/2020/12/18/LearnSparkExample/"},"headline":"Zhanhang Zeng's Blog | 小树的个人博客","image":["https://zengzhanhang.com/notes/Spark/KmeansExample/thumbnail.png"],"datePublished":"2020-12-18T09:07:37.000Z","dateModified":"2021-01-05T07:28:00.000Z","author":{"@type":"Person","name":"Zhanhang (Matthew) ZENG"},"description":"Industry applications of machine learning generally require us to have the ability to deal with massive datasets. Spark provides a machine learning library named mllib allowing us to build machine le"}</script><link rel="canonical" href="https://zengzhanhang.com/2020/12/18/LearnSparkExample/"><link rel="icon" href="/img/thumbnail.png"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?3a6a069adc986db638814c1b4a9e5ce6";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-167464294-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-167464294-1');</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/img/logo_light.png" alt="Zhanhang Zeng&#039;s Blog | 小树的个人博客" height="28"><img class="logo-img-dark" src="/img/logo_dark.png" alt="Zhanhang Zeng&#039;s Blog | 小树的个人博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/notes_TOC">Notes - 数据科学知识手册</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="/notes/Spark/KmeansExample/thumbnail.png" alt="Building K-Means with Spark"></span></div><article class="card-content article" role="article"><h1 class="title is-3 is-size-4-mobile">Building K-Means with Spark</h1><div class="article-meta size-small is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time class="level-item" dateTime="2020-12-18T09:07:37.000Z" title="2020-12-18T09:07:37.000Z">2020-12-18</time></span><span class="level-item"><i class="far fa-calendar-check"> </i><time class="level-item" dateTime="2021-01-05T07:28:00.000Z" title="2021-01-05T07:28:00.000Z">Edit: 2021-01-05</time></span><span class="level-item"><i class="fas fa-user"> </i>Zhanhang (Matthew) ZENG</span><span class="level-item"><i class="far fa-clock"></i> 8 minutes read (About 1215 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span> visits</span></div><div class="level-left is-uppercase mt-2"><span class="level-item"><i class="fas fa-folder-open has-text-grey"></i> <a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a><span> / </span><a class="link-muted" href="/categories/Big-Data/">Big Data</a><span> / </span><a class="link-muted" href="/categories/Machine-Learning/Statistical-Learning/">Statistical Learning</a><span> / </span><a class="link-muted" href="/categories/Big-Data/Spark/">Spark</a></span><span class="level-item"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/Big-Data-Spark-Machine-Learning/">Big Data,Spark,Machine Learning</a></span></div></div><div class="content"><div class="notification is-warning">
<p>Industry applications of machine learning generally require us to have the ability to deal with massive datasets. Spark provides a machine learning library named <code>mllib</code> allowing us to build machine learning models efficiently and parallelly.</p>
<p>This post is going to start with a Spark ML modelling example based on <code>pyspark</code> on Python, K-Means, and to explain some basic steps as well as the usage of Spark APIs when building an ML model on Spark.<br></div></p>
<a id="more"></a>
<h1 id="K-Means-with-Spark"><a href="#K-Means-with-Spark" class="headerlink" title="K-Means with Spark."></a>K-Means with Spark.</h1><h2 id="Loading-data-with-spark-sql"><a href="#Loading-data-with-spark-sql" class="headerlink" title="Loading data with spark.sql()"></a>Loading data with spark.sql()</h2><p><code>spark.sql()</code> will returns a DataFrame representing the result of the given query.</p>
<p>Generally, we need to prepare features used to build our models, and save these features on our database. The following two lines of code will <em>select</em> the prepared data from the Hive Database and fill the <code>NaN/Null</code> with <code>-1</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="comment"># Loading data from Hive</span></span><br><span class="line"><span class="comment"># This requires us to prepare the feature engieering on Hive before loading data</span></span><br><span class="line">df = spark.sql(<span class="string">"SELECT * FROM dev_dm_mdm.keep_matthew_lc_analysis_feature"</span>).fillna(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vs = [Vectors.dense([<span class="number">-2.0</span>, <span class="number">2.3</span>, <span class="number">0</span>]), Vectors.dense([<span class="number">3.8</span>, <span class="number">0.0</span>, <span class="number">1.9</span>])]</span><br><span class="line">dataset = sc.parallelize(vs)</span><br><span class="line">standardizer = StandardScaler(<span class="literal">True</span>, <span class="literal">True</span>)</span><br><span class="line">model = standardizer.fit(dataset)</span><br><span class="line">result = model.transform(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> result.collect(): r</span><br><span class="line">DenseVector([<span class="number">-0.7071</span>, <span class="number">0.7071</span>, <span class="number">-0.7071</span>])</span><br><span class="line">DenseVector([<span class="number">0.7071</span>, <span class="number">-0.7071</span>, <span class="number">0.7071</span>])</span><br><span class="line"></span><br><span class="line">int(model.std[<span class="number">0</span>])</span><br><span class="line"><span class="number">4</span></span><br><span class="line"></span><br><span class="line">int(model.mean[<span class="number">0</span>]*<span class="number">10</span>)</span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><h3 id="Features-assembling"><a href="#Features-assembling" class="headerlink" title="Features assembling"></a>Features assembling</h3><p><code>pyspark.ml.feature.VectorAssembler(inputCols=None, outputCol=None, handleInvalid=&#39;error&#39;)</code> will merges multiple columns into a vector column.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([(<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>)], [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>])</span><br><span class="line">vecAssembler = VectorAssembler(outputCol=<span class="string">"features"</span>)</span><br><span class="line">vecAssembler.setInputCols([<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>])</span><br><span class="line">vecAssembler.transform(df).head().features</span><br><span class="line"></span><br><span class="line">Out[<span class="number">14</span>]: DenseVector([<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>])</span><br></pre></td></tr></table></figure>
<p>The example above is a spark official example snip which creates a <code>DataFrame</code> containing three columns and shows how to merge these three columns into a single column named <code>features</code>. The <code>VectorAssembler</code> is called and passed a list of <code>[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]</code> to <code>setInputCols([])</code> so that the Assembler knows which columns you want to combine. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">dfWithNullsAndNaNs = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="literal">None</span>), </span><br><span class="line">    (<span class="number">3.0</span>, float(<span class="string">"nan"</span>), <span class="number">4.0</span>),</span><br><span class="line">    (<span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>)],</span><br><span class="line">    [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>]</span><br><span class="line">)</span><br><span class="line">vecAssembler2 = VectorAssembler(inputCols=[<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>], outputCol=<span class="string">"features"</span>, handleInvalid=<span class="string">"keep"</span>)</span><br><span class="line">vecAssembler2.transform(dfWithNullsAndNaNs).show()</span><br><span class="line">+---+---+----+-------------+</span><br><span class="line">|  a|  b|   c|     features|</span><br><span class="line">+---+---+----+-------------+</span><br><span class="line">|<span class="number">1.0</span>|<span class="number">2.0</span>|null|[<span class="number">1.0</span>,<span class="number">2.0</span>,NaN]|</span><br><span class="line">|<span class="number">3.0</span>|NaN| <span class="number">4.0</span>|[<span class="number">3.0</span>,NaN,<span class="number">4.0</span>]|</span><br><span class="line">|<span class="number">5.0</span>|<span class="number">6.0</span>| <span class="number">7.0</span>|[<span class="number">5.0</span>,<span class="number">6.0</span>,<span class="number">7.0</span>]|</span><br><span class="line">+---+---+----+-------------+</span><br><span class="line"></span><br><span class="line">vecAssembler2.setParams(handleInvalid=<span class="string">"skip"</span>).transform(dfWithNullsAndNaNs).show()</span><br><span class="line">+---+---+---+-------------+</span><br><span class="line">|  a|  b|  c|     features|</span><br><span class="line">+---+---+---+-------------+</span><br><span class="line">|<span class="number">5.0</span>|<span class="number">6.0</span>|<span class="number">7.0</span>|[<span class="number">5.0</span>,<span class="number">6.0</span>,<span class="number">7.0</span>]|</span><br><span class="line">+---+---+---+-------------+</span><br></pre></td></tr></table></figure>
<h3 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h3><p><code>pyspark.ml.feature.StandardScaler(withMean=False, withStd=True, inputCol=None, outputCol=None)</code>, this function is going to standardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set.</p>
<ul>
<li><strong>Parameters:</strong><ul>
<li><strong>withMean</strong> – False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input.</li>
<li><strong>withStd</strong> – True by default. Scales the data to unit standard deviation.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([(Vectors.dense([<span class="number">0.0</span>]),), (Vectors.dense([<span class="number">2.0</span>]),), (Vectors.dense([<span class="number">3.0</span>]),)], [<span class="string">"col_1"</span>])</span><br><span class="line">df.show()</span><br><span class="line">+-----+</span><br><span class="line">|col_1|</span><br><span class="line">+-----+</span><br><span class="line">|[<span class="number">0.0</span>]|</span><br><span class="line">|[<span class="number">2.0</span>]|</span><br><span class="line">|[<span class="number">3.0</span>]|</span><br><span class="line">+-----+</span><br><span class="line"></span><br><span class="line">stdScaler = StandardScaler(inputCol=<span class="string">"col_1"</span>, outputCol=<span class="string">"scaled_col"</span>)</span><br><span class="line">model = stdScaler.fit(df)</span><br><span class="line">model.mean</span><br><span class="line">Out[<span class="number">9</span>]: DenseVector([<span class="number">1.6667</span>])</span><br><span class="line"></span><br><span class="line">model.std</span><br><span class="line">Out[<span class="number">9</span>]: DenseVector([<span class="number">1.5275</span>])</span><br><span class="line"></span><br><span class="line">df_2 = model.transform(df)</span><br><span class="line">df_2.show()</span><br><span class="line">+-----+--------------------+</span><br><span class="line">|col_1|          scaled_col|</span><br><span class="line">+-----+--------------------+</span><br><span class="line">|[<span class="number">0.0</span>]|               [<span class="number">0.0</span>]|</span><br><span class="line">|[<span class="number">2.0</span>]|[<span class="number">1.3093073414159542</span>]|</span><br><span class="line">|[<span class="number">3.0</span>]|[<span class="number">1.9639610121239313</span>]|</span><br><span class="line">+-----+--------------------+</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="K-Means-Modelling"><a href="#K-Means-Modelling" class="headerlink" title="K-Means Modelling"></a>K-Means Modelling</h2><p><em>class</em> <code>pyspark.ml.clustering.KMeans(featuresCol=&#39;features&#39;, predictionCol=&#39;prediction&#39;, k=2, initMode=&#39;k-means||&#39;, initSteps=2, tol=0.0001, maxIter=20, seed=None, distanceMeasure=&#39;euclidean&#39;, weightCol=None)</code></p>
<p>The following code snip is a K-Means example where we are trying to find the best ‘k’ value by calculating the <code>within set sum of squared error</code> and the <code>Silhouette</code>. We create a list of <code>k</code> from 20, 30, 40, …, all the way to 200 so as to write a simple for loop to compute values of these 2 metrics (<code>within set sum of squared error</code> and <code>Silhouette</code>) of each <code>k</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Finding the best-k</span></span><br><span class="line">K = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>, <span class="number">200</span>, <span class="number">10</span>)]</span><br><span class="line">errors = []</span><br><span class="line">silhouette = []</span><br><span class="line">trtime = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</span><br><span class="line">    t0 = time.time()</span><br><span class="line">    km = KMeans(featuresCol = <span class="string">"features"</span>, k=k, seed=<span class="number">77</span>, maxIter=<span class="number">20</span>)</span><br><span class="line">    val_df = df_std.sample(<span class="number">0.1</span>) <span class="comment"># Sampling 抽样</span></span><br><span class="line">    model = km.fit(val_df)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    trtime.append(t1-t0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute wssse</span></span><br><span class="line">    pre = model.transform(val_df)</span><br><span class="line">    wssse = model.computeCost(val_df) <span class="comment"># 计算平方和</span></span><br><span class="line">    errors.append(wssse)</span><br><span class="line">    <span class="comment"># compute silhouette</span></span><br><span class="line">    evaluator = ClusteringEvaluator() <span class="comment"># 评估模型</span></span><br><span class="line">    socre = evaluator.evaluate(pre)</span><br><span class="line">    silhouette.append(score)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"==&gt; Training time : %s \n"</span> %(t1-t0))</span><br><span class="line">    print(<span class="string">"==&gt; with k = &#123;&#125;"</span>.format(k))</span><br><span class="line">    print(<span class="string">"==&gt; within set sum of squared Errors = &#123;&#125;"</span>.format(wssse))</span><br><span class="line">    print(<span class="string">"==&gt; Silhouette with squared euclidean distance = &#123;&#125;"</span>.format(score))</span><br><span class="line"></span><br><span class="line">pkmeans_stats = pd.DataFrame(&#123;<span class="string">'K'</span>: K, <span class="string">'errors'</span>: errors, <span class="string">'silhouette'</span>: silhouette, <span class="string">'training_time'</span>: trtime&#125;)</span><br><span class="line">pkmeans_stats.to_csv(<span class="string">"home/users/mdm_app/zengzhanhang/kmeans/kmeans_results_1227_01.csv"</span>, header=<span class="literal">None</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="Spark-K-Means-code-summarization"><a href="#Spark-K-Means-code-summarization" class="headerlink" title="Spark K-Means code summarization"></a>Spark K-Means code summarization</h1><figure class="highlight bash"><figcaption><span>Run Spark on Terminal</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/users/mdm_app/matthew/kmenas</span><br><span class="line"><span class="built_in">export</span> PYSPARK_DRIVER_PYTHON=/usr/<span class="built_in">local</span>/anaconda3/bin/ipython</span><br><span class="line">pyspark --name matthew --num-executors=10 --executor-cores=1 --executor-memory=16GB --driver-memory=32GB --conf spark.port.maxRetries=100</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>K-Means training script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.clustering <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> ClusteringEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler, StandardScaler, PCA</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> rand, randn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loading data from Hive</span></span><br><span class="line"><span class="comment"># This requires us to prepare the feature engieering on Hive before loading data</span></span><br><span class="line">df = spark.sql(<span class="string">"SELECT * FROM dev_dm_mdm.keep_matthew_lc_analysis_feature"</span>).fillna(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Getting insight our data</span></span><br><span class="line">df.printSchema()</span><br><span class="line">print(df.count(), len(df.columns))</span><br><span class="line">df.show(<span class="number">10</span>)</span><br><span class="line">df.head(<span class="number">5</span>)</span><br><span class="line">df.tail(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Basic processing</span></span><br><span class="line">feature_name = df.columns[<span class="number">1</span>:]</span><br><span class="line">vec_assembler = VectorAssembler(inputCols = feature_name, outputCol = <span class="string">'vas_features'</span>)</span><br><span class="line">df_vas = vec_assembler.transform(df) <span class="comment"># 将每一行所有列组合成一个特征行向量</span></span><br><span class="line">scaler = StandardScaler(withMean=<span class="literal">True</span>, withStd=<span class="literal">True</span>).setInputCol(<span class="string">"vas_features"</span>).setOutputCol(<span class="string">"Features"</span>) <span class="comment"># 标准化</span></span><br><span class="line">df_std = scaler.fit(df_vas).transform(df_vas)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Finding the best-k</span></span><br><span class="line">K = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>, <span class="number">200</span>, <span class="number">10</span>)]</span><br><span class="line">errors = []</span><br><span class="line">silhouette = []</span><br><span class="line">trtime = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</span><br><span class="line">    t0 = time.time()</span><br><span class="line">    km = KMeans(featuresCol = <span class="string">"features"</span>, k=k, seed=<span class="number">77</span>, maxIter=<span class="number">20</span>)</span><br><span class="line">    val_df = df_std.sample(<span class="number">0.1</span>) <span class="comment"># Sampling 抽样</span></span><br><span class="line">    model = km.fit(val_df)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    trtime.append(t1-t0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute wssse</span></span><br><span class="line">    pre = model.transform(val_df)</span><br><span class="line">    wssse = model.computeCost(val_df) <span class="comment"># 计算平方和</span></span><br><span class="line">    errors.append(wssse)</span><br><span class="line">    <span class="comment"># compute silhouette</span></span><br><span class="line">    evaluator = ClusteringEvaluator() <span class="comment"># 评估模型</span></span><br><span class="line">    socre = evaluator.evaluate(pre)</span><br><span class="line">    silhouette.append(score)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"==&gt; Training time : %s \n"</span> %(t1-t0))</span><br><span class="line">    print(<span class="string">"==&gt; with k = &#123;&#125;"</span>.format(k))</span><br><span class="line">    print(<span class="string">"==&gt; within setsum of squared Errors = &#123;&#125;"</span>.format(wssse))</span><br><span class="line">    print(<span class="string">"==&gt; Silhouette with squared euclidean distance = &#123;&#125;"</span>.format(score))</span><br><span class="line"></span><br><span class="line">pkmeans_stats = pd.DataFrame(&#123;<span class="string">'K'</span>: K, <span class="string">'errors'</span>: errors, <span class="string">'silhouette'</span>: silhouette, <span class="string">'training_time'</span>: trtime&#125;)</span><br><span class="line">pkmeans_stats.to_csv(<span class="string">"home/users/mdm_app/zengzhanhang/kmeans/kmeans_results_1227_01.csv"</span>, header=<span class="literal">None</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># K-means inference based on the best-k</span></span><br><span class="line">best_k = <span class="number">50</span></span><br><span class="line">km = KMeans(featuresCol = <span class="string">'features'</span>, k = best_k, seed = <span class="number">77</span>, maxIter = <span class="number">30</span>)</span><br><span class="line">val_df = df_std.sample(<span class="number">0.1</span>)</span><br><span class="line">model = km.fit(val_df)</span><br><span class="line">save_path = <span class="string">"hdfs://tmp/zengzhanhang/kmeans_&#123;&#125;.model"</span>.format(best_k)</span><br><span class="line">model.write().overwrite().save(save_path)</span><br><span class="line">transformed = model.transform(df_std).select(<span class="string">"cust_nbr"</span>, <span class="string">"prediction"</span>)</span><br><span class="line">transformed.write.mode(<span class="string">"overwrite"</span>).saveAsTable(<span class="string">"dev_dm_mdm.tmp_kmeans_pre_resutls&#123;&#125;"</span>.format(best_k)) <span class="comment">## save table in our datebases</span></span><br><span class="line"></span><br><span class="line">res = model.transform(df_std).groupby(<span class="string">"prediction"</span>).avg(*feature_name).toDF(*([<span class="string">"predict"</span>] + feature_names)).sort(<span class="string">"prediction"</span>)</span><br><span class="line">res2 = res.toPandas()</span><br><span class="line">cnt = model.transform(df_std).groupby(<span class="string">"prediction"</span>).count().sort(<span class="string">"prediction"</span>).toPandas()</span><br><span class="line">res2[<span class="string">'cnt'</span>] = cnt[<span class="string">'count'</span>]</span><br><span class="line">res3 = res2.copy()</span><br><span class="line">res_ = res3.T</span><br><span class="line">res_.to_csv(<span class="string">"home/users/mdm_app/zengzhanhang/kmeans/kmeans_results_1227_inference_&#123;&#125;_01.csv"</span>.format(best_k))</span><br></pre></td></tr></table></figure></div><ul class="post-copyright"><li><strong>Title: </strong><a href="/2020/12/18/LearnSparkExample/">Building K-Means with Spark</a></li><li><strong>Author: </strong><a href="/">Zhanhang (Matthew) ZENG</a></li><li><strong>Link: </strong><a href="/2020/12/18/LearnSparkExample/">https://zengzhanhang.com/2020/12/18/LearnSparkExample/</a></li><li><strong>Released Date: </strong>2020-12-18</li><li><strong>Last update: </strong>2021-01-05</li><li><strong>Statement: </strong>All articles in this blog, unless otherwise stated, are based on the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> license.</li></ul><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex mb-2"><div class="level-start"><div class="article-tags size-small is-uppercase"><span class="mr-2"><i class="fas fa-tags has-text-grey"></i> #</span><a class="link-muted mr-2" rel="tag" href="/tags/Big-Data-Spark-Machine-Learning/">Big Data,Spark,Machine Learning</a></div></div><div class="level-start"><div style="text-align:center"></div></div></div><div style="text-align:center"><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=5ebfed406b62a000122baf21&amp;product=inline-share-buttons" defer></script></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/QRcode/alipay.jpg" alt="Alipay"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/QRcode/WeChatPay.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/08/10/BashShellTrick/"><span class="level-item">Useful Trick with Linux Command</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '289e0b88c3522dd659ca3aa9d085da73',
            repo: 'zengzhanhang.github.io',
            owner: 'zengzhanhang',
            clientID: '13624a11c95a2d5eed31',
            clientSecret: '67d97a17e7f385cf24a8a24e3bbfaa1e8fcc20c4',
            admin: ["zengzhanhang"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="/img/icon(square)-min.png" alt="Zhanhang (Matthew) ZENG"></figure><p class="title is-size-4 is-block line-height-inherit">Zhanhang (Matthew) ZENG</p><p class="is-size-6 is-block">Statistics, Machine Learning &amp; Ai</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Edinburgh, Scotland</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">18</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded follow-button" href="https://github.com/zengzhanhang" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/zhanhang-zeng-801a67185/"><i class="fab fa-linkedin-in"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/zengzhanhang"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/cengzhanhang"><i class="fab fa-instagram"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="https://www.weibo.com/zengzhanhang"><i class="fab fa-weibo"></i></a></div></div></div><div class="card widget is-sticky" id="toc"><div class="card-content"><div class="menu catalogue-other-setting"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#K-Means-with-Spark"><span class="mr-2">1</span><span>K-Means with Spark.</span></a><ul class="menu-list"><li><a class="is-flex" href="#Loading-data-with-spark-sql"><span class="mr-2">1.1</span><span>Loading data with spark.sql()</span></a></li><li><a class="is-flex" href="#Data-preprocessing"><span class="mr-2">1.2</span><span>Data preprocessing</span></a><ul class="menu-list"><li><a class="is-flex" href="#Features-assembling"><span class="mr-2">1.2.1</span><span>Features assembling</span></a></li><li><a class="is-flex" href="#Standardization"><span class="mr-2">1.2.2</span><span>Standardization</span></a></li></ul></li><li><a class="is-flex" href="#K-Means-Modelling"><span class="mr-2">1.3</span><span>K-Means Modelling</span></a></li></ul></li><li><a class="is-flex" href="#Spark-K-Means-code-summarization"><span class="mr-2">2</span><span>Spark K-Means code summarization</span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/img/logo_light.png" alt="Zhanhang Zeng&#039;s Blog | 小树的个人博客" height="28"><img class="logo-img-dark" src="/img/logo_dark.png" alt="Zhanhang Zeng&#039;s Blog | 小树的个人博客" height="28"></a><p class="size-small"><span>&copy; 2021 Zhanhang (Matthew) ZENG</span>  All Rights Reserved<br>Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span>    <span id="busuanzi_container_site_pv">Totally, <span id="busuanzi_value_site_pv">0</span> page views</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><i class="fab fa-creative-commons"></i> <i class="fab fa-creative-commons-by"></i> <i class="fab fa-creative-commons-nc"></i> <i class="fab fa-creative-commons-sa"></i> </a><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Zhanhang&#039;s GitHub" href="https://github.com/zengzhanhang"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://zengzhanhang.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/imaegoo/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>