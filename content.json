{"pages":[{"title":"About Me 关于我","text":"展航(Zhanhang)，即将毕业的学生； 一个统计学学生正在想着计算机方向挣扎。 学习学习 之前一直没有把学过的内容总结起来的意识，最近搭了这个GitHub Page，希望能慢慢总结起来方便自己翻查自己的笔记。‘…’ 代表还未接触到的内容，希望自己能持续学习。 数学基础 微积分（Calculus） 线性代数（Linear Algebra） 概率论与数理统计（Statistics） 统计分析 回归分析（Regression Analysis） 时间序列分析（Time Series Analysis） 主成分分析（Principal Component Analysis） 因子分析（Factor Analysis） 聚类分析（Cluster Analysis） 相关分析（Correlation Analysis） 对应分析（Correspondence Analysis） 方差分析(ANOVA/Analysis of Variance) 数据可视化 R-ggplot2; plotly Python-Matplotlib; seaborn; plotly Dashboard 数据挖掘 标准化 + 异常值检测（预处理） 数据降维（PCA, kernel PCA, MDS, Isomap） 模型评估（Generalisation, over/under-fitting, Cross-validation…） 机器学习 k近邻法（k-Nearest Neighbors） 朴素贝叶斯（naïve Bayes） 支持向量机（SVM） 决策树（Decision tree） 集成学习（Ensemble Learning） k均值聚类（k-Means Clustering） 关联规则 EM算法（Expectation–maximization algorithm） 隐马尔可夫模型（HMM） 条件随机场（CRF） … 推荐系统 深度学习 DNN CNN RNN（LSTM） GAN … Python for Machine Learning scikit-learn Keras PyTorch … 计算机视觉 OpenCV … 自然语言处理 …（mark： Stanford CS224n Natural Language Processing with Deep Learning：Bilibili, YouTube Stanford CS224u Natural Language Understanding：Bilibili, YouTube） 强化学习（Reinforcement Learning） …（mark：UCL Reinforcement Learning：Biliblili, YouTube） 提升效率的工具 Git（推荐：廖雪峰Git教程） Linux（Cheat Sheet） LaTeX Resources: MIT 6.S191: Intro to Deep Learning. Official website: http://introtodeeplearning.com/#schedule （Bilibili; YouTube）","link":"/about/copy.html"},{"title":"About Me 关于我","text":"分享一句喜欢的话： “If you want to go fast, go alone. If you want to go far, go together.” 展航(Zhanhang)，即将毕业的学生；一个统计学学生正在向着计算机方向努力。 &nbsp; &nbsp; &nbsp; &nbsp; These notes listed in the table of Contents are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","link":"/about/index.html"},{"title":"Data Science Notes - 数据科学手册","text":"笔记目录 table of Contents UoE INFR11007 DME - Data Mining and Exploration Exploratory Data Analysis (EDA) Principal Component Analysis (PCA) Dimensionality Reduction Predictive Modelling and Generalization INFR11130 MLPR - Machine Learning and Pattern Recognition Linear Regression Training, Testing and Evaluating Models Gaussian Distribution Classification Optimisation Neural Networks Bayesian Regression Gaussian Processes Ensembles and Model Combination 大数据 大数据技术原理与应用 大数据基础 大数据概述 大数据处理框架 Hadoop 大数据存储与管理 分布式文件系统 HDFS 分布式数据库 HBase NoSQL 数据库 大数据处理与分析 MapReduce Hadoop 在探讨 Spark 流计算 图计算 大数据在各领域的应用 &nbsp; &nbsp; &nbsp; &nbsp; These notes listed in the table of Contents are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","link":"/notes_TOC/index.html"}],"posts":[{"title":"Useful Trick with Linux Command","text":"Cheatsheet for Linux usage 1. Download Google Drive files with wgetExample Google Drive shared file: https://drive.google.com/open?id=[ThisIsFileID] For general usage (not a big file)Example command: wget --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=FILEID' -O &quot;**FILENAME**&quot; Where, the FILEID is the [ThisISFiledID] shwon above. Download big filesCommand for download any big file from google drive (for big file we need confirm download) wget --load-cookies /tmp/cookies.txt &quot;https://docs.google.com/uc?export=download&amp;confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&amp;id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&amp;id=FILEID&quot; -O &quot;FILENAME&quot; &amp;&amp; rm -rf /tmp/cookies.txt Also, subsititute FILEID and FILENAME. Note that tere are 2 FILEID. Reference: https://gist.github.com/iamtekeste/3cdfd0366ebfd2c0d805 2. Download files from the requested login page with wgetExample: Download HumanEvaI data on its official website, http://humaneva.is.tue.mpg.de/datasets_human_1. Two steps: Log in to the server. This only needs to be done once. 12345wget --save-cookies cookies.txt \\ --keep-session-cookies \\ --post-data 'user=foo&amp;password=bar' \\ --delete-after \\ &quot;http://humaneva.is.tue.mpg.de/sessions&quot; Make sure the --post-data parameter is properly percent-encoded (especially ampersands!) or the request will probably fail. Also make sure that user and password are the correct keys; you can find out the correct keys by sleuthing the HTML of the login page (look into your browser’s “inspect element” feature and find the name attribute on the username and password fields). Thus, the --post-data parameter for the HumanEvaI website should be: --post-data 'login=foo&amp;password=bar' \\ Now grab the page or pages we care about. 12wget --load-cookies cookies.txt \\ http://server.com/interesting/article.php Reference: https://stackoverflow.com/questions/1324421/how-to-get-past-the-login-page-with-wget","link":"/2020/02/10/BashShellTrick/"},{"title":"DME - Data Mining and Exploration (INFR 11007) Review","text":"This is my review note of the DME course (Data Mining and Exploration (INFR11007), 2019) at the University of Edinburgh. The note include every steps to develop machine learning models and related knowledge, e.g., Exploratory Data Analysis (EDA), Data Preprocessing, Modeling and Model Evaluations. Remeber to read the ‘Lab’ section of each chapter 1. Exploratory Data Analysis1.1 Numberical Data Description1.1.1 Location Non-robust Measure Sample Mean (arithmetic mean or average): $\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ for random variable: $\\mathbb{E}[x] = \\int xp(x) dx$ Robust Measure Median: $$ median(x) = \\begin{cases} x_{[(n+1)\\mathbin{/}2]}& \\text{; if $n$ is odd}\\\\ \\frac{1}{2}[x_{(n\\mathbin{/}2)}+x_{(n\\mathbin{/}2)+1}]& \\text{; if $n$ is even} \\end{cases} $$ Mode: Value that occurs most frequent $\\alpha_{th}$ Sample Quantile (rough data point, i.e. $q_{\\alpha} \\approx x_{([n\\alpha])}$) $Q_{1} = q_{0.25}$, $Q_{2} = q_{0.5}$, $Q_{3} = q_{0.75}$ ExampleData_1=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9]Data_2=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9000] DataSet Mean Median $Q_{1}$ $Q_{3}$ Data 1 3.0 2.5 1.0 4.0 Data 2 902.1 2.5 1.0 4.0 1.1.2 Scale Non-robust Measure Sample Variance: $Var(x) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x})^2$ for random variable: $Var[x] = \\int [x-\\mathbb{E}[x]]^2 dx$ Standard Deviation: $Std(x) = \\sqrt{Var(x)}$ Robust Measure Median Absolute Deviation(MAD): $$MAD(x) = median[|x_{i} - median(x)|]$$ IQR(interquartile range): $$IQR = Q_{3} - Q_{1}$$ 1.1.3 Shape: Non-robust Measure Skewness: measures the asymmetry of data $$skew(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{3}$$ Kurtosis: measures how heavy the tails of distribution are, in other word, measures how often x takes on values that are considerable larger or smaller than its standard deviation.$$kurt(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{4}$$ Robust Measure Galtons’s measure of skewness: $$skew(x) = \\frac{(Q_{3}-Q_{2})-(Q_{2}-Q_{1})}{Q_{3}-Q_{1}}$$ Robust kurtosis: $$kurt(x) = \\frac{(q_{7/8}-q_{5/8})-(q_{3/8}-q_{1/8})}{Q_{3}-Q_{1}}$$ 1.1.4 Multivariate Measure: Sample Covariance: $$Cov(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x}) (y_{i} - \\hat{y})$$ for random variable: $Cov[x, y] = \\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])] = \\mathbb{E}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]$ Pearson’s Correlation Coefficient:$$\\rho(x,y) = \\frac{\\text{cov}(x,y)}{Std(x) Std(y)}$$ $\\rho=0$ doesn’t mean statistical independent, since it only measures linear correlation $-1 \\le \\rho \\le 1$ Simple way to measure non-linear correlation: $\\rho(g(x),g(y)) = \\frac{\\text{cov}(g(x),g(y))}{Std(g(x)) Std(g(y))}$ Covariance Matrix: $$Cov[X] = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{T}]$$ Eigenvalue decomposition: $Cov[X] = U\\Lambda U^{T}$ $\\sum_{i=1}^{d}Var[x_{i}]=trace(Var[X])=\\sum_{i=1}^{d} \\lambda_{i}$ $cov[Ax+b] = Acov[x]A^{T}$ Correlation Matrix:$$\\rho(X) = diag\\left( \\frac{1}{std(X)} \\right) Cov[X]diag\\left( \\frac{1}{std(X)} \\right)$$ Rank Correlation - Kendall’s $\\tau$: $$\\tau(x,y) = \\frac{n_{c}(x,y) - n_{d}(x,y)}{n(n-1)/2}$$ $n_c$: total number of concordant pairs, $n_d$: total number of disconcordant pairs 1.2 Data Visualisation1.3 Data Preprocessing:1.3.1 Standardisation:Normalising data to have 0 (sample) mean and unit (sample) variance: Centering Matrix: $$C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}$$ Where, $1_n = [1, 1, \\dots, 1]^T$ Multiplying it from right: removes sample mean of each row, i.e., $X = \\tilde{X}C_{n}$ left: removes sample mean of each column 1.3.2 Outlier Detection: Tukey’s fences: $[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)] = [Q_1 - k \\times IQR, Q_3 + k \\times IQR]$ Typically, $k = 1.5$ for outlier removal 1.4 Lab for Chapter.1 2. Principal Component Analysis(PCA)2.1 PCA by Variance Maximisation2.1.1 Sequential ApproachPrincipal Component(PC) direction: $\\boldsymbol{w}$, projected data: $\\boldsymbol{w}^{T} \\boldsymbol{x}$ The First Principal Component Direction: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{1}}}{\\text{maximise}} & & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = Var(z_{1}) \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{1}} = 1|| \\end{aligned} $$ According to the eigenvalue decomposition of convariance matrix $\\Sigma$: $\\Sigma = U \\Lambda U^{T}$ Let $\\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i} \\boldsymbol{u_{i}} = U \\boldsymbol{a}$, then $$ \\begin{aligned} & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & ||\\boldsymbol{w_{1}}|| = \\boldsymbol{w_{1}}^{T} \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ Thus, the optimisation problem can be written as: $$ \\begin{aligned} & {\\text{maximise}} & & \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & \\text{subject to} & & \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ $\\boldsymbol{a} = (1, 0, \\dots, 0)^T$ is the unique solution, if $lambda_{1} &gt; \\lambda{i}$. So the first PC direction is $$\\boldsymbol{w_{1}} = U \\boldsymbol{a} = \\boldsymbol{u_{1}}$$ , where the first PC direction given by the first eigen vector, $\\boldsymbol{u_{1}}$, of $\\Sigma$ corresponding to the first(largest) eigen value $\\lambda_{1}$. $Var(z_{1})= \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\lambda_{1}$ $\\mathbb{E}(z_{1}) = \\mathbb{E}(\\boldsymbol{w_{1}}^{T} \\boldsymbol{x}) = \\boldsymbol{w_{1}}^{T} \\mathbb{E}(\\boldsymbol{x}) = 0$ First PC scores: $\\boldsymbol{z_{1}}^{T} = \\boldsymbol{w_{1}}^{T} X_{d \\times n}$ Subsequent PC Direction $\\boldsymbol{w_{m}}$: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{m}}}{\\text{maximise}} & & \\boldsymbol{w_{m}}^T \\Sigma \\boldsymbol{w_{m}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{m}} = 1|| \\\\ & & & \\boldsymbol{w_{m}}^{T}\\boldsymbol{w_{i}} = 0 & & i = 1, 2, \\dots, m-1 \\end{aligned} $$ Solution: similar to the previous procedure $\\boldsymbol{w_{m}} = \\boldsymbol{u_{m}}$ is the m-th PC direction given by the m-th eigen vector of $\\Sigma$ corresponding to the m-th largest eigen value $\\lambda_{m}$. $Var(z_{m}) = \\lambda_{m}$, $\\mathbb{E}(z_{m}) = 0$ PCs (scores) uncorrelated: $$ \\begin{aligned} Cov(z_i, z_j) & = \\mathbb{E}(z_i z_j) - \\mathbb{E}(z_i) \\mathbb{E}(z_j)\\\\ & = \\mathbb{E}(\\boldsymbol{w_{i}}^{T} \\boldsymbol{x} \\boldsymbol{w_{j}}^{T} \\boldsymbol{x}) - 0\\\\ & = \\boldsymbol{w_{j}}^{T} \\mathbb{E}(\\boldsymbol{x} \\boldsymbol{x}) \\boldsymbol{w_{j}}^{T}\\\\ & = \\boldsymbol{w_{j}}^{T} \\Sigma \\boldsymbol{w_{j}}^{T} \\\\ & = \\boldsymbol{e_{i}}^T U^T U \\Lambda U^T U \\boldsymbol{e_{j}} \\\\ & = 0 \\end{aligned} $$ Fraction of variance explained $= \\frac{\\sum_{i}^{k} \\lambda_{i}}{\\sum_{i}^{d} \\lambda_{i}}$ how much variability in data is captured by the first k principal components. 2.1.2 Simultaneous Approach $$ \\begin{aligned} & \\text{maximise} & & \\sum_{i=1}^{k}\\boldsymbol{w_{i}}^T \\Sigma \\boldsymbol{w_{i}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, m-1\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ Subtle technical point: the sequential approach corresponds to solving this optimisation problem in greedy manner(algorithm), which doesn’t guarantee to yield optimal solution. However, sequential approach and simultaneous yield same results. 2.2 PCA by Minimisation of Approximation Error Projection Matrix: $$ P = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} = W_{k} W_{k}^{T} $$ , where $W_{k} = (\\boldsymbol{w_{1}}, \\dots, \\boldsymbol{w_{k}})$ is $d \\times k$ matrix . Approximating $\\boldsymbol{x}$ into subspace $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} \\boldsymbol{x}$ Approximation Error: $\\mathbb{E}||\\boldsymbol{x} - P \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - W_{k} W_{k}^T \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2$ Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2 \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, k\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ So, the optimal PC directions $\\boldsymbol{w_{i}}$ are the first k eigen vectors $\\boldsymbol{u_{i}}$ of $\\Sigma$ The optimal projection matrix is $P = U_k U_{k}^{T}$ $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = U_{k} U_{k}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} \\boldsymbol{u_{i}}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} z_{i}$ $\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2 = \\sum_{i=1}^{d} \\lambda_{i} - \\sum_{i=1}^{k} \\lambda_{i} = \\sum_{i=k+1}^{d} \\lambda_{i}$, which means minimising expected error = maximising variance explained. Relative Approximation Error: $$ \\frac{\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2}{\\mathbb{E}||\\boldsymbol{x}||^2} = 1 - \\frac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{i=1}^{d} \\lambda_{i}} = 1 - \\text{fraction of variance explained} $$ 2.3 PCA by Low Rank Matrix Approximation2.3.1 Approximation from Data Matrix Let $X_{d \\times n} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})$, where $\\boldsymbol{x}$ is $d \\times 1$ matrix (d-dimension). Express $X$ via its Singular Value Decomposition(SVD): $X = U S V^{T}$ , where $U_{d \\times d}$ and $V_{n \\times n}$ are orthonormal. $S$ is zero everwhere, but first r diagonal elements. Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\sum_{ij} \\left[ (X)_{ij} - (M)_{ij} \\right]^2 = ||X - \\hat{X}||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ \\end{aligned} $$ So, Optimal solution: $\\hat{X} = \\sum_{i=1}^{k} \\boldsymbol{u_i} \\boldsymbol{s_i} \\boldsymbol{v_i}^{T} = U_K S_K V_K^T$ ((truncated singular value decomposition). left singular vectors $\\boldsymbol{u_i}$ are eigen vectors of $\\Sigma$, so $\\boldsymbol{u_i}$ are PC directions. $s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$: $\\lambda_i = \\frac{s_i^2}{n}$. (Proof in Appendix A) PC scores: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = s_i \\boldsymbol{v_i}^T$ Proof: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = \\boldsymbol{u_i}^T U S V^T = \\boldsymbol{u_i}^T \\sum_{j=1}^{r}\\boldsymbol{u_j} s_j \\boldsymbol{v_j}^T = s_i \\boldsymbol{v_i}^T$ 2.3.2 Approximation from Sample Covariance Matrix Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & ||\\Sigma - M||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ & & & M^T = M \\end{aligned} $$ Optimal solution: $M = \\hat{\\Sigma} = U_k \\Lambda_k U_k^T = \\Sigma^T$, i.e., $\\sum_{i=1}^{k}\\lambda_i \\boldsymbol{u_i} \\boldsymbol{u_i}^T$ 2.3.3 Approximation from Gram Matrix Gram Matrix:$$G = X^T X \\text{, where} (G)_{ij} = \\boldsymbol{x_i}^T\\boldsymbol{x_j}$$ Gram Matrix is positive semi-definite According the SVD of $X$: $$ G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T = \\sum_{i=1}^{n} s_i^2 \\boldsymbol{v_i} \\boldsymbol{v_i}^T $$ Thus, the best rank k approximation of $G$ is $\\hat{G} = \\sum_{i=1}^{k} \\boldsymbol{v_i} s_i^2 \\boldsymbol{v_i}^T$. Denote $\\tilde{\\Lambda} = S^T S$ is the top k eigen value of $G$, $V_k = (\\boldsymbol{v_1}, \\boldsymbol{v_2}, \\dots, \\boldsymbol{v_k})_{n \\times k}$ $$ Z_k = \\sqrt{\\tilde{\\Lambda}_k} V_k^T $$ 2.3.4 Probabilistic PCA (PPCA) Advantages: PPCA can samples artificial data points (generative model). Formulation allows us to deal with missing data. Probabilistic Model: $$ Z \\sim \\mathcal{N}(0,\\,I_k)\\\\ \\epsilon \\sim \\mathcal{N}(0, \\, \\sigma^2 I_d)\\\\ \\underset{d \\times 1}{\\boldsymbol{x}} = \\underset{d \\times k}{W} \\; \\underset{k \\times 1}{\\boldsymbol{z}} + \\underset{d \\times 1}{\\boldsymbol{\\mu}} + \\underset{d \\times 1}{\\boldsymbol{\\epsilon}} $$ Joint, Conditional and Observation Distribution Conditional Distribution: $$ p(\\boldsymbol{x}|\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{\\mu},\\; \\sigma^2I_{d}) $$ Joint Distribution: $$ \\begin{aligned} p(\\boldsymbol{z},\\; \\boldsymbol{x}) & = p(\\boldsymbol{x}|\\boldsymbol{z})p(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{u},\\; \\sigma^2I_{d}) \\mathcal{N}(\\boldsymbol{z};\\; 0,\\; I_k)\\\\ & = \\frac{1}{const}exp \\left[ -\\frac{1}{2} [(\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu})^{T} (\\frac{1}{\\sigma^2}I_{d}) (\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu}) + \\boldsymbol{z}^{T} \\boldsymbol{z}] \\right] \\end{aligned} $$ Important Equations: For multivariate normal distribution: $$ \\begin{aligned} -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu}) & = -\\frac{1}{2}\\boldsymbol{x}^T \\Sigma^{-1} \\boldsymbol{x} + \\boldsymbol{x}^{T} \\Sigma^{-1}\\mu + const\\\\ & = -\\frac{1}{2}\\boldsymbol{x}^T A \\boldsymbol{x} + \\boldsymbol{x}^{T} \\xi + const \\end{aligned} $$ Thus, $\\Sigma = A^{-1}$ and $\\boldsymbol{\\mu} = \\Sigma \\ \\xi$ . Observation Distribution: $$ p(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}; \\; \\boldsymbol{\\mu}, \\; W W^{T} + \\sigma^2 I) $$ Maximum Likelihood:The maximum likelihood solutions are shown by Tipping and Bishop, 1999: $$ W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}} R\\\\ \\sigma_{ML}^2 = \\frac{1}{d-k} \\sum_{i=k+1}^{d}\\lambda_{i} $$ $U_k$ are $k$ principal eigenvectors of $\\hat{\\Sigma} = Cov(X) = \\frac{1}{n}X X^T$ . $\\Lambda_k$ is diagonal matrix with eighenvalues. $R$ is arbitrary orthogonal matrix, interpreted as a rotation in the latent space, indicating not unique solutions. Another option to find $W$ and $\\sigma^2$ is EM algorithm. Relation to PCA: The closest thing to PCA mapping is the posterior distribution $p(\\boldsymbol{z}| \\; \\boldsymbol{x})$. To find it, we can fix $\\boldsymbol{x}$ as a constant in the joint distribution $p(\\boldsymbol{z},\\; \\boldsymbol{x})$ and use the important equation just mentioned above. $$ p(\\boldsymbol{z}| \\; \\boldsymbol{x} = \\mathcal{N}(\\boldsymbol{z}; \\; M^{-1} W^{T} (\\boldsymbol{x} - \\boldsymbol{\\mu}), \\; \\sigma^2 M^{-1}) $$ , where $M = W^T W + \\sigma^2 I$ . PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\hat{\\boldsymbol{x}} = W_{ML} \\mathbb{E}(\\boldsymbol{z}|\\; \\boldsymbol{x}) = W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} $$ , where $M_{ML} = W_{ML}^{T} W_{ML} + \\sigma^{2}I \\;$ and $\\; W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}}$ . For $\\sigma^2 \\rightarrow 0$, we recover the PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\begin{aligned} W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} & = U_k \\Lambda_k^{1/2} ((U_k \\Lambda_k^{1/2})^T (U_k \\Lambda_k^{1/2}))^{-1} (U_k \\Lambda_k^{1/2})^{T} \\boldsymbol{x}\\\\ & = U_k U_k^T \\boldsymbol{x} \\end{aligned} $$ 2.4 Lab for Chapter.2 3. Dimensionality Reduction3.1 Linear Dimensionality Reduction3.1.1 From Data Matrix Observed (uncentered) data: $\\tilde{X} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})_{d \\times n}$ Center data: $X = \\tilde{X} C_n$ , where $C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}\\ $ . Option 1 - compute PC scores via eigen values decomposition: $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T = U \\Lambda U^T \\end{aligned} $$ Denote $U_k$ with the first $k$ eigen vectors of $\\Sigma$ corresponding to the top $k$ eigen values: $U_k = (\\boldsymbol{u_1}, \\boldsymbol{u_2}, \\dots, \\boldsymbol{u_k})_{d \\times k}$ PC scores: $$ \\begin{aligned} \\underset{k \\times 1}{\\boldsymbol{z}_i} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times 1}{\\boldsymbol{x}_i} , & & \\underset{k \\times n}{Z} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times n }{X} \\end{aligned} $$ Option 2 - compute PC scores via Gram Matrix: $$ \\begin{aligned} G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T \\end{aligned}\\\\ \\begin{aligned} \\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\underset{k \\times n}{V_k^T}, & & V_k = (\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_k) \\end{aligned} $$ 3.1.2 From Inner Product $$ \\begin{aligned} (G)_{ij} = \\boldsymbol{x}_i^T \\boldsymbol{x}_j & & X = \\tilde{X} C_n & & \\tilde{G} = \\tilde{X}^T \\tilde{X} \\end{aligned}\\\\ G = X^T X = C_n \\tilde{X}^T \\tilde{X} C_n = C_n \\tilde{G} C_n $$ 3.1.3 From Distance Matrix If only given squared distance $\\delta_{ij}^2$ between data points $\\tilde{\\boldsymbol{x_i}}$ and $\\tilde{\\boldsymbol{x_j}} \\ $. $$ \\delta_{ij}^2 = ||\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}||^2 = (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}})^T (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}) $$ Distance Matrix $\\Delta$ contains elements $\\delta_{ij} \\ $. $$ \\delta_{ij}^2 = ||(\\tilde{\\boldsymbol{x_i}} -\\mu) - (\\tilde{\\boldsymbol{x_j}} - \\mu)||^2 = ||\\boldsymbol{x_i} - \\boldsymbol{x_j}||^2 = (\\boldsymbol{x_i} - \\boldsymbol{x_j})^T(\\boldsymbol{x_i} - \\boldsymbol{x_j})\\\\ \\delta_{ij}^2 = ||\\boldsymbol{x_i}||^2 + ||\\boldsymbol{x_j}||^2 -2\\boldsymbol{x_i}^T \\boldsymbol{x_j} $$ Center the distance: $$ (C_n \\Delta C_n)_{ij} = (\\Delta C_n)_{ij} - \\frac{1}{n} \\sum_{i} (\\Delta C_n)_{ij} = - 2\\boldsymbol{x_i}^T \\boldsymbol{x_j}\\\\ G = -\\frac{1}{2}C_n \\Delta C_n $$ 3.2 (Non-linear) Dimensionalisty Reduction via Kernel PCA To obtain new data matrix $\\Phi$ using the transforming function $\\phi(\\boldsymbol{x}_i)$. $$ \\Phi = (\\phi_1, \\phi_2, \\dots, \\phi_n) = (\\phi(\\boldsymbol{x}_1), \\phi(\\boldsymbol{x}_2), dots, \\phi(\\boldsymbol{x}_n)) $$ Kernel Trick: inner product of some functions can be computed as: $$ \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ uncentered Gram Matrix $G$ of $\\Phi$ with elements $(\\tilde{G})_{ij}$: $$ \\tilde{G})_{ij} = \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ Polynomial kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = (\\boldsymbol{x}_i^T \\boldsymbol{x}_j)^\\alpha$Gaussian kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = exp \\left( - \\frac{||\\boldsymbol{x_i} - \\boldsymbol{x}_j||^2}{2 \\sigma^2} \\right)$ Then applying methods in Sec 3.1.2 and Sec 3.1.1 to compute PC scores. 3.3 Multidimensional Scaling (MDS)3.3.1 Metric MDS Assumption: the numerical values of dissimilarities (e.g. Euclidean distance) carry information. Optimisation Problem: $$ \\begin{aligned} \\text{minimise}& & w_{ij}(||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - \\delta_{ij})^2 \\end{aligned} $$ $\\delta_{ij}$ are dissimilarities between two data items, e.g. Euclidean Distance. $||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| \\ $ is Euclidean distance betweeen $\\boldsymbol{z}_i \\ $ and $\\ \\boldsymbol{z}_j \\ $, i.e., $\\ \\sqrt{(\\boldsymbol{z}_i - \\boldsymbol{z}_j)^T (\\boldsymbol{z}_i - \\boldsymbol{z}_j)} \\ $. $w_{ij} \\ $ are some weights specified by users. if $\\ w_{ij} = \\frac{1}{\\delta_{ij}} \\ $, the MDS is called Sammon nonlinear mapping emphasing the faithful representation of samll dissimilarities. Solved by gradient descent. 3.3.2 Non-metric MDS Assumption: only relationship between $\\ \\delta_{ij} \\ $ matters, i.e., whether $\\ \\delta_{12} &gt; \\delta_{13}\\ $ or $\\ \\delta_{12} &lt; \\delta_{13}\\ $ . Optimisation Problem: $$ \\begin{aligned} \\underset{\\boldsymbol{z_1}, \\boldsymbol{z_2}, \\dots, \\boldsymbol{z_n}, f}{\\text{minimise}}& & \\sum_{i \\le j} w_{ij} (||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - f(\\delta_{ij}))^2 \\end{aligned} $$ Actual values of $\\ \\delta_{ij} \\ $ do not matter. $f \\ $ is monotonic (non-decreasing) function converting dissimilarities to distances. Solved by iterating between optimisation w.r.t $\\ \\boldsymbol{z}_i \\ $ and optimisation w.r.t $\\ f \\ $, which can be done by regression. 3.3.3 Classical MDS: Assumption: numerical values of $\\ \\delta_{ij} \\ $ matter. Dissimilarities $\\ \\delta_{ij} \\ $ are (squared) Eucldiean distance between some unknown vectors. Distance matrix $\\ \\Delta \\ $ is formed by $\\ \\delta_{ij} \\ $ Using the method in Sec 3.1.3: Compute hypothetical Gram matrix $\\ G’ \\ $ of unknown centered data points. $$ \\begin{aligned} G = -\\frac{1}{2}C_n \\Delta C_n ,& & C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T} \\end{aligned} $$ Compute top k eigen values $\\ \\sigma_k^2 \\ $ and corresponding eigen vectors $\\ \\boldsymbol{v}_k \\ $ of $\\ G \\ $ and form $\\ \\tilde{\\Lambda}_k = diag(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_k^2) \\ $ and $\\ V_k = (\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_k)_{n \\times k}$ $\\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\; \\underset{k \\times n}{V_k^T}$ $\\Delta \\ $ is not necessary positive semi-definite, thus, some eigen values might be negative. Solution: choose $\\ k \\ $small enough to avoid negative eigen values. Classical MDS solution for $\\ k’ &lt; k \\ $ is directly given by the first $\\ k’ \\ $ corordinates of $\\ k \\ $ dimensional $\\ \\boldsymbol{z} \\ $. Alternative approximate negative definite $\\ \\Delta \\ $ by: $$ \\begin{aligned} & \\text{minimise}& & ||(-\\frac{1}{2}C_n \\Delta C_n) - M^T M||_F\\\\ & \\text{subject to}& & rank(M^T M) = k \\end{aligned} $$ 3.3.4 Isometric Features Mapping (Isomap) Steps of Isomap Construct the neighbourhood graph via ‘k nearest neighbour‘ or all data points within a certain (Euclidean) distance. Construct the shortest path (distances) as geodesic distance Construct the low dimensional embeding of these data via MDS so as to represent these data. Geodesic distance is measured by the shortest distance between them when only allowed to travel on the data manifold from one neighbouring data point to the next. Isomap well represents the circular structure when learned graph is connected. 3.4 Lab for Chapter.3 4. Predictive Modelling and Generalization4.1 Prediction and Training Loss4.1.1 Prediction Loss $$ \\mathcal{J}(h) = \\mathbb{E}_{\\hat{y}, \\ y} \\left[ \\mathcal{L}(\\hat{y}, \\ y) \\right] = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(h(\\boldsymbol{x}), \\ y) \\right] $$ The term $\\ \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\ $ means expectation w.r.t $\\ p(\\boldsymbol{x},\\ y) \\ $ . 4.1.2 Training Loss $$ \\mathcal{J}_{\\lambda}^{*} = \\underset{\\theta}{min} \\ \\mathcal{J}_{\\lambda}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\mathcal{L}(h(\\boldsymbol{x}_i; \\ \\theta), \\ y_i) \\right] $$ 4.2 Generalisation Performance4.2.1 Generalisation Loss For prediction function $$ \\mathcal{J}(\\hat{h}) = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(\\hat{h}(\\boldsymbol{x}), \\ y) \\right] $$ Done with held-out data For algorithm $$ \\bar{\\mathcal{J}}(\\mathcal{A}) = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\hat{h}) \\right] = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\mathcal{A}(D^{train})) \\right] $$ See DME Lecture Notes for more details. 4.2.2 Overfitting and Underfitting Overfitting: Reducing the model complexity, the prediction loss decreases. Underfitting: Increasing the model complexity, the prediction loss decreases. Solutions: Model Selection or Regularisation . Regularisation: $$ \\begin{aligned} & \\text{minimise} & & \\mathcal{J}_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\theta}) + \\lambda_{reg} R(\\boldsymbol{\\theta}) \\end{aligned} $$ L2 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} \\theta_i^2 \\; $ L1 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} |\\theta_i| \\; $ Either model complexity and size of training data matter generalisation performance, See 4.2.3 Example on DME Lecture Notes. 4.3 Estimating the Generalisation PerformanceWe typically need to estimate the generalisation performance twice: Once for hyperparameter selection, and once for ﬁnal performance evaluation. 4.3.1 Methods for Estimating the Generalisation PerformanceHeld-out Approach Prediction function: $$ \\begin{aligned} \\hat{h} = \\mathcal{A}(D^{train}) \\end{aligned} $$ Prediction Loss on Testing/ Validation Sets $\\ \\tilde{D} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}}(\\hat{h}: \\ \\tilde{D}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L} \\left( \\hat{h}(\\tilde{\\boldsymbol{x}}_i, \\ \\tilde{y}_i) \\right) \\end{aligned} $$ Common split ratios $\\ n/ \\tilde{n} \\ $: 60/40, 70/30 or 80/20 . If the number of (hyper-)parameters is large, let more data on training set. Split randomly. Stratification: classes are presented in same proportion in both sets. Drawback: estimated prediction loss may varies strongly in different $\\ \\tilde{D} \\ $, unless $\\ \\tilde{n} \\ $ is large. Solve by Cross-Validation Cross-Validation Approach K-fold: Construct k pairs of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} & D^{train} = D_{i \\neq k} & & D^{val} = D_k \\end{aligned} $$ K Prediction functions: obtained by using k training sets . $$ \\begin{aligned} \\hat{h}_k = \\mathcal{A}(D_{k}^{train}) \\end{aligned} $$ K performance Estimations: evaluated on k validation sets . $$ \\begin{aligned} \\hat{\\mathcal{J}}_k = \\hat{\\mathcal{J}}(\\hat{h}_k : \\ D_k^{val}) \\end{aligned} $$ Cross Validation (CV) Score: averaging all k $\\ \\hat{\\mathcal{J}}_k \\ $ $$ \\begin{aligned} CV = \\frac{1}{K} \\sum_{k=1}^{K}\\hat{\\mathcal{J}}_k \\left(\\mathcal{A} (D_k^{train}: D_k^{val}) \\right) = \\hat{{\\bar{\\mathcal{J}}}} (\\mathcal{A}) \\end{aligned} $$ Estimate Variability of CV score $$ \\begin{aligned} Var(CV) \\approx \\frac{1}{k} Var(\\hat{\\mathcal{J}}_k), & & Var{\\hat{\\mathcal{J}}} = \\frac{1}{k} = (\\hat{\\mathcal{J}}_k - CV) ^2 \\end{aligned} $$ LOOCV (Leave-One-Out Cross-Validation): $\\ D^{val} \\ $ contains only one data point. Generally expensive, but for some problems, the computation can be done quickly. For a further discussion of the choice of K, see e.g. Section 7.10 in the textbook by Hastie, Tibshirani, and Friedman (2009). 4.3.2 Hyperparameters Selection and Performance Evaluation:Option 1 - Two Times Held-out Split off some testing data to evaluate the final performance., e.g. typically, $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Split remaining data into $\\ D^{train} \\ $, $\\ D^{val} \\ $, e.g. 80/20 ratio. Tuning parameters $\\ \\boldsymbol{\\lambda} \\ $ on $\\ D^{train} \\ $, return a set of $\\ \\hat{\\boldsymbol{\\lambda}} \\ $ . $$ \\begin{aligned} \\hat{h}_{\\boldsymbol{\\lambda}} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) \\end{aligned} $$ Compute prediction loss $\\ PL({\\boldsymbol{\\lambda}}) \\ $ on $\\ D^{val} \\ $. $$ \\begin{aligned} PL(\\boldsymbol{\\lambda}) = \\hat{\\mathcal{J}} (\\hat{h}_{\\boldsymbol{\\lambda}}: \\ D^{val}) \\end{aligned} $$ and choosing the $\\ \\boldsymbol{\\lambda} \\ $ by minimising $\\ PL(\\boldsymbol{\\lambda}) \\ $ $$ \\begin{aligned} \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }} PL(\\boldsymbol{\\lambda}) \\end{aligned} $$ Using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $, re-estimate $\\ \\boldsymbol{\\theta} \\ $ on the union of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} \\hat{h} = \\mathcal{A}_{\\hat{\\boldsymbol{\\lambda}}} = \\left( D^{train} U D^{val} \\right) \\end{aligned} $$ Compute prediction loss on $\\ D^{test} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}} = \\hat{\\mathcal{J}}(\\hat{h}:\\ D^{test}) \\end{aligned} $$ Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ Option 2 - Cross-validation + Held-out Split of $\\ D^{test} \\ $, e.g. $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Compute CV score on remaining data $\\ D^{train} \\ $. $$ EPL(\\boldsymbol{\\lambda}) = CV $$ Choose $\\ \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }}EPL(\\boldsymbol{\\lambda}) \\ $ Re-estimate $\\ \\boldsymbol{\\theta} \\ $ on $\\ D^{train} \\ $ using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $. $$ \\hat{h} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) $$ Compute prediction loss on $\\ D^{test} \\ $. Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ 4.4 Loss Functions in Predictive Models.4.4.1 Regression $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = | \\hat{y} - y | & & \\text{(Absolute Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & \\text{if } | \\hat{y} - y |< \\delta\\\\ \\delta | \\hat{y} - y | - \\frac{1}{2} \\delta^2 & \\text{otherwise} \\end{cases} & & \\text{(Huber Loss)} \\end{aligned} $$ 4.4.2 Classification4.4.2.1 Non-differentiable Loss Function Assume k different classes, loss function $\\ L(\\hat{y}, \\ y) \\ $ can be represented as $\\ k \\times k \\ $ matrix. $$ L(\\hat{y}, \\ y) = \\begin{bmatrix} L(1,1) & L(1,2) & \\dots & L(1,k) \\\\ L(2,1) & L(2,2) & \\dots & L(2,k) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ L(k,1) & L(k,2) & \\dots & L(k,k) \\end{bmatrix} $$ The diagonal $\\ L(i,i) \\ $ are zero as correct prediction. The off-diagonal $\\ L(i,j) \\ $ are positive: loss incurred when predicting ‘i’ instead of ‘j’ Zero-One loss: If $\\ L(i,\\ j) = 1 \\ $ for $\\ i \\neq j \\ $, and 0 otherwise $$ L(\\hat{y}, \\ y) = \\begin{cases} 1 & i \\neq j\\\\ 0 & otherwise \\end{cases} $$ The expected prediction loss: $$ \\begin{aligned} \\mathcal{J}(h) & = \\mathbb{E}_{\\boldsymbol{x}, \\ y} L \\left(h(\\boldsymbol{x}), \\ y) \\right)\\\\ & = \\mathbb{E}_{\\hat{y}, \\ y} L \\left(\\hat{y}, \\ y) \\right)\\\\ & = \\sum_{i, j} L(i,\\ j) p(i,\\ j)\\\\ & = \\sum_{i \\neq j} p(i, \\ j)\\\\ & = \\mathbb{P}(y \\neq \\hat{y}) \\end{aligned} $$ , where $\\ p(i,\\ j) = p(\\hat{y} = i, \\ y = j) \\ $ Known as ‘missclassification rate’ Binary Classification receiver operating characteristic curve (ROC curve) Minimising the false-positive (or false-negative) rate alone is not a very meaningful strategy: The reason is that the trivial classiﬁer $\\ h(x) = \\hat{y} = −1 \\ $ would be the optimal solution. But for such a classiﬁer the true-positive rate would be zero. ROC curve visualise a generally a trade-oﬀ between true-positive rate (TPR) and false-positive rates (FPR). 4.4.2.2 Diﬀerentiable Loss FunctionsFor simplicity, we consider here binary classiﬁcation only. Let us assume that $\\ \\hat{y} ∈{−1,1} \\ $ is given by $$ \\hat{y}(\\boldsymbol{x}) = sign(h(\\boldsymbol{x})) $$ , where $\\ h(\\boldsymbol{x})\\ $ is real-valued. $$ \\text{correct classiﬁcation of } \\boldsymbol{x} ⇐⇒ yh(\\boldsymbol{x}) > 0. $$ Loss Function: $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\begin{cases} 1 & \\text{if } y h(\\boldsymbol{x} < 0)\\\\ 0 & \\text{otherwise.} \\end{cases} & & \\text{(Zero-One Loss)}\\\\ & L(\\hat{y},\\ y) = (h(\\boldsymbol{x}) - y)^2 = (1 - y h(\\boldsymbol{x}))^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = log \\left( 1 + exp(- y h(\\boldsymbol{x})) \\right) & & \\text{(Log Loss)}\\\\ & L(\\hat{y},\\ y) = exp(- y h(\\boldsymbol{x})) & & \\text{(Exponential Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right) & & \\text{(Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & & \\text{(Square Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} - 4 y h(\\boldsymbol{x}) & \\text{if } y h(\\boldsymbol{x}) < -1\\\\ max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & \\text{otherwise} \\end{cases} & & \\text{(Huberised Square Hinge Loss)} \\end{aligned} $$ 4.5 Lab for Chapter.4 Appendix A$s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$Assume $X$ centered, then, according the SVD of $X$, the covariance matrix is $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T \\\\ & = \\frac{1}{n}U S V^T (U S V^T)^T = \\frac{1}{n} U (\\frac{1}{n}S S^T) U^T\\\\ & = U \\Lambda U^T \\end{aligned} $$ Reference[1]: Michael E Tipping and Christopher M Bishop. “Probabilistic principal component analysis”. In: Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61.3 (1999), pp. 611–622 [2]: T. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning. Springer, 2009.","link":"/2019/05/14/DME-Data-Mining-and-Exploration-Revision/"},{"title":"Keras 笔记","text":"To take notes about the essential Keras elements to build basic neural networks. The explainations of each section haven’t finished yet. 1. Single Layer Neural Network (Linear Regression) 单层神经网络相当于（非）线性回归模型，第一个例子是构建一个最简单一元线性回归模型。 创建数据 单层神经网络模型需要数据进行训练，因此我们使用 numpy 创建一些人造数据，且我们的 $y$ 为 $y = ax+b$ 。 1234567891011121314import numpy as npimport tensorflow as tffrom tensorflow.keras import layersimport matplotlib.pyplot as pltplt.style.use('seaborn')# create dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) #randomize the dataY = 2*X + 10 + np.random.normal(0, 0.05, (200,))# plot dataplt.scatter(X, Y)plt.show() 构建神经网络 tf.keras.models.Sequential() 1234tf.keras.models.Sequential( layers=None, name=None) 用Keras创建神经网络，我们首先需要用 tf.keras.models.Sequential() 来建立网络，这里面只有一个argument，即 layers。这里添加神经层的方法有两种，一种是建立 model 的时候直接放入神经层，另一种是通过 model.add 来添加。如： 12345678# option 1model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1, activation = None, use_bias = True)])# option 2model = tf.keras.models.Sequential()model.add(tf.keras.layers.Dense(1, activation = None, use_bias = True)) tf.keras.layers.Dense() 上面的例子中我们给 model 添加了一个 tf.keras.layers.Dense(), 它代表最典型的全连接神经网络层，即每个输入节点连接到每个输出节点。 123456789101112tf.keras.layers.Dense( units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Arguments: units: 正整数，输出空间的维数。 activation：要使用的激活功能。如果未指定任何内容，则不应用激活（即 “linear” activation：a（x）= x）。 use_bias：Boolean，该层是否使用bias vector。 …其他看文档: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense models.Sequential().compile() 给 model 添加完神经层后，必须进行 model.compile() 才能继续后续的模型训练，并且在 model.compile 的时候需要指定 optimizer 和 loss。 12345678910model.compile( optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, distribute=None,) Arguments: optimizer：String（优化器名称）或优化器实例。请参阅tf.keras.optimizers。 loss：String（目标函数的名称）或目标函数。见tf.losses。 metrics：模型在训练和测试期间要评估的度量列表。通常，您将使用 metrics = ['accuracy']。 …其他看文档：https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential 完整例子 2. Multiple Layer Neural Network 3. Convolutional Neural Network 4. Recurrent Neural Network4.1 RNN 4.2 LSTM 5. Generative Adversarial Network Reference TensorFlow Tutorial：https://www.tensorflow.org/tutorials/ TensorFlow Guide：https://www.tensorflow.org/guide/","link":"/2019/06/12/Keras-notes/"},{"title":"Human Pose Estimation Literature Review","text":"【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。 2D Human Pose EstimationWhat is Human Pose Estimation? Human Pose Estimation 主要是在图像或视频中检测估计人体的一些关键点（例如，关节，五官等）的问题。它也可以被定义成在所有关节姿势的空间中搜索特定姿势的问题。 2D Pose Estimation - 从图像中估计2D姿态（关键点）坐标，即 2D pose (x,y) coordinates。 3D Pose Estimation - 估计出关键点的3D坐标，即Estimate a 3D pose (x,y,z) coordinates a RGB image. 3D mesh (shap) estimation - 从图像中估计object的shape，例如数据集DensePose。 应用 - Human Pose Estimation 有很多应用，主要被用于 Action recognition, Animation, Gaming, Gait recognition 等等。具体的应用场景集中在在智能视频监控，病人监护系统，人机交互，虚拟现实，人体动画，智能家居，智能安防，运动员辅助训练等等。 例如：HomeCourt 使用 Pose Estimation 去分析篮球运动员的运动。 Why is it hard? - 由于人体具有相当的柔性，会出现各种姿态和形状，人体任何一个部位的微小变化都会产生一种新的姿态，同时其关键点的可见性受穿着、姿态、视角等影响非常大，而且还面临着遮挡、光照、雾等环境的影响，除此之外，2D人体关键点和3D人体关键点在视觉上会有明显的差异，身体不同部位都会有视觉上缩短的效果（foreshortening），使得人体骨骼关键点检测成为计算机视觉领域中一个极具挑战性的课题。 Relevant Datasets2D Datasets LSP（Leeds Sports Pose Dataset） - 单人人体关键点检测数据集，关键点个数为14，样本数2K，在目前的研究中基本上被弃用；[url] FLIC（Frames Labeled In Cinema） - 单人人体关键点检测数据集，关键点个数为9，样本数2W，在目前的研究中基本上被弃用；[url] MPII Human Pose dataset - 单人/多人人体关键点检测数据集，关键点个数为16，样本数25K，40K People，410 human activities，全身。 [url] MSCOCO - 多人，全身数据集。关键点个数为17，样本数多于300K，目前的相关研究基本上还需要在该数据集上进行验证；[url] AI Challenger - 多人，全身数据集。关键点：17，样本数约380K（210K Training, 30K Validation, 30K Testing）.[url] PoseTrack[url] - 多人，全身视频数据集。关键点：15，主要用于多人姿态估计和姿态追（Multi-person Pose Tracking）,数据集: $&gt;$ 1356 video sequences， $&gt;$ 46K annotated video frames $&gt;$ 276K body pose annotations VGG Human Pose Estimation datasets - 单人，上半身视频数据集。[url] 3D Datasets Human 3.6M - 室内场景，采集人数11人，3D human shape 数据集，3.6 M 图像。[url] Download: Google Drive Related Repo: https://github.com/mks0601/3DMPPE_POSENET_RELEASE DensePose - 用于3D shape研究的数据集。[url]；长线来讲，3D shape 估计是非常有价值的研究方向。 Why is it hard?Strong articulations, small and barely visible joints, occlusions, clothing, and lighting changes make this a difficult problem.","link":"/2019/12/31/Human-Pose-Estimation-Review/"},{"title":"Numpy&amp;Pandas Tutorial","text":"Numpy和Pandas对python中的数据处理很重要。尤其对于数据分析/挖掘，Pandas几乎不可或缺。写tutorial的起因是因为一次面试中被问到numpy中去重用哪个函数，发现自己对numpy的不熟悉，所以希望以此加深印象…(haven’t started yet) 1. NumpyNumpy Cheat Sheet Numpy Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF. 2. PandasPandas Cheat Sheet Pandas Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF.","link":"/2019/05/10/Numpy-Pandas-Tutorial/"},{"title":"DenseNet — Dense卷积网络（图像分类）","text":"【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。 作者 | Sik-Ho Tsang (原文：Review: DenseNet — Dense Convolutional Network (Image Classification))编译 | Xiaowen 该译文转载自：https://mp.weixin.qq.com/s/F2GRDen0v2zbnHlB-xHioA 目录 Dense Block DenseNet 结构 DenseNet 的优势 CIFAR &amp; SVHN 小规模数据集结果 ImageNet 大规模数据集结果 特征复用的进一步分析 1. Dense Block 在 andard ConvNet 中，输入图像经过多次卷积，得到高层次特征。 在 ResNet 中，提出了恒等映射（ identity mapping ）来促进梯度传播，同时使用使用 Element-wise addition 。它可以看作是将状态从一个 ResNet 模块传递到另一个 ResNet 模块的算法。 (It can be viewed as algorithms with a state passed from one ResNet module to another one.) 在 DenseNet 中，每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联(Concatenation)方式，每一层都在接受来自前几层的“集体知识（collective knowledge）”。(Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.) 由于每个层从前面的所有层接收特征映射，所以网络可以更薄、更紧凑，即信道数可以更少。增长速率k是每个层的附加信道数。 因此，它具有较高的计算效率和存储效率。下图显示了前向传播中级联的概念： 2. DenseNet 结构2.1 基础 DenseNet 组成层 对于每个组成层使用 Pre-Activation Batch Norm (BN) 和 ReLU，然后用k通道的输出特征映射进行 $\\boldsymbol{3 \\times 3}$ 卷积，例如，将$x_0$、$x_1$、$x_2$、$x_3$ 转换为 $x_4$。这是 Pre-Activation ResNet 的想法。 2.2 DenseNet-B (Bottleneck 层) 为了降低模型的复杂度和规模，在 BN-ReLU-3×3 conv 之前进行了 BN-ReLU-1×1 conv . 2.3 具有转换层（transition layer）的多Dense块 采用1×1 Conv和2×2平均池化作为相邻 dense block 之间的转换层。 特征映射大小在 dense block 中是相同的，因此它们可以很容易地连接在一起。 在最后一个 dense block 的末尾，执行一个全局平均池化，然后附加一个 Softmax 分类器。 2.4 DenseNet-BC (进一步压缩) 如果 Dense Block 包含 $m$ 个特征映射，则转换层（transition layer）生成 $\\theta_m$ 输出特征映射，其中 $0 &lt; \\theta \\leq 1$ 称为压缩因子。 当 $\\theta = 1$ 时，跨转换层的特征映射数保持不变。$\\boldsymbol{\\theta &lt; 1}$ 的 DenseNet 称为 DenseNet-C，在实验中采用 $\\theta = 0.5$ 。 当同时使用 bottleneck 和 $\\boldsymbol{\\theta &lt; 1}$ 时的转换层时，该模型称为 DenseNet-BC 模型。 最后，训练 with/without B/C 和不同 L 层 和 k growth rate 的 DenseNet。 3. DenseNet 的优势3.1 Strong Gradient Flow 误差信号可以更直接地传播到早期的层中。这是一种隐含的深度监督，因为早期的层可以从最终的分类层直接获得监督。 3.2 Parameter &amp; Computational Efficiency 对于每个层，RetNet 中的参数与c×c成正比，而 DenseNet 中的参数与1×k×k成正比。 由于 k &lt;&lt; C, 所以 DenseNet 比 ResNet 的size更小。 3.3 More Diversified Features 由于 DenseNet 中的每一层都接收前面的所有层作为输入，因此特征更加多样化，并且倾向于有更丰富的模式。 3.4 Maintains Low Complexity Features 在标准ConvNet中，分类器使用最复杂的特征。 在 DenseNet 中，分类器使用所有复杂级别的特征。它倾向于给出更平滑的决策边界。它还解释了为什么 DenseNet 在训练数据不足时表现良好。 4. CIFAR &amp; SVHN 小规模数据集结果4.1 CIFAR-10 Pre-Activation ResNet is used in detailed comparison. 数据增强（C10+），测试误差： Small-size ResNet-110: 6.41% Large-size ResNet-1001 (10.2M parameters): 4.62% State-of-the-art (SOTA) 4.2% Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 4.5% Large-size DenseNet (L=250, k=24): 3.6% 无数据增强（C10），测试误差： Small-size ResNet-110: 11.26% Large-size ResNet-1001 (10.2M parameters): 10.56% State-of-the-art (SOTA) 7.3% Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 5.9% Large-size DenseNet (L=250, k=24): 4.2% 在 Pre-Activation ResNet 中出现严重的过拟合，而 DenseNet 在训练数据不足时表现良好，因为DenseNet 使用了复杂的特征。 左：DenseNet-BC 获得最佳效果。 中：Pre-Activation ResNet 已经比 alexnet 和 vggnet 获得更少的参数，DenseNet-BC(k=12)的参数比 Pre-Activation ResNet 少3×10，测试误差相同。 右：与 Pre-Activation ResNet-1001有10.2m 参数相比，0.8 参数的 DenseNet-BC-100 具有相似的测试误差。 4.2 CIFAR-100CIFAR-100类似的趋势如下： 4.3 Detailed Results SVHN 是街景房屋编号的数据集。蓝色代表最好的效果。DenseNet-BC 不能得到比基本 DenseNet 更好的结果，作者认为 SVHN 是一项相对容易的任务，非常深的模型可能会过拟合。 5. ImageNet 大规模数据集结果 左：20M参数的DenseNet-201与大于40M参数的ResNet-101产生类似的验证错误。 右：相似的计算次数趋势(GOLOPS)。 底部：DenseNet-264(k=48)最高误差为20.27%，前5误差为5.17%。 6. 特征复用的进一步分析 从非常早期的层中提取的特征被同一 Dense Block 中的较深层直接使用。 转换层的权重也分布在前面的所有层中。 第二和第三 dense block 内的各层一贯地将最小权重分配给转换层的输出。(第一行) 在最终分类层，权重似乎集中在最终 feature map 上。一些更高级的特性在网络中产生得很晚。 Reference[2017 CVPR] [DenseNet]Densely Connected Convolutional Networks 原文链接：https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","link":"/2019/07/21/densenet/"},{"title":"统计学习 - Statistical Learning","text":"统计学习方法笔记总结。haven’t finished yet 1. k近邻法（k-Nearest Neighbors） 直观理解: 分类：在数据中找到与某个点（目标）最近的k个点，把该点（目标）的类分为k个点中多数的类。 回归：在数据中找到与某个点（目标）最近的k个点，k个点的均值为目标点的预测值。 优点： $k$ 近邻法是个非参数学习算法，它没有任何参数（ $k$ 是超参数，而不是需要学习的参数）。 近邻模型具有非常高的容量，这使得它在训练样本数量较大时能获得较高的精度。 缺点： 计算成本很高。因为需要构建一个 $N \\times N$ 的距离矩阵，其计算量为 $O(N^2)$，其中 $N$ 为训练样本的数量。 当数据集是几十亿个样本时，计算量是不可接受的。 在训练集较小时，泛化能力很差，非常容易陷入过拟合。 无法判断特征的重要性。 1.1 k近邻模型 模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间 一般是$n$维实数向量空间$\\mathbb{R}^d$。使用的距离是欧氏距离，但也可以是其他距离，如更一般的$L_p$距离（$L_p$ distance）或Minkowski距离（Minkowski distance）。 $$ L_p(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^p \\right)^{1/p}\\ , \\quad \\quad p \\geqslant 1\\\\ \\overrightarrow{\\boldsymbol{x}_i}, \\overrightarrow{\\boldsymbol{x}_j} \\in \\mathcal{X}, \\quad \\overrightarrow{\\boldsymbol{x}_i} = (x_i^{(1)}, x_i^{(2)}, \\dots, x_i^{(d)})^T ,\\quad {\\boldsymbol{x}_j} = (x_j^{(1)}, x_j^{(2)}, \\dots, x_j^{(d)})^T $$ 当 $p＝2$ 时，称为欧氏距离(Euclidean distance)：$L_2(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^2 \\right)^{1/2}$ 当 $p＝1$ 时，称为曼哈顿距离(Manhattan distance)：$L_1(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|$ 当 $p＝\\infty$ 时，它是各个坐标距离的最大：$L_{\\infty}(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\underset{i}{\\text{max}}|x_{i,l} - x_{j,l}|$ 不同的距离度量所确定的最近邻点是不同的。 k值的选择 k值的选择会对k近邻法的结果产生重大影响。 较小的k值：k值的减小就意味着整体模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，“学习”的近似 误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对 预测结果起作用。但是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。 优点：减少”学习”的偏差。 缺点：增大”学习”的方差（即波动较大）。 较大的k值：k值的增大就意味着整体的模型变得简单。相当于用较大邻域中的训练实例进行预测。其优点是可以减 少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似 的）训练实例也会对预测起作用，使预测发生错误。 优点：减少”学习”的方差（即波动较小）。 缺点：增大”学习”的偏差。 应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则 分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。也可以基于距离的远近进行加权投票。 多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为 $$ \\mathcal{f} : \\mathbb{R}^d \\rightarrow \\{c_1, c_2, \\dots, c_K \\} $$ 对给定的实例$\\overrightarrow{x} \\in \\mathcal{X}$，其最近邻的k个训练实例点构成集合$\\mathcal{N}_k (\\overrightarrow{x})$。如果涵盖$\\mathcal{N}_k (\\overrightarrow{x})$的区域的类别是cj，那么误分类率是 $$ L = \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i \\neq c_m) = 1 - \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m) $$ $L$ 就是训练数据的经验风险。要使经验风险最小，则使得 $\\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$ 最大。即多数表决：$c_m = \\underset{c_m}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$。所以多数表决规则等价于经验风险最小化。 1.2 k近邻算法 k近邻的分类算法： 输入：训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$ 为实例的特征向量，$y_i \\in \\overrightarrow{\\boldsymbol{y}} = \\left\\{ c_1, c_2, \\dots, c_k \\right\\} $为实例的类别，$i = 1, 2, \\dots, n$ 。 输出： 实例 $\\overrightarrow{\\boldsymbol{x}}$ 所属的类 y 。 步骤： 根据给定的距离度量，在训练集 $D$ 中找出与 $\\overrightarrow{\\boldsymbol{x}}$ 最邻近的k个点，涵盖这k个点的 $\\overrightarrow{\\boldsymbol{x}}$ 的邻域记作 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ ; 在 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ 中根据分类决策规则（如多数表决）决定 $\\overrightarrow{\\boldsymbol{x}}$ 的类别 $y$ ： $$ y = \\underset{c_j}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_j), \\quad i = 1, 2, \\dots, n; \\quad j = 1, 2, \\dots, k $$ $I$ 为指示函数，即当$y_i＝c_j$ 时 $I$ 为1，否则 $I$ 为0。 2. 朴素贝叶斯 直观理解: 朴素贝叶斯和贝叶斯分类（回归）器都是基于贝叶斯理论进行预测或者分类的模型。过程是利用训练数据学习 $P(X|Y)$ 和 $P(Y)$ 的估计，得到联合概率分布 $P(X,Y)＝P(Y)P(X|Y)$ ，然后利用贝叶斯定理进行预测，将输入 $\\overrightarrow{\\boldsymbol{x}}$ 分到后验概率最大的类。 $$ P(Y|X) = \\frac{P(X, Y)}{P(X)} = \\frac{P(Y)P(X|Y)}{\\sum_{Y}P(Y)P(X|Y)} \\propto P(Y)P(X|Y) \\\\ \\hat{y} = \\underset{c_k}{\\text{argmax }}P(Y=c_k) \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) $$ 条件独立假设: $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}}) & = \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) \\end{aligned} $$ 优点： 性能相当好，它速度快，可以避免维度灾难。 支持大规模数据的并行学习，且天然的支持增量学习。 缺点： 无法给出分类概率，因此难以应用于需要分类概率的场景。 2.1 贝叶斯定理 全概率公式 (Law of Total Probability Theorem)： $$ P(A) = \\sum_{j=1}^{n}P(A|B_j)P(B_j) $$ 贝叶斯定理 (Bayes’ theorem) 在数学上表示为以下等式： $$ \\begin{aligned} P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{\\sum_{j=1}^{d}P(B|A_j)P(A_j)} \\propto P(B|A) \\cdot P(A) \\end{aligned} $$ 2.2 朴素贝叶斯朴素贝叶斯（naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。 2.2.1 基本方法及解释2.2.1.1 基本方法 设输入空间 $X \\subseteq R_n$ 为 $d$ 维向量的集合，输出空间为类标记集合 $y ＝{c_1, c_2, \\dots, c_K}$。输入为特征向量$\\overrightarrow{\\boldsymbol{x}} \\in X$ ，输出为类标记（class label）$y \\in Y $ 。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布，训练数据集 $D$ 由 $P(X,Y)$ 独立同分布产生。 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 步骤： 学习以下先验概率分布及条件概率分布。 $$ \\begin{aligned} \\text{(先验概率)} & \\quad \\quad P(Y=c_k), \\quad \\ k=1,2,\\dots, k\\\\ \\text{(条件概率)} & \\quad \\quad P(X = \\overrightarrow{\\boldsymbol{x}} | P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\end{aligned} $$ 条件独立性的假设： $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k) & = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\\\ & = \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ 条件独立假设等于 是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。 朴素贝叶斯法分类时，对给定的输入 $\\overrightarrow{\\boldsymbol{x}}$ ，通过学习到的模型计算后验概率分布 $P(Y＝ c_k|X＝\\overrightarrow{\\boldsymbol{x}})$。 $$ \\begin{aligned} P(Y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) & = \\frac{P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}{\\sum_{k} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}\\\\ \\text{(独立性假设)} & = \\frac{P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}{\\sum_{k} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}, \\quad k = 1,2,\\dots,K \\end{aligned} $$ 将后验概率最大的类作为x的类输出: $$ \\begin{aligned} \\hat{y} = f(\\overrightarrow{\\boldsymbol{x}}) = \\underset{c_k}{\\text{argmax }} P(Y = c_k) \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ Note：因为后验概率的分母都相同，因此在这可以忽略。 2.2.1.2 后验概率最大的含义等价于期望风险最小化 (Optional) 假设选择 0-1损失函数： $$ \\begin{aligned} L(Y, f(X)) = \\begin{cases} 0, \\quad \\ Y = f(X)\\\\ 1, \\quad \\ Y \\neq f(x) \\end{cases} \\end{aligned} $$ 期望风险函数 $$ \\begin{aligned} R_{exp}(f) = \\mathbb{E} \\left[ L(Y, f(X)) \\right] = \\sum_{\\overrightarrow{\\boldsymbol{x}}}\\sum{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(\\overrightarrow{\\boldsymbol{x}}, y) = \\mathbb{E}_X \\left[ \\sum_{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(y | \\overrightarrow{\\boldsymbol{x}}) \\right] \\end{aligned} $$ 为了使期望风险最小化，只需对 $\\overrightarrow{\\boldsymbol{x}}$ 逐个极小化 $$ \\begin{aligned} f(\\overrightarrow{\\boldsymbol{x}}) & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} L(c_k, y) P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} P(y \\neq c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} (1 - P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}))\\\\ & = \\underset{y \\in Y}{\\text{argmax}} \\sum_{k} P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) \\end{aligned} $$ 2.2.1.3 参数估计 - 极大似然估计朴素贝叶斯中可以利用极大似然估计先验概率和条件概率。 $$ \\begin{aligned} P(Y = c_k) \\frac{1}{n} \\sum_{i=1}^{n} I(y_i = c_k)\\\\ P(X^{(j)} = a_{jl} \\ |\\ Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} \\ |\\ y_i = c_k \\right)}{\\sum_{i=1}^{n} I(y_i=c_k)}\\\\ j = 1, 2, \\dots, d; \\quad l = 1, 2, \\dots, S_j; \\quad k = 1, 2, \\dots, K \\end{aligned} $$ 其中，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征；$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值；$I$ 为指示函数。 2.2.1.4 贝叶斯估计用极大似然估计可能会出现所要估计的概率值为0的情况。解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计（or also known as add-alpha smoothing）是 $$ \\begin{aligned} P_{\\alpha}(X^{(j)} = a_{jl} | Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} | y_i = c_k \\right) + \\alpha}{\\sum_{i=1}^{n} I(y_i=c_k) + S_j \\alpha}\\\\ \\end{aligned} $$ 其中，$\\alpha \\geqslant 0$ 当 $\\alpha = 0$ 为极大似然估计，当 $\\alpha = 1$为拉普拉斯平滑（Laplace smoothing, also known as add-one smoothing）。 2.2.2 算法 朴素贝叶斯算法（naïve Bayes algorithm） 利用极大使然估计先验概率 $P(Y = c_k)$ 和条件概率 $P(X^{(j)} = a_{jl} \\ |\\ Y = c_k)$，如Sec 2.2.1.2 对于给定的实例 $\\overrightarrow{\\boldsymbol{x}}＝(x^{(1)},x^{(2)},…,x^{(d)})^T$ ，计算 $$ \\begin{aligned} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k), \\quad \\ k = 1, 2, \\dots, K \\end{aligned} $$ 确定实例 $\\overrightarrow{\\boldsymbol{x}}$ 的类 $$ \\hat{y} = \\underset{c_k}{\\text{argmax}} \\ P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k) $$ 例子： 3. 支持向量机 直观理解: 支持向量机（support vector machines，SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习算法是求解凸二次规划的最优化算法。 间隔最大使它有别于感知机； 感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 支持向量机还支持核技巧，从而使它成为实质上的非线性分类器。 支持向量机学习方法包含构建由简至繁的模型： 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机； 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机； 非线性支持向量机；当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机 特征向量之间的内积就是核函数，使用核函数可以学习非线性支持向量机。 非线性支持向量机等价于隐式的在高维的特征空间中学习线性支持向量机，这种方法称作核技巧。 3.1 线性可分支持向量机 假设给定一个特征空间上的训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$, $y_i \\in {-1, +1}, \\; i=1,2,\\dots,n$ .$\\overrightarrow{\\boldsymbol{x}}_i$ 为第 $i$ 个(d维)特征向量，也称为实例，$(x_i，\\ y_i)$ 称为样本点。 假设数据集线性可分。则学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$\\boldsymbol{W}_{k \\times d} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}}$，它由法向量 $\\boldsymbol{W}$ 和截距 $\\overrightarrow{\\boldsymbol{b}}$ 决定，可用 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 来表示。例如： 给定线性可分训练数据集，通过间隔最大化或等 价地求解相应的凸二次规划问题学习得到的分离超平面为 $\\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} = 0$ 以及相应的分类决策函数 $f(\\overrightarrow{\\boldsymbol{x}}) = sign( \\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} )$ 3.1.1 函数间隔 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i)$ 的函数间隔为 $$\\hat{\\gamma}_i = y_i (\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的函数间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 函数间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 可以将一个点距离分离超平面的远近来表示分类预测的可靠程度： 一个点距离分离超平面越远，则该点的分类越可靠。 一个点距离分离超平面越近，则该点的分类则不那么确信。 在超平面 $\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}} = 0$ 确定的情况下： $| \\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}}| $ 能够相对地表示点 $\\overrightarrow{\\boldsymbol{x}}_i$ 距离超平面的远近。 3.1.2 几何间隔 因为只要成比例地改变 $\\boldsymbol{W}$ 和 $b$ ，例如将它们改为 $2w$ 和 $2b$ ，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量 $\\boldsymbol{W}$ 加某些约束，如规范化，$||\\boldsymbol{W}||＝ 1$ ，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i$ 的几何间隔为 $$\\hat{\\gamma}_i = y_i (\\frac{\\boldsymbol{W}}{||\\boldsymbol{W}||} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\frac{\\overrightarrow{\\boldsymbol{b}}}{||\\boldsymbol{W}||} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的几何间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 几何间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 由定义可知函数间隔和几何间隔有下列的关系：$$\\gamma_i = \\frac{\\hat{\\gamma}_i}{||W||}; \\quad \\gamma = \\frac{\\hat{\\gamma}}{||W||}$$ 当 $||\\boldsymbol{W}|| = 1$ 时，函数间隔和几何间隔相等。 如果超平面参数 $\\boldsymbol{W}$ 和 $\\overrightarrow{\\boldsymbol{b}}$ 成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 3.1.3 硬间隔最大化 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。 几何间隔最大化的物理意义：不仅将正负实例点分开，而且对于最难分辨的实例点（距离超平面最近的那些点），也有足够大的确信度来将它们分开。 求解一个几何间隔最大的分离超平面的问题可以表示为下面的约束最优化问题： $$ \\begin{aligned} & \\underset{\\boldsymbol{W}, \\overrightarrow{\\boldsymbol{b}}}{\\text{max}} & & \\gamma \\\\ & \\text{s.t.} & & y_i (\\frac{\\boldsymbol{W}}{||\\boldsymbol{W}||} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\frac {\\overrightarrow{\\boldsymbol{b}}}{||\\boldsymbol{W}||} ) \\geqslant \\gamma ; \\quad i=1,2,\\dots,n \\end{aligned} $$ 考虑几何间隔和函数间隔的关系式（7.8），可将这个问题改写为 $$ \\begin{aligned} & \\underset{\\boldsymbol{W}, \\overrightarrow{\\boldsymbol{b}}}{\\text{max}} & & \\frac{\\hat{\\gamma}}{||\\boldsymbol{W}||} \\\\ & \\text{s.t.} & & y_i (\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}} ) \\geqslant \\hat{\\gamma} ; \\quad i=1,2,\\dots,n \\end{aligned} $$ Reference 李航. 统计学习方法[M]. 清华大学出版社， 2012年3月. Ai算法工程师手册","link":"/2019/05/31/StatisticalLearning/"},{"title":"OpenCV - Python api 笔记","text":"Opencv-python是OpenCv的python API，包括数百种计算机视觉算法。这个页面记录了一些常用的Opencv-python函数，以便作为我的快速参考。 1. 安装和使用 Installation and Usage 安装 1pip install opencv-python 事实上一共有四种不同的packages，安装其中一个即可，四个packages都用同一个名字cv2（对于其他的packages，详见Documentation）。 2. OpenCV中的GUI特性2.1 图像基本操作(读取，显示，保存) 三个函数cv.imread(), cv.imshow(), cv.imwrite()分别用于读取；显示和保存图像。 cv.imread()：读取图像 1retval = cv.imread(filename[, flags]) 参数 Parameters: filename: 文件名 (Name of file to be loaded.) flags: 定义读取图片的方式，如彩色或灰色等 (takes values of cv::ImreadModes specified the way image should be read.){cv::IMREAD_UNCHANGED = -1,cv::IMREAD_GRAYSCALE = 0,cv::IMREAD_COLOR = 1,cv::IMREAD_ANYDEPTH = 2,cv::IMREAD_ANYCOLOR = 4,cv::IMREAD_LOAD_GDAL = 8,…} 例子 Example： 1234import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0) cv.imshow()：显示图像 1None = cv.imshow(winname, mat) 参数 parameters: winname：显示窗的名字 (Name of the window.) mat： 要显示的图像名 (Image to be shown.) 例子 Example: 1cv.imshow('image',img) cv.imwrite()：保存图像 1retval = cv.imwrite(filename, img[, params]) 参数 Parameters： filename：文件名（Name of the file.） img：要保存的图像名（Image to be saved.） params：Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, … .) see cv::ImwriteFlags 例子 Example： 1cv.imwrite('messigray.png',img) 2.2 色彩通道转换(Color conversions)：RGB - Gray RGB ↔ GRAY 除了RGB channel到灰度Y channel的转换，还有其他通道的转换，例如RGB ↔ CIE XYZ.Rec 709 with D65 white point； RGB ↔ YCrCb JPEG (or YCC)等等，详见Documentation。 RGB空间内的变换，例如添加/删除Alpha通道，反转通道顺序，转换为16位RGB颜色（R5：G6：B5或R5：G5：B5），以及转换为灰度/从灰度转换使用以下方式： $$ \\text{RGB[A] to Gray: Y←0.299⋅R+0.587⋅G+0.114⋅B} $$ and $$ \\text{Gray to RGB[A]: R←Y,G←Y,B←Y,A←max(}ChannelRange\\text{)} $$ 1cvtColor(src, dst, code, dstCn = 0) 参数 Parameters： src：输入图像 (input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC… ), or single-precision floating-point.) dst：输出图像 (output image of the same size and depth as src.) code：转换方式代码 (color space conversion code (see cv::ColorConversionCodes).){cv2.COLOR_RGB2GRAY,cv2.COLOR_GRAY2RGB,cv2.COLOR_BGR2GRAY,cv2.COLOR_RGB2GRAY,…} dstCn：目标图像中的通道数;如果参数为0，则从src和代码自动导出通道数 (number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code.) 例子 Examples： 123456import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0)# Convert image from Y channel to RGB channelimg2 = cv.cvtColor(img, cv.COLOR_GRAY2BGR) Additional ReadingOpenCV官方教程中文版（For Python），段力辉 译（搬运自：https://www.linuxidc.com/Linux/2015-08/121400.htm）","link":"/2019/05/12/OpenCV-Cheat-Sheet/"},{"title":"DeepMedic - multi-sacle 3D CNN with CRF for brain lesion segmentation","text":"论文阅读笔记，如果我有什么理解错误的地方，欢迎大家指正。论文：Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation 摘要：作者提出一种双路径，11层深的3D卷积神经网络用于brain lesion segmentation任务，主要解决了医学图像处理上的三个方面的问题。一是作者设计了一种dense training scheme的方案，采用全卷积神经网络，一次把相邻像素的image segments传入网络输出dense prediction (dense-inference)，从而节省了计算代价。作者使用更深的网络使模型判别能力更强。作者使用一种dual pathway architecture，即同时训练两个网络，使模型同时对高/低分辨率图像进行处理。最后作者采用全连接条件随机场(Fully connected Conditional Random Field)对网络output进行后处理(post-processing)改善图像类之间的边缘信息。 1. Dense Training在传统patch-wise的分类中，输入patch的尺寸和cnn最后一层神经元的感受野大小相同，这样网络得到一个single prediction对应输入patch的中心像素的值。而用全卷积实现的神经网络，因为其输入的patch的尺寸可以大于最后一层神经元的感受野，因此模型可以同时输出多个prediction，即dense-inference，而每一个prediction对应cnn’s receptive field的在输入patch上的每一步stride。 同时作者认为，这样dense-inference得到的每一个prediction都是可信的，只要感受野是完全在input patch上扫过，并且只捕捉到原始信息，因此没有使用padding。 Dense Training Scheme 的优点 节省计算代价和内存消耗 灵活：作者提到最佳的性能为将整个图像送入网络，但是这个做法不现实。如果GPU内存限制不允许，例如在需要缓存大量FM的大型3D网络的情况下，则将图像分成多个image-segments，这样会比单个segment大，但是可以去fit内存。（原文：If GPU memory constraints do not allow it, such as in the case of large 3D networks where a large number of FMs need to be cached, the volume is tiled in multiple image-segments, which are larger than individual patches, but small enough to fit into memory.） CNNs are trained patch-by-patch: 个人认为论文中提到的这个操作是在图像上randomly crop出一个individual patch作为网络的输入来训练网络，以此计算loss和进行gradient descent。 2. Class Balance(这里不太确定自己理解对不对) 在原文的Section 2.2中作者提到，这种dense training scheme的方案中的sampling input segments (即上面提到的randomly crop individual patches) 提供了一种灵活的方式去平衡training samples中的segmentation classes（正负类）的分布，而不同的分布对模型的性能有很大的影响。 从上图和原文Section 3.2的实验结果来看，感觉有点类似过采样的意思。上图说明，如果是crop一个以病变为中心 (lesion-centered) 的image segments，分别用 $7 \\times 7$ 和 $9 \\times 9$ 的框去crop下来，可以看到用 $9 \\times 9$ 的框去crop出这个image segment的话，绿色的内容会相对更多一点。 作者在Section 3.2的最后一段也提到，这个segments size在模型中是一个超参数，提到segment size会提升模型的性能，但是很快这种性能的提升久会达到平稳(level off)，并且在一系列的segment size中都会得到相似的性能。 3. 构建更深的网络 该部分总结：Deeper Networks + Smaller Kernel + Batch Norm + (p)ReLU + $ \\mathcal{N}\\left(0,\\ \\sqrt{2/n^{in}_{l}} \\right)$ 更深的网络有更好的判别能力，但是更深的网络同时意味着更高的计算代价和更吃内存。所以在这里作者采用的策略是层数增加，但是把kernel size从 $5^3$ 缩小到 $3^3$ 从而减少计算代价和节省内存。作者认为这里kernel size减小说明需要训练的parameters的数量减少，一定程度上有正则的效果。（但是原文好像只是给了个简单的 $\\frac{5^3}{3^3} \\approx 4.6$，但是层数的增加会增加参数的数量，不知道作者有没有算对，我也没去算它….） 因为更深的网络变得容易train不下去，所以作者采用了 ReLU-based 的网络 (在github文件看，DeepMedic的网络好像采用的是pReLU)。同时，作者不采用标准正态分布去初始化kernel weights，才是采用了 $ \\mathcal{N}\\left(0,\\ \\sqrt{2/n^{in}_{l}} \\right)$ 。 Batch Normalisation 4. Dual Pathways Networks 作者为上述deeper networks增加了第二个pathway networks，对down-sampling的图像进行操作。这种dual pathways 3D CNN 同时对高/低分辨率图像进行训练，文中提到第二个网络(低分辨率)用于捕捉一些high level的信息，例如 location。而第一个网络（高分辨率）用于捕捉一些细节信息。为了使得最后输出的feature maps尺寸一致，要对第二个网络的输出进行上采样以匹配第一个网络输出的feature maps的尺寸。 5. 3D fully connected CRF作者认为CNN网络输出的结果偏向与smooth（如何smooth可以见下图），因此对DeepMedic网络（11层，dual pathways）网络的输出认为是soft segmentation。最后作者用CRF对soft segmentations 进行后处理（post-processing）改善其边缘信息。 5.1 为什么CRF可以用于处理图像任务？首先对于图像任务，我们可以把每个像素认为是一个单独的节点（node），像素与像素之间就构成了边（edge），同时，相邻像素之间相互影响，而这种影响是对称的，相当于edge，因此构成了一个概率无向图。 5.2 全连接条件随机场对于每个像素 $i$ 具有类别标签 $x_i$ 还有对应的观测值 $y_i$，这样每个像素点作为节点，像素与像素间的关系作为边，即构成了一个条件随机场。而且我们通过观测变量 $y_i$ 来推测像素 $i$ 对应的类别标签 $x_i$。条件随机场如下： 条件随机场符合吉布斯分布：(此处的 $x$ 即上面说的观测值) $$P(\\boldsymbol{X} = \\mathbf{x} | \\boldsymbol{I}) = \\frac{1}{Z(\\boldsymbol{I})} exp\\left( -\\mathbb{E}(\\mathbf{x}|\\boldsymbol{I}) \\right) $$ 其中的 $\\mathbb{E}(\\mathbf{x}|\\boldsymbol{I})$ 是能量函数，为了简便，以下省略全局观测 $\\boldsymbol{I}$： $$ E(\\mathbf{x})=\\sum_i{\\Psi_u(x_i)}+\\sum_{i,j}\\Psi_p(x_i, x_j) $$ 其中的一元势函数 $\\sum_i{\\Psi_u(x_i)}$ 即来自于前端FCN的输出。而二元势函数如下： $$\\Psi_p(x_i, x_j)=u(x_i, x_j)\\sum_{m=1}^M{\\omega^{(m)}k_G^{(m)}(\\boldsymbol{f_i, f_j)}}$$ 二元势函数就是描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片尽量在边界处分割。 而全连接条件随机场的不同就在于，二元势函数描述的是每一个像素与其他所有像素的关系，所以叫“全连接”。 Reference Harvard Referencing: Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon, D.K., Rueckert, D. and Glocker, B., 2017. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation. Medical image analysis, 36, pp.61-78. https://yq.aliyun.com/articles/232455 https://zhuanlan.zhihu.com/p/22308032","link":"/2019/07/18/deepmedic/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/04/18/hello-world/"},{"title":"Using Python with Docker","text":"Notes for running deep learning model with python inside Docker containers, and basic usage of Docker. 1. running Python (Deep Learning) with DockerA common headache in software projects is ensuring the correct versions of all dependencies are available on the current development system. Often you may be working on several distinct projects simultaneously each with its own potentially conflicting dependencies on external libraries. Additionally you may be working across multiple different machines (for example a personal laptop and University computers) with possibly different operating systems. Further, you may not have root-level access to a system you are working on and so not be able to install software at a system-wide level and system updates may cause library versions to be changed to incompatible versions. One way of overcoming these issues is to use project-specific virtual environments. In this context a virtual environment or machine are isolated development environments where the external dependencies of a project can be installed and managed independent of the system-wide versions (and those of the environments of other projects). Here, we introduce how to use Docker to create a virtual machine to run python 3. Running Docker containerNote: Make sure the Docker has been installed on your personal PC or other devices. The installation instructions can be found on the official documentation, for Windows, Mac, Ubuntu. Open a bash terminal. We first need to pull the latest nvidia/cuda docker image from the remote repo: 1docker pull nvidia/cuda we can use command docker search cuda to search docker images. Using docker images confirms whether the image was successfully pulled: 123456$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 549b9b86cb8d 3 weeks ago 64.2MBnvidia/cuda latest 9e47e9dfcb9a 5 weeks ago 2.83GBnginx latest 231d40e811cd 7 weeks ago 126MB Now, run the docker container with docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]: 1docker run -it -d --name VideoPose3D --runtime=nvidia --volume=\"$PWD:/app\" nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04 Here, the tag of the ‘nvidia/cuda’ image is 10.1-cudnn7-devel-ubuntu16.04 instead of latest. List running containers: 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES08f9ae536985 nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04 \"/bin/bash\" 3 hours ago Up 3 hours VideoPose3D Next, execute an interactive bash shell on the container. 1docker exec -it VideoPose3D bash This will create a new Bash session in the container VideoPose3D. If we did not assign a name to the container, we can exec the container with docker exec -it containerID bash. Now, we are done. We are already inside the Docker container. Then we install Miniconda. Installing Miniconda Because there is no necessary tools in the image, we need to install them first. Also, the image probably has no apt package cache, we need to run: 1apt-get update then, install wget command-line tool: 1apt-get install wget Download the latest 64-bit Python 3 Miniconda install script: 1wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh Now run the install script: 1bash Miniconda3-latest-Linux-x86_64.sh You will first be asked to review the software license agreement. Assuming you choose to agree, you will then be asked to choose an install location for Miniconda. The default is to install in the root of your home directory ~/miniconda3 or /root/miniconda3. We recommend going with this default unless you have a particular reason to do otherwise. Append the Miniconda binaries directory to PATH in manually in ~/.bash_env using 1echo \"export PATH=\\\"\"\\$PATH\":$HOME/miniconda3/bin\\\"\" &gt;&gt; ~/.bash_env For those who this appears a bit opaque to and want to know what is going on see here 1. We now need to source the updated ~/.bash_env so that the PATH variable in the current terminal session is updated: 1source ~/.bash_env From the next time you log in all future terminal sessions should have the updated PATH loaded by default. Creating the Conda environmentYou should now have a working Conda installation. If you run1conda --helpNote: from a terminal you should see the Conda help page displayed. If you get a No command 'conda' found error you should check you have set up your PATH variable correctly (you can get a demonstrator to help you do this). Assuming Conda is working, we will now create our Conda environment: 1conda create --name vp3d python=3 This bootstraps a new Conda environment named vp3d with a minimal Python 3 install. You will be presented with a ‘package plan’ listing the packages to be installed and asked whether to proceed: type y then enter. We will now activate our created environment: 1source activate vp3d When a environment is activated its name will be prepended on to the prompt which should now look something like(vp3d) [machine-name]:~$ on terminal. You need to run this source activate vp3d command every time you wish to activate the vp3d environment in a terminal. When the environment is activated, the environment will be searched first when running commands so that e.g. python will launch the Python interpreter installed locally in the vp3d environment rather than a system-wide version. We can now install the dependencies for the specific project into the new environment, like:1conda install numpy scipy matplotlib jupyter Once the installation is finished, to recover some disk space we can clear the package tarballs Conda just downloaded:1conda clean -t These tarballs are usually cached to allow quicker installation into additional environments however we will only be using a single environment here so there is no need to keep them on disk. [1] The echo command causes the following text to be streamed to an output (standard terminal output by default). Here we use the append redirection operator &gt;&gt; to redirect the echo output to a file ~/.bash_env, with it being appended to the end of the current file. The text actually added is export PATH=&quot;$PATH:[your-home-directory]/miniconda/bin&quot; with the \\&quot; being used to escape the quote characters. The export command defines system-wide environment variables (more rigorously those inherited by child shells) with PATH being the environment variable defining where bash searches for executables as a colon-seperated list of directories. Here we add the Miniconda binary directory to the end of the current PATH definition. ↩ 2. Docker Basic Logic Flow 3. Docker commandCheck docker version Usage: docker version Example: 1234567891011121314151617181920212223242526272829$ docker versionClient: Docker Engine - CommunityVersion: 19.03.5API version: 1.40Go version: go1.12.12Git commit: 633a0eaBuilt: Wed Nov 13 07:22:37 2019OS/Arch: windows/amd64Experimental: falseServer: Docker Engine - CommunityEngine:Version: 19.03.5API version: 1.40 (minimum version 1.12)Go version: go1.12.12Git commit: 633a0eaBuilt: Wed Nov 13 07:29:19 2019OS/Arch: linux/amd64Experimental: falsecontainerd:Version: v1.2.10GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339runc:Version: 1.0.0-rc8+devGitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657docker-init:Version: 0.18.0GitCommit: fec3683 Search Images Usage: Search the Docker Hub for images, [more info] 1docker search [OPTIONS] TERM Example: 1docker search debian Pull Images Usage: Pull an image or a repository from a registry, [more info] 1docker pull [OPTIONS] NAME[:TAG|@DIGEST] Example: 1docker pull debian List Images Usage: 1docker images [OPTIONS] [REPOSITORY[:TAG]] Example: running docker images. 123456$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 549b9b86cb8d 3 weeks ago 64.2MBnvidia/cuda latest 9e47e9dfcb9a 5 weeks ago 2.83GBnginx latest 231d40e811cd 7 weeks ago 126MB List Containers Usage: 1docker ps [options] Options: Name, shorthand Default Description –all , -a Show all containers (default shows just running) –last , -n -1 Show n last created containers (includes all states) … … Example: running docker ps 12345$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c01db0b339c ubuntu:12.04 bash 17 seconds ago Up 16 seconds 3300-3310/tcp webappd7886598dbe2 crosbymichael/redis:latest /redis-server --dir 33 minutes ago Up 33 minutes 6379/tcp redis,webapp/db Run Containers Usage: Run a command in a new container, [more info] 1docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...] Options: Name, shorthand Default Description –detach , -d Run container in background and print container ID –publish , -p Publish a container’s port(s) to the host –name Assign a name to the container –volume , -v Bind mount a volume –interactive , -i Keep STDIN open even if not attached –tty , -t Allocate a pseudo-TTY … … Example: 1docker run -it -d --name VideoPose3D --runtime=nvidia --volume=\"$PWD:/app\" nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04 Run a command in a running container Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG…], [more info] 1docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Options: Name, shorthand Default Description –detach , -d Run container in background and print container ID –interactive , -i Keep STDIN open even if not attached –tty , -t Allocate a pseudo-TTY … … Example: 1docker exec -it VideoPose3D bash This will create a new Bash session in the container VideoPose3D. If we did not assign a name to the container, we can exec the container with docker exec -it containerID bash.","link":"/2020/01/14/docker_usage_cheat_sheet/"},{"title":"大数据技术原理与应用 - (2). 大数据处理框架 Hadoop","text":"【第一篇】 - 大数据基础, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据 (Big Data) 的基本概念、影响、应用领域等，还介绍了大数据处理框架 Hadoop。 第1章 - 大数据概述 第2章 - 大数据处理框架 Hadoop 第二章介绍了 Hadoop 特性、应用现状，还介绍了 Hadoop 生态系统及其各个组件。 Hadoop 概述Hadoop 简介 Hadoop是Apache软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构 Hadoop是基于Java语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中 Hadoop的核心是分布式文件系统HDFS（Hadoop Distributed File System）和MapReduce Hadoop 特性 高可靠性。采用冗余数据存储方式，即使一个副本发生故障，其他副本也可以保证正常对外提供服务。 高效性。Hadoop 采用分布式存储和分布式处理两大核心技术，能够高效地处理BP级数据。 高可扩展性。Hadoop 的设计目标是可以高效稳定地运行在廉价的计算机集群上，可以扩展到数以千计的计算机节点上。 高容错性。采用荣誉存储方式，自动保存数据的多个副本，并且能够自动将失败的任务进行重新分配。 成本低。 （硬件上可以采用廉价的机器） 运行在Linux平台上。 支持多种编程语言 Hadoop 应用现状 其中 Mahout 提供了许多 Data Mining 的算法实现。 Hadoop 版本 Hadoop 1.0 到 2.0 的变化： 由 HDFS, MapReduce 和 YARN 三个分支构成 HDFS: NameNode Federation, High Availability (HA) MapReduce: 运行在 YARN 上的MR计算框架 Hadoop 1.0 中的问题： HDFS: NameNode 单点故障，难以应用于在线场景 NameNode 压力过大，内存受限，影响系统扩展性 MapReduce: JobTracker 访问压力过大，影响系统扩展性 难以支持除 MapReduce 外的计算框架，如 Spark, Storm 等 Hadoop 2.0 解决方案： HDFS 2.x: HDFS HA: 解决单点故障。采用主备 NameNode 策略，如果主 NameNode 发生故障，则切换到备 NameNode 上 (非热备份)。 NN Federation: 解决内存受限。能够水平扩展，支持多个 NameNode, 每个 NameNode 分管一部分目录，所有 NameNode 共享所有 DataNode 存储的数据。 MapReduce: 2.0后，MapReduce 只负责计算 资源调度分离出 YARN 来负责 Hadoop 各个发行版本选择因素： 是否开源 (即是否免费) 是否稳定版 是否经实践检验 是否有强大的社区支持 不同发行版本比较： 厂商名称 开放性 易用性 平台功能 性能 本地支持 总体评价 Apache 完全开源，Hadoop 托管在 Apache 社区里 安装: 2使用: 2维护: 2 Apache 是标准的 Hadoop 平台，其他都是在此基础上改进 2 无 2 Cloudera 与 Apache 功能同步，部分开源 安装: 5使用: 5维护: 5 有自主研发产品，如 impala, navigator 等 4.5 2014年进入中国, 上海 4.5 Hortonworks 与 Apache 功能同步，完全开源 安装: 4.5使用: 5维护: 5 是 Apache Hadoop 平台最大贡献者，如 Tez 4.5 无 4.5 MapR 在 Apache 基础上修改 安装: 4.5使用: 5维护: 5 在 Apache 平台上优化了很多，形成自身产品 5 无 3.5 星环 核心组件与 Apache 同步，底层优化多，完全封闭的平台 安装: 5使用: 4维护: 4 有自主 Hadoop 产品，如 Inceptor, Hyperbase 4 本地厂商 4 通常商业公司的版本性能会更好，也会更好用 学生通常使用开源、免费的 Apache Hadoop 中国国内公司的可能还需要考虑是否能获得本地支持，例如国内厂商: 星环 Hadoop 项目结构Hadoop的项目结构不断丰富发展，已经形成一个丰富的Hadoop生态系统。 表：各组件功能简介 组件 功能 HDFS 分布式文件系统 MapReduce 分布式并行编程模型 YARN 资源管理和调度器 Tez 运行在YARN之上的下一代Hadoop查询处理框架 Hive Hadoop上的数据仓库 HBase Hadoop上的非关系型的不是数据库 Pig 一个基于Hadoop的大规模数据分析平台，提供类似SQL的查询语言Pig Latin Sqoop 用于在Hadoop与传统数据库之间进行数据传递 Oozie Hadoop上的工作流管理系统 Zookeeper 提供分布式协调一致性服务 Storm 流计算框架 Flume 一个高可用，高可靠，分布式的海量日志采集、聚合和传输的系统 Ambari Hadoop快速部署工具，支持Apache Hadoop集群的供应、管理和监控 Kafka 一种高吞吐量的根不是发布订阅消息系统，可以处理消费者规模的网络的所有动作流数据 Spark 类似于Hadoop MapReduce的通用并行框架 HDFSHadoop 分布式文件系统 (Hadoop Distributed File System, HDFS) 是 Hadoop 项目的两大核心之一，是针对谷歌文件系统 (Google File System, GFS) 的开源实现。HDFS 具有处理超大数据、流式处理、可以运行在廉价商用服务器上等优点。 容错: HDFS 再设计之初就是考虑再廉价机器上运行，因此设计上把设备故障作为常态来考虑，可以保证部分硬件发生故障的情况下仍然能够保证文件系统的可用。 高吞吐率: 为有大量数据访问的应用提供高吞吐量的支持。 大文件存储: 支持存储TB-PB级别的数据。 适用场景： HDFS适合用于：大文件存储，流失数据访问。 HDFS不适合用于：大量小文件处理，随机写入，低延迟读写。 HBaseHBase 是一个提供高可靠性、可伸缩、实时读写、分布式的列式数据库，一般采用 HDFS 作为其底层数据存储。HBase 是针对谷歌 BigTable 的开源实现，二者采用相同的数据模型，具有强大的非结构化数据存储能力。 HBase 与传统数据库的区别： HBase 是基于列的存储 传统关系型数据库通常是基于行的存储 MapReduceHadoop MapReduce 是针对谷歌 MapReduce 的开源实现。MapReduce 是一种编程模型，用处大规模数据集（大于1TB）的并行运算，它将并行计算过程高度地抽象到两个函数 —— Map 和 Reduce 上。 核心思想 —— 分而治之 把输入的数据集切分为若干独立的数据块，分发给一个主节点管理下的各个分节点来共同并行完成；最后，通过整合各个节点的中间结果得到最终结果。 适用场景： MapReduce适合用于：大规模数据离线批处理，子任务相对独立。 MapReduce不适合用于：实时交互计算，流失计算、实时分析，子任务相互依赖。 HiveHive 是一个基于 Hadoop 的数据仓库工具，可以用于对 Hadoop 文件中的数据进行数据整理、特殊查询和分析存储。Hive 提供了类似关系型数据库 SQL 语言的查询语句 —— Hive QL，学习门槛较低。Hive 自身可以将 Hive QL 转换成 MapReduce 任务进行运行，不必开发专门的 MapReduce 应用。 Hive适合数据仓库的统计分析： 数据汇总：每天/每周用户点击数，点击排行。 非实时分析：日子分析，统计分析。 数据挖掘：用户行为分析，兴趣分析、区域展示。 PigPig 是一种数据流语言和运行环境，适合于使用 Hadoop 和 MapReduce 平台来查询大型半结构化数据集。 Pig 包含两部分： 用于描述数据流的语言: Pig Latin 用于运行 Pig Latin 程序的执行环境 。一个是本地的单 JVM 执行环境，一个就是在 Hadoop 集群上的分布式执行环境。 Pig与Hive的区别？Pig 与 Hive 作为一种高级数据语言，均运行于 HDFS 之上，是 Hadoop 上层的衍生架构，用于简化 Hadoop 任务，并对 MapReduce 进行一个更高层次的封装。Pig与Hive的区别如下： Pig是一种面向过程的数据流语言；Hive是一种数据仓库语言，并提供了完整的sql查询功能。 Pig更轻量级，执行效率更快，适用于实时分析；Hive适用于离线数据分析。 Hive查询语言为Hql，支持分区；Pig查询语言为Pig Latin，不支持分区。 Hive支持JDBC/ODBC；Pig不支持JDBC/ODBC。 Pig适用于半结构化数据(如：日志文件)；Hive适用于结构化数据。 MahoutMahout 是 Apache 软件基金会 (Apache Software Foundation, AFS) 旗下的一个开源项目，提供一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建智能应用程序。 Mahout 是一个算法库，提供了许多算法 api。 Mahout 包含许多算法实现，包括聚类、分类、推荐过滤、频繁子项挖掘。 通过使用 Apache Hadoop 库，Mahout 可以有效地扩展到Hadoop集群。 ZookeeperZookeeper 是针对谷歌 Chubby 的一个开源实现。是搞笑和可靠的协同工作系统，提供分布式锁之类的基本服务（如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务。 FlumeFlume 是 Cloudera 提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统。它将大批量的不同数据源的日志数据收集、聚合、移动到数据中心（HDFS）进行存储的系统。即 Flume 是实时采集日志的数据采集引擎。 Flume 特性： 可靠性。 当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为： End-to-end （收到数据 agent 首先将 event 写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送） Store on failure （这也是 scribe 采用的策略，当数据接收方 crash 时，将数据写到本地，待恢复后，继续发送） Best effort （数据发送到接收方后，不会进行确认） 可扩展性。Flume 采用了三层架构，分别为 agent, collector 和 storage，每一层均可以水平扩展。其中，所有 agent 和 collector 由 master 统一管理，这使得系统容易监控和维护，且 master 允许有多个（使用 ZooKeeper 进行管理和负载均衡），这就避免了单点故障问题。 Flume 的优势： 可以高速采集数据，采集的数据能够以想要的文件格式及压缩方式存储在 HDFS 上。 事务功能保证了数据在采集的过程中数据不丢失。 部分 Source 保证了 Flume 挂了以后重启依旧能够继续在上一次采集点采集数据，真正做到数据零丢失。 SqoopSqoop 是 SQL-to-Hadoop 的缩写，主要用来在 Hadoop 和传统关系型数据库之间交换数据，可以改进数据的互操作性。通过 Sqoop 可以实现关系型数据库（如 MySQL, Oracle, PostgreSQL 等）和 Hadoop 之间数据迁移 （即互相导入、导出）。 Reference[1] 大数据技术原理与应用 - 林子雨","link":"/2020/05/17/intro2BigData2/"},{"title":"活用 Bulma 美化 Icarus 文章","text":"Icarus 相信大家都很熟悉，但是有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以 Icarus 有哪些高级玩法呢？下面就让 iMaeGoo 带大家一起了解吧。（误 按钮 Info Success Warning Danger 点击展开代码 >folded123456&lt;div class=\"buttons\"&gt; &lt;button class=\"button is-info\"&gt;Info&lt;/button&gt; &lt;button class=\"button is-success\"&gt;Success&lt;/button&gt; &lt;button class=\"button is-warning\"&gt;Warning&lt;/button&gt; &lt;button class=\"button is-danger\"&gt;Danger&lt;/button&gt;&lt;/div&gt; 光有按钮肯定是不行的，一般我们还需要给按钮增加事件，比如点击下面的按钮，可以显示一条一言（Hitokoto）。 显示一言 ↑↑↑ 试着点击“显示一言”！ function showHitokoto (event) { event.target.classList.add('is-loading'); $.ajax({ type: 'GET', url: 'https://v1.hitokoto.cn/', success: function (data) { $('.hitokoto').text(data.hitokoto); event.target.classList.remove('is-loading'); } }); } 点击展开代码 >folded12345678910111213141516&lt;button class=\"button is-info\" onclick=\"showHitokoto(event)\"&gt;显示一言&lt;/button&gt;&lt;blockquote class=\"hitokoto\"&gt;↑↑↑ 试着点击“显示一言”！&lt;/blockquote&gt;&lt;script&gt;function showHitokoto (event) { event.target.classList.add('is-loading'); $.ajax({ type: 'GET', url: 'https://v1.hitokoto.cn/', success: function (data) { $('.hitokoto').text(data.hitokoto); event.target.classList.remove('is-loading'); } });}&lt;/script&gt; 进度条 点击展开代码 >folded12345&lt;progress class=\"progress is-info\" value=\"20\" max=\"100\"&gt;&lt;/progress&gt;&lt;progress class=\"progress is-success\" value=\"40\" max=\"100\"&gt;&lt;/progress&gt;&lt;progress class=\"progress is-warning\" value=\"60\" max=\"100\"&gt;&lt;/progress&gt;&lt;progress class=\"progress is-danger\" value=\"80\" max=\"100\"&gt;&lt;/progress&gt;&lt;progress class=\"progress is-info\" max=\"100\"&gt;&lt;/progress&gt; 标签页 Pictures Music Videos Documents Pixabay 是全球知名的图库网站及充满活力的创意社区,拥有上百万张免费正版高清照片素材,涵盖风景、人物、动态、静物等多种分类,你可以在任何地方使用 Pixabay 图库中的素材... 网易云音乐 是一款专注于发现与分享的音乐产品,依托专业音乐人、DJ、好友推荐及社交功能,为用户打造全新的音乐生活。 哔哩哔哩 是国内知名的视频弹幕网站,这里有最及时的动漫新番,最棒的ACG氛围,最有创意的Up主。大家可以在这里找到许多欢乐。 石墨文档 是全新一代云 Office 办公软件,支持多人在线协作编辑文档和表格,独有内容级安全,全过程留痕可追溯。PC 端和移动端全覆盖,随时随地远程办公。即写即存... .content .tabs ul { margin: 0; } .tab-content { display: none; } function onTabClick (event) { var tabTitle = $(event.currentTarget).children('span:last-child').text(); $('.article .content .tab-content').css('display', 'none'); $('.article .content .tabs li').removeClass('is-active'); $('#' + tabTitle).css('display', 'block'); $(event.currentTarget).parent().addClass('is-active'); } 点击展开代码 >folded12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;div class=\"tabs is-toggle\"&gt;&lt;ul&gt;&lt;li class=\"is-active\"&gt;&lt;a onclick=\"onTabClick(event)\"&gt;&lt;span class=\"icon is-small\"&gt;&lt;i class=\"fas fa-image\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/span&gt;&lt;span&gt;Pictures&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a onclick=\"onTabClick(event)\"&gt;&lt;span class=\"icon is-small\"&gt;&lt;i class=\"fas fa-music\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/span&gt;&lt;span&gt;Music&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a onclick=\"onTabClick(event)\"&gt;&lt;span class=\"icon is-small\"&gt;&lt;i class=\"fas fa-film\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/span&gt;&lt;span&gt;Videos&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a onclick=\"onTabClick(event)\"&gt;&lt;span class=\"icon is-small\"&gt;&lt;i class=\"far fa-file-alt\" aria-hidden=\"true\"&gt;&lt;/i&gt;&lt;/span&gt;&lt;span&gt;Documents&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;{% raw %}&lt;div id=\"Pictures\" class=\"tab-content\" style=\"display: block;\"&gt;{% endraw %}[Pixabay](https://pixabay.com/zh/) 是全球知名的图库网站及充满活力的创意社区,拥有上百万张免费正版高清照片素材,涵盖风景、人物、动态、静物等多种分类,你可以在任何地方使用 Pixabay 图库中的素材...{% raw %}&lt;/div&gt;{% endraw %}{% raw %}&lt;div id=\"Music\" class=\"tab-content\"&gt;{% endraw %}[网易云音乐](https://music.163.com/) 是一款专注于发现与分享的音乐产品,依托专业音乐人、DJ、好友推荐及社交功能,为用户打造全新的音乐生活。{% raw %}&lt;/div&gt;{% endraw %}{% raw %}&lt;div id=\"Videos\" class=\"tab-content\"&gt;{% endraw %}[哔哩哔哩](https://www.bilibili.com/) 是国内知名的视频弹幕网站,这里有最及时的动漫新番,最棒的ACG氛围,最有创意的Up主。大家可以在这里找到许多欢乐。{% raw %}&lt;/div&gt;{% endraw %}{% raw %}&lt;div id=\"Documents\" class=\"tab-content\"&gt;{% endraw %}[石墨文档](https://shimo.im/) 是全新一代云 Office 办公软件,支持多人在线协作编辑文档和表格,独有内容级安全,全过程留痕可追溯。PC 端和移动端全覆盖,随时随地远程办公。即写即存...{% raw %}&lt;/div&gt;{% endraw %}&lt;style type=\"text/css\"&gt;.content .tabs ul { margin: 0; }.tab-content { display: none; }&lt;/style&gt;&lt;script&gt;function onTabClick (event) { var tabTitle = $(event.currentTarget).children('span:last-child').text(); $('.article .content .tab-content').css('display', 'none'); $('.article .content .tabs li').removeClass('is-active'); $('#' + tabTitle).css('display', 'block'); $(event.currentTarget).parent().addClass('is-active');}&lt;/script&gt; 彩色突出 Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;div class=\"notification is-info\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;div class=\"notification is-success\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;div class=\"notification is-warning\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;div class=\"notification is-danger\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;article class=\"message is-info\"&gt;&lt;div class=\"message-body\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;&lt;/article&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;article class=\"message is-success\"&gt;&lt;div class=\"message-body\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;&lt;/article&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;article class=\"message is-warning\"&gt;&lt;div class=\"message-body\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;&lt;/article&gt;{% endraw %} Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded123{% raw %}&lt;article class=\"message is-danger\"&gt;&lt;div class=\"message-body\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;&lt;/article&gt;{% endraw %} 活用 Bulma 美化 Icarus 文章Icarus 主题以白色的简洁为主，但有时候我们希望在文章中用特别的样式注明一些内容，markdown 语法就不够用了，所以在此分享一下我的高级玩法。 点击展开代码 >folded12345{% raw %}&lt;article class=\"message is-info\"&gt;&lt;div class=\"message-header\"&gt;{% endraw %}活用 Bulma 美化 Icarus 文章{% raw %}&lt;/div&gt;&lt;div class=\"message-body\"&gt;{% endraw %}[Icarus](https://blog.zhangruipeng.me/hexo-theme-icarus/) 主题以白色的简洁为主，但有时候我们希望在文章中用**特别的样式**注明一些内容，*markdown* 语法就不够用了，所以在此分享一下我的高级玩法。{% raw %}&lt;/div&gt;&lt;/article&gt;{% endraw %} 点击展开代码点击展开代码 >folded123{% codeblock \"点击展开代码\" lang:js &gt;folded %}console.log('I love Icarus!');{% endcodeblock %} 你知道的太多了 .heimu { color: #000; background-color: #000; } .heimu:hover { color: #fff; } iMaeGoo 出自独立游戏 World of Goo 里小粘球的叫声，读作 /ɪ’mæɡu/ 不是爱妹狗啊，在家里电脑还是个大头（CRT）的时候就在玩了，其实头像也是在当时设定的，一直沿用至今。找不到女朋友誓不改头像 点击展开代码 >folded1234567{% raw %}&lt;style type=\"text/css\"&gt;.heimu { color: #000; background-color: #000; }.heimu:hover { color: #fff; }&lt;/style&gt;{% endraw %}**iMaeGoo** 出自独立游戏 [World of Goo](https://store.steampowered.com/app/22000/) 里小粘球的叫声，读作 /ɪ'mæɡu/ {% raw %}&lt;span class=\"heimu\"&gt;不是爱妹狗啊&lt;/span&gt;{% endraw %}，在家里电脑还是个大头（CRT）的时候就在玩了，其实头像也是在当时设定的，一直沿用至今。{% raw %}&lt;span class=\"heimu\"&gt;找不到女朋友誓不改头像&lt;/span&gt;{% endraw %} 让简介不在正文出现我们知道 Hexo 用 &lt;!-- more --&gt; 可以分隔简介和正文部分，但这样简介也会在正文中出现，如果我们不想让简介部分出现在正文呢？ 点击展开代码 >folded123456789101112131415这里的内容会出现在 **简介和正文**{% raw %}&lt;div class=\"post-summary\"&gt;{% endraw %}这里的内容只会出现在 **简介**{% raw %}&lt;/div&gt;{% endraw %}&lt;!-- more --&gt;&lt;style type=\"text/css\"&gt;.post-summary { display: none; }&lt;/style&gt;这里的内容只会出现在 **正文** 封面图来源声明 &nbsp;&nbsp; Vector Landscape Vectors by Vecteezy 点击展开代码 >folded1234&lt;a class=\"tag is-dark is-medium\" href=\"https://www.vecteezy.com/free-vector/vector-landscape\" target=\"_blank\"&gt;&lt;span class=\"icon\"&gt;&lt;i class=\"fas fa-camera\"&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp;&amp;nbsp;Vector Landscape Vectors by Vecteezy&lt;/a&gt; 这就是关于 Icarus 的高级玩法了，大家有什么想法呢，欢迎在评论区告诉小编一起讨论哦！（不是 Title: 活用 Bulma 美化 Icarus 文章 Author: iMaeGoo Link: https://www.imaegoo.com/2020/icarus-with-bulma/ Released Date: 2020-04-21 18:55 Last update: 2020-05-07 16:53 Statement: 本博客所有文章除特别声明外，均采用 CC BY 4.0 许可协议。转载请注明出处！","link":"/2020/05/13/icarus-with-bulma/"},{"title":"大数据技术原理与应用 - (4). 分布式数据库 HBase","text":"【第二篇】 - 大数据存储与管理, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据存储与管理相关技术的概念与原理，包括 第3章 - Hadoop 分布式文件系统 (HDFS) 第4章 - 分布式数据库 (HBase) 第5章 - NoSQL 数据库 HBase 是针对 Google BigTable 的开源实现，是一个高可靠、高性能、面向列、可伸缩的分布式数据块，主要用来存储非结构化和半结构化的松散数据。 本章介绍 HBase 与关系型数据库的区别、访问接口、数据模型、实现原理和运行机制。 HBase 简介HBase 是针对 Google BigTable 的开源实现，是一个高可靠、高性能、面向列、可伸缩的分布式数据块，主要用来存储非结构化和半结构化的松散数据。 HBase 与传统数据库的区别 传统关系数据库 HBase 数据类型 关系模型，具有丰富的数据类型和存储方式。 采用了更加简单的数据模型，它把数据存储为未经解释的字符串。 数据操作 包含了丰富的操作，如增删改查等，还会涉及复杂的多表连接。 不存在表与表的关系，只有简单的插入、查询、删除、清空等操作。 存储模式 基于行模式存储，元组或行会被连续地存储在磁盘页中。在读取数据时，需要顺序扫描每个元组。 基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的。 数据索引 通常可以针对不同列构建复杂的多个索引 (主索引 + 多个二级索引)，以提高数据访问性能。 只有一个索引——行键。 数据维护 更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。 HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留。 可伸缩性 很难实现横向扩展 (增加机器)，纵向扩展 (升级内存、cpu 等) 的空间也比较有限。 为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩。 HBase 访问接口 类型 特点 场合 Native Java API 最常规和高效的访问方式 适合 Hadoop MapReduce 作业并行批处理 HBase 表数据 HBase Shell HBase 的命令行工具，最简单的接口 适合 HBase 管理使用 Thrift Gateway 利用 Thrift 序列化技术，支持 C++、PHP、Python 等多种语言 适合其他异构系统在线访问 HBase 表数据 REST Gateway 解除了语言限制 支持 REST 风格的 Http API 访问 HBase Pig 使用 Pig Latin 流式编程语言来处理 HBase 中的数据 适合做数据统计 Hive 简单 当需要以类似 SQL 语言方式来访问 HBase 的时候 HBase 数据模型 HBase 是一个稀疏、多维度、排序的映射表，这张表的索引是行键 (Row Key)、列族 (Column Family)、列限定符 (Column Qualifier) 和时间戳 (Timestamp) 来进行索引 每个值是一个未经解释的字符串，没有数据类型 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换 HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的） 数据模型的相关概念 概念 描述 表 HBase 采用表来组织数据，表由行和列组成，列划分为若干了列族。 行 表由若干行组成，行由行键 (Row Key) 来标识。访问方式：1) 单个行键访问；2) 行键区间访问；3) 全表扫描 列族 1. 表被份组成许多列族的集合，它是基本访问控制单元。列族需要在创建时定义好，数量不能太多，不能频繁修改，存储在一个列族下的所有数据都属于同一数据类型。2. 表中的列都归属于某个列族下，数据存放在列族的某个列下。 列限定符 列族中的数据通过列限定符 (或列) 来定位。不用事先定义，无需再不同行间保持一致，没有数据类型。 单元格 单元格 (cell) 存储的数据没有数据类型 (未经解释的字符串) ，格中可以保存一个数据的多个版本，每个版本对应一个不同的时间戳。 时间戳 每次对一个单元格执行操作 (新建、修改、删除) 时，HBase 都会隐式地自动生成并存储一个时间戳，用于定位格中不同版本地数据。 数据坐标HBase 用四维坐标定位一个单元格: [行键, 列族, 列限定符, 时间戳]。 如果把思维坐标当作整体，可以视为 “键”，单元格中的数据视为 “值” 的话，HBase可以看作一个键值数据库。 键 值 [“201505003”, “Info”, “email”, “1174184619081”] “xie@qq.com“ [“201505003”, “Info”, “email”, “1174184620720”] “you@163.com“ 视图概念 HBase 表的概念试图中，每行都包含相同的列族，但是行不需要在每个列族里存储数据。从这个角度看，HBase 表是一个稀疏的映射关系，即里面存在很多空的单元格。 物理视图 物理存储层面，HBase 采用了基于列的存储方式，不像传统关系型数据库那样采用基于行存储的方式，这是 HBase 与传统数据库的重要区别。因此概念视图中的数据在物理层面实际上被存成了两个小片段，也就是 HBase 表按照 contents 和 anchor 两个列族分别存放，属于同一列族的数据保存在一起，同时每个列族一存放的还有行键和时间戳。 面向列的存储 列式数据库采用 DSM (Decomposition Storage Model) 存储模型，DSM 会对关系进行垂直分解，并为每个属性分配一个子关系。因此，一个具有 $n$ 个属性的关系会被分解为 $n$ 个子关系，每个子关系单独存储，每个子关系只有当前相应的属性被请求时，才会被访问。DSM 是以关系数据库中的属性或列为单位进行存储的，关系中多个元组的同一属性值会被存储在一起，而一个元组中的不同属性值通常会被存放在不同的磁盘页中。 HBase 是以列族为单位进行分解，而不是每个列都不单独存储。 行存储：数据按行存储在底层文件系统中。通常，每一行会被分配固定的空间。 优点：有利于增加、修改整行记录等操作；有理由数据的读取操作。 缺点：单列查询时，会读取一些不必要的数据。 列存储：数据以列为单位，存储在底层文件系统中。 优点：有利于面向单列数据的读取、统计等操作。 缺点：整行读取时，可能需要多次I/O操作。 适用场景 行式数据库：适合小批量得数据处理，如联机事务型数据处理。主要用于如银行个人信息数据库之类得场景，对数据不出错要求较高。 列式数据库：适合批量数据处理和即席查询 (Ad-Hoc Query)。主要用于数据挖掘、决策支持、地理信息系统等查询密集型系统中，因为每次查询都不必遍历所有数据库。 为什么列式存储适合数据分析 因为分析中常常是以一个一个属性 (也就是列) 进行分析的，而列式数存储可以直接取出需要分析的那一列数据。相反地，行式数据库中如果需要取出某一列 (例如性别、年龄) 进行分析，需要扫描整个数据包才能取出对应数据。 HBase 存储原理HBase 功能组件HBase的实现包括三个主要的功能组件： 库函数：链接到每个客户端 一个 Master 服务器 负责管理和维护 HBase 表的分区信息，如一个表被分成了哪些 Region 和被存放到哪台 Region 服务器。 负责维护 Region 服务器列表，如监测 Region 服务器，确保它们之间负载均衡；对故障 Region 服务器中存储的 Region 进行重新分配。 处理模式变化，如表和列族的创建。 多个 Region 服务器 负责存储和维护分配给自己的 Region。 处理来自客户端的读写请求。 客户端并不依赖 Master，而是通过 Zookeeper 来获得 Region 位置信息，大多数客户端甚至从来不和 Master 通信，这种设计方式使得 Master 负载很小。 表和 Region HBase 中存储了许多表。一个 HBase 表根据行键的字典序又被划分为许多 Region。 Region 是 HBase 分布式存储的最基本单元 随着数据不断插入，Region 会持续增大，当 Region 中行数达到阈值 hbase.hregion.max.filesize，就会自动分裂成两个新的 Region。 快速分裂，一般在2-3S内完成，因为只是修改只想信息，实际数据还是存在旧的 Region。 直到”合并”过程把存储文件异步地写到独立的文件之后才会读取新文件。 每个 Region 的最佳大小取决于单台服务器的有效处理能力。 目前推荐 1~2GB 以上 (2013年后的硬件水平) 不同的 Region 可以分布在不同的 Region 服务器上 同一个 Region 不会被分拆到多个 Region 服务器 Region 定位HBase 表中有多个 Region，它们会被分发到不同的 Region 服务器上。因此，HBase 设计了三层结构实现 Region 的寻址和定位。 Zookeeper: 文件记录了 -ROOT- 表 的位置信息。 -ROOT- 表: 记录. META. 表的 Region 的位置信息 (因为 .META. 表增大后会分裂成多个 Region)。 -ROOT- 表不能被分割，只有唯一一个 Region。 .META.表 (元数据表): 存储了 Region 和 Region 服务器的映射关系。 当 HBase 表很大时， .META.表也会被分裂成多个 Region。 为了加快访问速度，.META. 表的全部 Region 都会被保存在内存中。 这样的设计能存多少数据？ 假设 .META. 表中每行 (每个映射条目) 在内存中大约占 1 KB，每个 Region 限制为 128 MB。 一个 -ROOT- 表只有一个 Region, i.e., 128 MB 大小： 128 MB 可以存 $128 MB/ 1KB = 2^{17}$ 行映射条目。 也就是 一个 -ROOT- 表可以寻址 $2^{17}$ 个 .META. 表的 Region 位置信息。 同理，一个 .META. 表的 Region 可以寻址用户数据表的 Region 个数是 $128 MB/ 1KB = 2^{17}$。 最终三层结构可以保存的 Region 数目是 $(128 MB/ 1 KB) \\times (128 MB/ 1 KB) = 2^{34}$ 个 Region。 客户端访问数据时的“三级寻址”: 为了加速寻址，客户端会缓存位置信息，这样可以直接从客户端缓存中获取 Region 位置信息。 解决缓存失效问题: 惰性监测。当访问数据时，发现缓存中 Region 位置信息不存在，才会判断出缓存失效。然后再重新启动上面的“三级寻址”过程重新获取最新的 Region 信息。 寻址过程客户端只需要询问 Zookeeper 服务器，不需要连接 Master 服务器。 HBase 的运行机制HBase 系统架构HBase 系统架构包含下图组件。由于 HBase 通常采用 HDFS 作为底层数据存储，因此下图加入了 HDFS 和 Hadoop。 客户端 (Client) 包含访问 HBase 的接口，同时在缓存中维护着已经访问过的 Region 位置信息，用来加快后续数据访问过程。 Zookeeper 实现集群的协同管理服务。Zookeeper 可以不是一台机器，可以是多台机器构成的集群。 Zookeeper 可以帮助选举出一个 Master 作为集群的总管，并保证在任何时刻总有唯一一个 Master 在运行，避免了 Master 的“单点失效”问题。 Master: 主服务器Master主要负责表和Region的管理工作 管理用户对表的增加、删除、修改、查询等操作。 实现不同 Region 服务器之间的负载均衡。 在 Region 分裂或合并后，负责重新调整 Region 的分布。 对发生故障失效的 Region 服务器上的 Region 进行迁移。 Region 服务器 负责维护分配给自己的 Region，并响应用户的读写请求。 Region 服务器工作原理 各个组件: 每个 Region 服务器中的若干个 Region 公用一个 HLog 每个 Region 中由多个 Store 组成 每个 Store 对应表中一个列族的存储 一个 Store 包含一个 MemStore 和若干个 StoreFile MemStore 是在内存中缓存，保存最近更新的数据 StoreFile 是磁盘中的文件，这些文件都是 B树的结构，方便快速读取。 StoreFile 在底层的实现方式 HDFS 文件系统的 HFile 用户读写数据的过程 当用户写入数据时，会被分配到相应Region服务器去执行。 用户数据首先被写入到 MemStore 和 HLog 中。 只有当操作写入 Hlog 之后，commit() 调用才会将其返回给客户端。 当用户读取数据时，Region 服务器会首先访问 MemStore 缓存，如果找不到，再去磁盘上面的 StoreFile 中寻找。 缓存的刷新 MemStore 缓存容量有限，系统会周期性地调用 Region.flushcache() 把 MemStore 缓存里地内容写到磁盘地 StoreFile 文件中，并清空缓存，同时在 HLog 文件中写入一个标记。 每个 Region 服务器都有一个自己的 HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入 MemStore，再刷写到 StoreFile，最后删除旧的 Hlog 文件，开始为用户提供服务。 StoreFile 的合并与分裂 随着 StoreFile 数量增加，达到预先设定的阈值后，系统会触发合并操作，将多个 StoreFile 合并成一个大的 StoreFile。 每次刷写都生成一个新的 StoreFile，数量太多，影响查找速度。 因此系统会调用 Store.compact() 把多个 StoreFile 合并成一个大文件。 合并操作比较耗费资源，因此只有 StoreFile 文件数量达到一个阈值才启动合并操作。 单个 StoreFile 过大时，又触发分裂操作，1个父 Region 被分裂成两个子 Region。 HLog 工作原理分布式环境必须要考虑系统出错。HBase 采用 HLog 保证系统恢复。 HBase 系统为每个 Region 服务器配置了一个 HLog 文件，它是一种预写式日志（Write Ahead Log） 用户更新数据必须首先写入日志后，才能写入 MemStore 缓存，并且，直到 MemStore 缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘 Zookeeper 会实时监测每个 Region 服务器的状态，当某个 Region 服务器发生故障时，Zookeeper 会通知 Master Master 首先会处理该故障 Region 服务器上面遗留的 HLog 文件，这个遗留的 HLog 文件中包含了来自多个 Region 对象的日志记录 系统会根据每条日志记录所属的 Region 对象对 HLog 数据进行拆分，分别放到相应 Region 对象的目录下，然后，再将失效的 Region 重新分配到可用的 Region 服务器中，并把与该 Region 对象相关的 HLog 日志记录也发送给相应的 Region 服务器 Region 服务器领取到分配给自己的 Region 对象以及与之相关的 HLog 日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到 MemStore 缓存中，然后，刷新到磁盘的 StoreFile 文件中，完成数据恢复 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志 HBase实际应用中的性能优化方法 行键（Row Key）行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 InMemory创建表的时候，可以通过 HColumnDescriptor.setInMemory(true) 将表放到 Region 服务器的缓存中，保证在读取的时候被 cache 命中。 Max Version创建表的时候，可以通过 HColumnDescriptor.setMaxVersions(int maxVersions) 设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置 setMaxVersions(1)。 Time To Live创建表的时候，可以通过 HColumnDescriptor.setTimeToLive(int timeToLive) 设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置 setTimeToLive(2 * 24 * 60 * 60)。","link":"/2020/05/21/intro2BigData4/"},{"title":"大数据技术原理与应用 - (5). NoSQL 数据库","text":"【第二篇】 - 大数据存储与管理, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据存储与管理相关技术的概念与原理，包括 第3章 - Hadoop 分布式文件系统 (HDFS) 第4章 - 分布式数据库 (HBase) 第5章 - NoSQL 数据库 NoSQL (Not only SQL) 是一种不同于关系数据库的数据库管理系统设计方式。 NoSQL（Not only SQL）表示关系和非关系型数据库各有优缺点，彼此都无法互相取代。NoSQL 是一种不同于关系数据库的数据库管理系统设计方式，是对非关系型数据库的统称，它所采用的数据模型并非传统关系数据库的关系模型，而是类似键/值、列族、文档等非关系型模型。 NoSQL 特点: 灵活的可扩展性 灵活的数据模型 与云计算紧密融合 NoSQL与关系数据库的比较RDBMS即关系数据库管理系统 (Relational Database Management System) 。 比较标准 关系型数据库 (RDBMS) NoSQL 数据库原理 RDBMS有关系代数理论作为基础 NoSQL没有统一的理论基础 数据规模 RDBMS很难实现横向扩展，纵向扩展的空间也比较有限，性能会随着数据规模的增大而降低 可以很容易通过添加更多设备来支持更大规模的数据 数据库模式 需要定义数据库模式，严格遵守数据定义和相关约束条件 不存在数据库模式，可以自由灵活定义并存储各种不同类型的数据 查询效率 借助于索引机制可以实现快速查询（包括记录查询和范围查询） 没有面向复杂查询的索引，虽然NoSQL可以使用MapReduce来加速查询，但是，在复杂查询方面的性能仍然不如RDBMS 一致性 严格遵守事务ACID模型，可以保证事务强一致性 NoSQL数据库放松了对事务ACID四性的要求，而是遵守BASE模型，只能保证最终一致性 数据完整性 可以很容易实现数据完整性，比如通过主键或者非空约束来实现实体完整性，通过主键、外键来实现参照完整性，通过约束或者触发器来实现用户自定义完整性 无法实现 扩展性 很难实现横向扩展，纵向扩展的空间也比较有限 NoSQL在设计之初就充分考虑了横向扩展的需求，可以很容易通过添加廉价设备实现扩展 可用性 RDBMS在任何时候都以保证数据一致性为优先目标，其次才是优化系统性能，随着数据规模的增大，RDBMS为了保证严格的一致性，只能提供相对较弱的可用性 大多数NoSQL都能提供较高的可用性 标准化 RDBMS已经标准化（SQL） NoSQL还没有行业标准，不同的NoSQL数据库都有自己的查询语言，很难规范应用程序接口 技术支持 RDBMS经过几十年的发展，已经非常成熟，Oracle等大型厂商都可以提供很好的技术支持 NoSQL在技术支持方面仍然处于起步阶段，还不成熟，缺乏有力的技术支持 可维护性 RDBMS需要专门的数据库管理员(DBA)维护 NoSQL数据库虽然没有DBMS复杂，也难以维护 关系数据库 优势：以完善的关系代数理论作为基础，有严格的标准，支持事务ACID四性，借助索引机制可以实现高效的查询，技术成熟，有专业公司的技术支持 劣势：可扩展性较差，无法较好支持海量数据存储，数据模型过于死板、无法较好支持Web2.0应用，事务机制影响了系统的整体性能等 NoSQL数据库 优势：可以支持超大规模数据存储，灵活的数据模型可以很好地支持Web2.0应用，具有强大的横向扩展能力等 劣势：缺乏数学理论基础，复杂查询性能不高，大都不能实现事务强一致性，很难实现数据完整性，技术尚不成熟，缺乏专业团队的技术支持，维护较困难等 关系数据库和NoSQL数据库各有优缺点，彼此无法取代 关系数据库应用场景：电信、银行等领域的关键业务系统，需要保证强事务一致性 NoSQL数据库应用场景：互联网企业、传统企业的非关键业务（比如数据分析） NoSQL 四大类型 键值数据库键值数据库 (Key-Value Database) 会使用一个哈希表，这个表中由一个特定的 Key 和一个指针指向特定的 Value。 数据模型 键/值对 键是一个字符串对象 值可以是任意类型的数据，比如整型、字符型、数组、列表、集合等 典型应用 涉及频繁读写、拥有简单数据模型的应用 内容缓存，比如会话、配置文件、参数、购物车等 存储配置和用户数据信息的移动应用 优点 扩展性好，灵活性好，大量写操作时性能高 缺点 无法存储结构化信息，条件查询效率较低 不适用情形 不是通过键而是通过值来查：键值数据库根本没有通过值查询的途径 需要存储数据之间的关系：在键值数据库中，不能通过两个或两个以上的键来关联数据 需要事务的支持：在一些键值数据库中，产生故障时，不可以回滚 使用者 百度云数据库（Redis）、GitHub（Riak）、BestBuy（Riak）、Twitter（Redis和Memcached）、StackOverFlow（Redis）、Instagram （Redis）、Youtube（Memcached）、Wikipedia（Memcached） 列族数据库 数据模型 列族 典型应用 分布式数据存储与管理 数据在地理上分布于多个数据中心的应用程序 可以容忍副本中存在短期不一致情况的应用程序 拥有动态字段的应用程序 拥有潜在大量数据的应用程序，大到几百TB的数据 优点 查找速度快，可扩展性强，容易进行分布式扩展，复杂性低 缺点 功能较少，大都不支持强事务一致性 不适用情形 需要ACID事务支持的情形，Cassandra等产品就不适用 使用者 Ebay（Cassandra）、Instagram（Cassandra）、NASA（Cassandra）、Twitter（Cassandra and HBase）、Facebook（HBase）、Yahoo!（HBase） 文档数据库 数据模型 键/值 值（value）是版本化的文档 典型应用 存储、索引并管理面向文档的数据或者类似的半结构化数据 比如，用于后台具有大量读写操作的网站、使用JSON数据结构的应用、使用嵌套结构等非规范化数据的应用程序 优点 性能好（高并发），灵活性高，复杂性低，数据结构灵活 提供嵌入式文档功能，将经常查询的数据存储在同一个文档中 既可以根据键来构建索引，也可以根据内容构建索引 缺点 缺乏统一的查询语法 不适用情形 在不同的文档上添加事务。文档数据库并不支持文档间的事务，如果对这方面有需求则不应该选用这个解决方案 使用者 百度云数据库（MongoDB）、SAP （MongoDB）、Codecademy （MongoDB）、Foursquare （MongoDB）、NBC News （RavenDB） 图形数据库 数据模型 图结构 典型应用 专门用于处理具有高度相互关联关系的数据，比较适合于社交网络、模式识别、依赖分析、推荐系统以及路径寻找等问题 优点 灵活性高，支持复杂的图形算法，可用于构建复杂的关系图谱 缺点 复杂性高，只能支持一定的数据规模 使用者 Adobe（Neo4J）、Cisco（Neo4J）、T-Mobile（Neo4J） NoSQL 的三大基石NoSQL 三大基石包括 CAP、BASE和最终一致性。 CAPCAP指的是： C (Consistency)：一致性。是指任何一个读操作总是能够读到之前完成的写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所有节点在同一时间具有相同的数据 A (Availability)：可用性。是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应； P (Tolerance of Network Partition)：分区容忍性。是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。 CAP理论告诉我们，一个分布式系统不可能同时满足一致性、可用性和分区容忍性这三个需求，最多只能同时满足其中两个。 当处理CAP的问题时，可以有几个明显的选择： CA: 也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server和PostgreSQL），都采用了这种设计原则，因此，扩展性都比较差 CP: 也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务 AP: 也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据 BASE说起BASE（Basically Availble, Soft-state, Eventual consistency），不得不谈到ACID。 ACID BASE 原子性 (Atomicity) 基本可用 (Basically Available) 一致性 (Consistency) 软状态/柔性事务 (Soft state) 隔离性 (Isolation) 最终一致性 (Eventual consistency) 持久性 (Durable) 一个数据库事务具有 ACID 四性： A (Atomicity): 原子性，是指事务必须是原子工作单元，对于其数据修改，要么全都执行，要么全都不执行 C (Consistency): 一致性，是指事务在完成时，必须使所有的数据都保持一致状态 I (Isolation): 隔离性，是指由并发事务所做的修改必须与任何其它并发事务所做的修改隔离 D (Durability): 持久性，是指事务完成之后，它对于系统的影响是永久性的，该修改即使出现致命的系统故障也将一直保持 BASE 的基本含义是基本可用 (Basically Availble)、软状态 (Soft-state) 和最终一致性 (Eventual consistency)： 基本可用基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现 软状态“软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态”是指状态可以有一段时间不同步，具有一定的滞后性 最终一致性一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下，后续操作是否能够获取最新的数据。对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到的都是更新后的最新数据，那么就是弱一致性。而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新后的数据。 最终一致性最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为： 因果一致性：如果进程A通知进程 B 它已更新了一个数据项，那么进程 B 的后续访问将获得A写入的最新值。而与进程 A 无因果关系的进程 C 的访问，仍然遵守一般的最终一致性规则 “读己之所写”一致性：可以视为因果一致性的一个特例。当进程 A 自己执行一个更新操作之后，它自己总是可以访问到更新过的值，绝不会看到旧值 单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续访问都不会返回在那个值之前的值 会话一致性：它把访问存储系统的进程放到会话 (session) 的上下文中，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会话 单调写一致性：系统保证来自同一个进程的写操作顺序执行。系统必须保证这种程度的一致性，否则就非常难以编程了 如何实现各种类型的一致性？对于分布式数据系统： N — 数据复制的份数 W — 更新数据是需要保证写完成的节点数 R — 读取数据的时候需要读取的节点数 如果 W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主一备同步复制的关系型数据库，N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。一般设定是 R+W = N+1，这是保证强一致性的最小设定。 如果 W+R&lt;=N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。 对于分布式系统，为了保证高可用性，一般设置 N&gt;=3。不同的 N,W,R 组合，是在可用性和一致性之间取一个平衡，以适应不同的应用场景。 如果 N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于数据分布的N个节点是同步写入的，因此可以保证强一致性。 实例：HBase 是借助其底层的HDFS来实现其数据冗余备份的。HDFS 采用的就是强一致性保证。在数据没有完全同步到 N 个节点前，写操作是不会返回成功的。也就是说它的 W＝N，而读操作只需要读到一个值即可，也就是说它 R＝1。 像 Voldemort，Cassandra和Riak 这些类 Dynamo 的系统，通常都允许用户按需要设置 N，R，W 三个值，即使是设置成 W＋R&lt;= N 也是可以的。也就是说他允许用户在强一致性和最终一致性之间自由选择。而在用户选择了最终一致性，或者是 W &lt; N 的强一致性时，则总会出现一段“各个节点数据不同步导致系统处理不一致的时间”。为了提供最终一致性的支持，这些系统会提供一些工具来使数据更新被最终同步到所有相关节点。 从 NoSQL 到 NewSQL 数据库","link":"/2020/05/26/intro2BigData5/"},{"title":"大数据技术原理与应用 - (1). 大数据概述","text":"【第一篇】 - 大数据基础, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据 (Big Data) 的基本概念、影响、应用领域等，还介绍了大数据处理框架 Hadoop。 第1章 - 大数据概述 第2章 - 大数据处理框架 Hadoop 第一张章介绍了大数据的基本概念、影响、应用领域、关键技术等等内容。 大数据时代第三次信息化浪潮： 信息化浪潮 发生时间 标志 解决的问题 代表企业 第一次浪潮 1980年前后 个人计算 信息处理 Intel、AMD、IBM、Apple、Microsoft、联想、戴尔、惠普等 第二次浪潮 1995年前后 互联网 信息传输 雅虎、谷歌、阿里巴巴、百度、腾讯等 第三次浪潮 2010年前后 物联网、云计算和大数据 信息爆炸 亚马逊、谷歌、IBM、VMWare、Palantir、Hortonworks、Cloudera、阿里云等 根据IBM前首席执行官 Louis Gerstner 的观点，IT领域每隔十五年就会迎来一次重大变革 信息科技为大数据时代提供技术支撑： 存储设备容量不断增加 CPU处理能力大幅提升 网络带宽不断增加 数据产生方式的变革促成大数据时代的来临： 大数据的概念 - 4V Volume: 数据量大 Variety: 数据类型繁多 Velocity: 处理速度快 Value: 价值密度低 大数据的影响科学研究上： 图灵奖获得者、著名数据库专家Jim Gray 博士观察并总结人类自古以来，在科学研究上，先后历经了实验、理论、计算和数据四种范式。 思维方式上： 全样而非抽样: 以前科学分析通常采取抽样方式，如今在大数据技术的支持下，分析可以针对全集数据进行。 效率而非精确: 以前由于抽样仅是针对样本的分析，抽样分析上的结果应用到全集数据后，抽样分析上的小误差可能会被放大；大数据常常要求‘秒级响应’，否则丧失数据价值，因此更关注效率。 相关而非因果: 大数据往往只能反映事物的相关性。 数据类型 结构化数据： 是指可以存储在数据库里，可以用二维表结果来逻辑表达实现的数据。 非结构化数据： 不方便用二维表结果来逻辑表来表现的数据。包括所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像、音频、视频信息等等。 半结构化数据： 介于结构化数据和非结构化数据之间的数据。HTML文档就属于半结构数据。 大数据关键技术表：大数据技术的不同层面及其功能 技术层面 功能 数据采集 利用ETL工具将分布的、异构数据源中的数据如关系数据、平面数据文件等，抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础；或者也可以把实时采集的数据作为流计算系统的输入，进行实时处理分析 数据存储和管理 利用分布式文件系统、数据仓库、关系数据库、NoSQL数据库、云数据库等，实现对结构化、半结构化和非结构化海量数据的存储和管理 数据处理与分析 利用分布式并行编程模型和计算框架，结合机器学习和数据挖掘算法，实现对海量数据的处理和分析；对分析结果进行可视化呈现，帮助人们更好地理解数据、分析数据 数据隐私和安全 在从大数据中挖掘潜在的巨大商业价值和学术价值的同时，构建隐私数据保护体系和数据安全体系，有效保护个人隐私和数据安全 两大核心技术 分布式存储 GFS/HDFS BigTable/HBase NoSQL (键值、列族、图形、文档数据库) NewSQL (如：SQL Azure) 分布式计算 MapReduce 大数据计算模式 大数据计算模式 解决问题 代表产品 批处理计算 针对大规模数据的批量处理 MapReduce、Spark等 流计算 针对流数据的实时计算 Storm、S4、Flume、Streams、Puma、DStream、Super Mario、银河流数据处理平台等 图计算 针对大规模图结构数据的处理 Pregel、GraphX、Giraph、PowerGraph、Hama、GoldenOrb等 查询分析计算 大规模数据的存储管理和查询分析 Dremel、Hive、Cassandra、Impala等 大数据产业 大数据产业是指一切与支撑大数据组织管理和价值发现相关的企业经济活动的集合 产业链环节 包含内容 IT基础设施层 包括提供硬件、软件、网络等基础设施以及提供咨询、规划和系统集成服务的企业，比如，提供数据中心解决方案的IBM、惠普和戴尔等，提供存储解决方案的EMC，提供虚拟化管理软件的微软、思杰、SUN、Redhat等 数据源层 大数据生态圈里的数据提供者，是生物大数据（生物信息学领域的各类研究机构）、交通大数据（交通主管部门）、医疗大数据（各大医院、体检机构）、政务大数据（政府部门）、电商大数据（淘宝、天猫、苏宁云商、京东等电商）、社交网络大数据（微博、微信、人人网等）、搜索引擎大数据（百度、谷歌等）等各种数据的来源 数据管理层 包括数据抽取、转换、存储和管理等服务的各类企业或产品，比如分布式文件系统（如Hadoop的HDFS和谷歌的GFS）、ETL工具（Informatica、Datastage、Kettle等）、数据库和数据仓库（Oracle、MySQL、SQL Server、HBase、GreenPlum等） 数据分析层 包括提供分布式计算、数据挖掘、统计分析等服务的各类企业或产品，比如，分布式计算框架MapReduce、统计分析软件SPSS和SAS、数据挖掘工具Weka、数据可视化工具Tableau、BI工具（MicroStrategy、Cognos、BO）等等 数据平台层 包括提供数据分享平台、数据分析平台、数据租售平台等服务的企业或产品，比如阿里巴巴、谷歌、中国电信、百度等 数据应用层 提供智能交通、智慧医疗、智能物流、智能电网等行业应用的企业、机构或政府部门，比如交通主管部门、各大医疗机构、菜鸟网络、国家电网等","link":"/2020/05/13/intro2BigData/"},{"title":"大数据技术原理与应用 - (3). 分布式文件系统 HDFS","text":"【第二篇】 - 大数据存储与管理, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据存储与管理相关技术的概念与原理，包括 第3章 - Hadoop 分布式文件系统 (HDFS) 第4章 - 分布式数据库 (HBase) 第5章 - NoSQL 数据库 Hadoop 分布式文件系统 (Hadoop Distributed File System, HDFS) 是真的 Google File System (GFS) 的开源实现，它是 Hadoop 两大核心组件之一，提供了在廉价服务器集群中进行大规模分布式存储的能力。 本章介绍分布式文件系统的基本概念、结构和设计需求，然后介绍 HDFS 的相关概念、体系结构、存储原理和读写过程。 分布式文件系统分布式文件系统 (Distributed File System) 是一种通过网络实现文件在多台主机 (集群) 上进行分布式存储的文件系统。 计算机集群结构 分布式文件系统把文件分布存储到多个计算机节点（如上图节点 $x$ 和 $y$）上，成千上万的计算机节点构成计算机集群。 每个机架可以放许多节点（计算机）。 同一机架上的节点通过网络互联（常用吉比特以太网）。 不同机架之间采用另一级（更快）网络活交换机互连。 分布式文件系统的结构分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类: 名称结点 (NameNode) 和 数据节点 (DataNode)。 HDFS 简介 HDFS 是 Master/ Slave architecture (主从架构)。一个 HDFS 集群包括一个 NameNode，也就是一个 Master，和多个 DataNode，也就是 Slave。 在 HDFS 中，一个文件将会被拆分成多个 Block（blocksize 默认 128MB，也可以设置更高），这些 block 被存储在一系列 DataNodes 上。 典型的部署是一台专门的机器运行 NameNode，集群的其他机器运行 DataNode A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. [来源: Apache Hadoop 3.2.1 - HDFS Architecture 文档] HDFS 要实现一下目标: 兼容廉价的硬件设备 流数据读写 大数据集 简单的文件模型 强大的跨平台兼容性 HDFS 局限性: 不适合低延迟数据访问。由于自身面向大规模数据批处理需求设计，高数据吞吐率同时意味着高响应延迟，因此不适合要求低延迟（实时反馈）的应用场景。 无法高效存储大量小文件。小文件指大小远远小于一个 Block 大小（通常 128 MB）的文件。 NameNode 管理文件系统的 metadata，这些 metadata 保存在内存中，从而客户端能够快速获取文件实际存储位置。大量小文件意味着 NameNode 需要花大量的内存来保存这类大量小文件 metadata，大大降低了 metadata 的索引效率。 对于 MapReduce，大量小文件意味着会产生过多的 Map 任务，导致线程管理开销大大增加。 本身访问大量小文件的速度远远低于访问几个大文件的速度，因为需要不断从一个 DataNode 跳到另一个 DataNode。 不支持多用户写入及任意修改文件。HDFS 只允许一个文件有一个写入者，且只允许对文件进行追加操作，不执行随机写操作。 HDFS 相关概念HDFS 相关概念包括块 (Block), 名称节点 (NameNode), 数据节点 (DataNode), 第二名称节点 (Secondary NameNode)。 Block - 块传统文件系统中，为了提高磁盘读写效率，一般采用数据块为单位，而非字节。 Hadoop 分布式文件系统 (HDFS) 与之区别与联系: 联系：同样采用了数据块的概念 区别：为了最小化寻址开销，HDFS 的块比传统文件系统的块大。HDFS 默认一个块 64MB。 HDFS 采用抽象的块概念带来的好处: 支持大规模文件存储：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量 简化系统设计：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据 适合数据备份：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性 Blocksize d的考虑: 大 Block 的好处: 访问 HDFS 要三级寻址 (元数据目录、数据节点、从数据节点取数据)。设置大的块可以将寻址开销分摊到较多的数据中，降低了单位数据的寻址开销。 过大 Block 的坏处: 在后续 MapReduce 中 Map 任务一次只处理一个块中的数据，如果启动的 Map 任务太少，就降低了作业并行处理的速度。 NameNode NameNode (名称节点) 或称为 ‘主节点’ (Master Node)。 负责文件和目录的创建、删除、重命名等。 管理 DataNode 和文件块 (Block) 的映射关系。 客户端只有访问 NameNode 才能请求 Block 的位置，进而读取相应数据。 NameNode 负责管理分布式文件系统的 NameSpace (命名空间)，保存了两个核心的数据结果，即 FsImage 和 EditLog。 FsImage: 维护文件系统树和文件树中所有的文件和文件夹的 metadata。 EditLog: 记录所有针对文件的创建、删除、重命名等操作。 NameNode 启动过程和后续： 将 FsImage 的内容加载到内存当中，然后执行 EditLog 中的各项操作，使内存中的 metadata 保持最新。 步骤1. 完成后，会创建一个新的 FsImage 和 空的 EditLog (启动完成)。 NameNode 正常运作后，后续 HDFS 的更新操作会被写进 新的空的 EditLog 中。 DataNode DataNode (数据节点) 或称为 ‘从节点’ (Slave Node)。 负责数据的存储和读取。 存储: 由 NameNode 分配存储位置，然后客户端直接写入相应的 DataNode 读取: 客户端从 NameNode 获得 DataNode 和 Block 的映射关系，然后到相应位置访问 Block。 定期向 NameNode 发送心跳信息，汇报自身及所有 block 信息，和健康状况； 数据冗余存储HDFS 采用了多副本方式对数据进行冗余存储，保证了系统的容错性和可用性。一个数据块的多个副本会被分布到不同的 DataNode 上，如下图所示。 冗余存储好处： 加快数据传输速度。当多个客户端需要同时访问同一个文件，可以让 Client 从不同的数据块中读取数据。 容易检查数据错误。 保证数据可靠性。单个 DataNode 故障不会导致数据丢失。 数据存取策略存储HDFS 默认的冗余复制因子是 3, i.e., 每个 block 会同时保存到 3 个 DataNode中。 第一个副本，集群挑选一个磁盘不太满、Cpu 不太忙的 DataNode 存放。 第二个副本，存放在相同机架 (rack) 上的不同 DataNode 中。 第三个副本，存放在不同机架 (rack) 上的其他 DataNode 中。 如果还有更多副本，随机选择 DataNode 存放。 读取Client 优先选择在同一机架上的 DataNode 来读取数据块副本。如果数据块副本不在同一机架，则随机选择 DataNode 进行读取。 SecondaryNameNodeSecondaryNameNode (第二名称节点) 的功能 解决 EditLog 不断增大的问题 作为 NameNode 的冷备份 解决 EditLog 不断增大SecondaryNameNode 定期对 EditLog 和 FsImage 进行合并操作，以减小 EditLog 文件大小，缩短 NameNode 重启时间。 SecondaryNameNode 定期和 NameNode 通信，请求其停止使用 EditLog 文件，暂时将新的写操作写到一个新的文件 edit.new 上，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； SecondaryNameNode 通过 HTTP GET 方式从 NameNode 上获取到 FsImage 和 EditLog 文件，并下载到本地的相应目录下； SecondaryNameNode 将下载下来的 FsImage 载入到内存，然后一条一条地执行 EditLog 文件中的各项更新操作，使得内存中的 FsImage 保持最新；这个过程就是 EditLog 和 FsImage文件合并； SecondaryNameNode 执行完（3）操作之后，会通过 post 方式将新的 FsImage 文件发送到 NameNode 节点上 NameNode 将从 SecondaryNameNode 接收到的新的 FsImage 替换旧的 FsImage 文件，同时将 edit.new 替换 EditLog 文件，通过这个过程EditLog就变小了 NameNode 冷备份上述合并的过程可以看出，SecondaryNameNode 会定期执行合并操作得到新的 FsImage。从这个角度来看，SecondaryNameNode 相当于周期性地保存了 NameNode 中的 metadata 信息。 HDFS 体系结构HDFS 命名空间管理HDFS 的命名空间包含目录、文件、和块。命名空间管理是指命名空间支持对 HDFS 中的目录、文件和块做类似文件系统的创建、修改、删除等基本操作。 通信协议所有 HDFS 通信协议都是构建在 TCP/IP 协议基础之上的。 客户端 (Client) 通过一个可配置的端口向 NameNode 主动发起 TCP 链接，并使用客户端协议与 NameNode 交互。 客户端与 DataNode 的交互通过 RPC (Remote Procedure Call) 来实现。 HDFS 数据读写过程读数据HDFS 读数据1234567891011121314151617181920212223import java.io.BufferedReader;import java.io.InputStreamReader ;import org.apache.hadoop.conf.Configuration ;import org.apache.hadoop.fs.FileSystem ;import org.apache.hadoop.fs.Path ;import org.apache.hadoop.fs.FSDataInputStream ;public class Chapter3 { public static void main(String[] args) { try { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Path filename = new Path(\"hdfs://localhost:9000/user/hadoop/test.txt\"); FSDataInputStream is = fs.open(filename); BufferedReader d = new BufferedReader(new InputStreamReader(is)); String content = d.readLine(); //读取文件一行 System.out.println(content); d.close(); //关闭文件 fs.close(); //关闭hdfs } catch (Exception e) { e.printStackTrace(); } }} 写数据HDFS 写数据12345678910111213141516171819import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.Path;public class Chapter3 { public static void main(String[] args) { try { Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); byte[] buff = \"Hello world\".getBytes(); // 要写入的内容 String filename = \"hdfs://localhost:9000/user/hadoop/test.txt\"; //要写入的文件名 FSDataOutputStream os = fs.create(new Path(filename)); os.write(buff,0,buff.length); System.out.println(\"Create:\"+ filename); } catch (Exception e) { e.printStackTrace(); } }} HDFS 编程实现常用 Shell 命令 三种写法 hadoop fs: 适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统 hadoop dfs: 只能适用于HDFS文件系统 hdfs dfs: 只能适用于HDFS文件系统 HDFS 的 Shell 统一格式1hadoop fs [genericOptions] [commandOptions] hadoop version: 打印 Hadoop 版本信息 hadoop fs -ls &lt;path&gt;: 显示 指定的文件的详细信息 hadoop fs -mkdir &lt;path&gt;: 创建 指定的文件夹 hadoop fs -cat &lt;path&gt;: 将 指定的文件的内容输出到标准输出 (stdout) hadoop fs -tail [-f] &lt;path&gt;: 将 指定的文件最后 1KB 的内容输出到标准输出 (stdout) haoop fs -put &lt;localsrc&gt; &lt;dest&gt;: 将本地文件系统的 复制到 Hadoop 文件系统。与 copyFromLocal 类似 hadoop fs -copyFromLocal &lt;localsrc&gt; &lt;dest&gt;: 将本地源文件 复制到路径 指定的文件或文件夹中 hadoop fs -get &lt;src&gt; &lt;localdest&gt;: 将 hadoop 文件系统的 复制到本地文件系统 。 hadoop fs -copyToLocal &lt;hdfs source&gt; &lt;localdst&gt;: 类似上面 get 与 copyFromLocal 相反 hadoop fs -chmod [-R] &lt;mode&gt; &lt;path&gt;: 将 指定的文件的权限更改为 。这个命令只适用于超级用户和文件的所有者。 hadoop fs -mv &lt;src&gt; &lt;dest&gt;: 将文件从源路径 移动到目标路径 hadoop fs -cp &lt;src&gt; &lt;dest&gt;: 将文件从源路径 复制到目标路径 hadoop fs -rm [-r] &lt;path&gt;: 删除 指定文件，只能删除空目录和文件，输入 -r 是递归删除","link":"/2020/05/19/intro2BigData3/"},{"title":"大数据技术原理与应用 - (7). MapReduce","text":"【第三篇】 - 大数据处理与分析, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据处理与分析的相关技术，包括 第7章 - MapReduce 第8章 - Hive - 基于 Hadoop 的数据仓库 第9章 - Hadoop 的优化与发展 第10章 - Spark 第11章 - 流计算 第12章 - 图计算 第13章 - 数据可视化 MapReduce 是一种并行编程模型，用于大规模数据集 (大于 1 TB) 的并行运算，它将复杂的、运行于大规模集群上的并行计算过程高度抽象到两个函数: Map 和 Reduce。 MapReduce 概述MapReduce 模型简介 MapReduce 将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数: Map 和 Reduce 编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算 MapReduce 采用 “分而治之” 策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片 (split)，这些分片可以被多个 Map 任务并行处理 MapReduce 设计的一个理念就是 “计算向数据靠拢”，而不是“数据向计算靠拢”，因为，移动数据需要大量的网络传输开销 MapReduce 框架采用了 Master/ Slave 架构，包括一个 Master 和若干个 Slave。Master 上运行 JobTracker，Slave 上运行 TaskTracker Hadoop 框架是用 Java 实现的，但是，MapReduce 应用程序则不一定要用 Java 来写 适合用 MapReduce 来处理的数据集需要满足一个前提条件：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 Map 和 Reduce 函数 函数 输入 输出 说明 Map $&lt;k_1, v_1&gt;$ 如: &lt;行号, “a b c”&gt; List($&lt;k_2, v_2&gt;$) 如: &lt;”a”, 1&gt; &lt;”b”, 1&gt; &lt;”c”, 1&gt; 1.将小数据集进一步解析成一批 $&lt;key,value&gt;$ 对，输入Map函数中进行处理 2.每一个输入的 $&lt;k1,v1&gt;$ 会输出一批 $&lt;k2,v2&gt;$。$&lt;k2,v2&gt;$ 是计算的中间结果 Reduce $&lt;k_2, List(v_2)&gt;$ 如: &lt; “a”, &lt;1,1,1&gt; &gt; $&lt;k_3, v_3&gt;$ 如: &lt;”a”, 3&gt; 输入的中间结果 $&lt;k2,List(v2)&gt; $中的 $List(v2)$ 表示是一批属于同一个 $k_2$ 的 $value$ Map 函数将输入地元素转换成 $&lt;key, value&gt;$ 形式地键值对，键和值地类型也是任意的，其中键不同于一般的标志属性，即键没有唯一性，不能作为输出地身份标识。 MapReduce 体系结构 MapReduce 主要有以下 4 个部分组成： Client 用户编写的 MapReduce 程序通过 Client 提交到 JobTracker 端 用户可通过 Client 提供的一些接口查看作业运行状态 JobTracker JobTracker 负责资源监控和作业调度 JobTracker 监控所有 TaskTracker 与 Job 的健康状况，一旦发现失败，就将相应的任务转移到其他节点 JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器 (TaskScheduler)，而调度器会在资源出现空闲时，选择合适的任务去使用这些资源 TaskTracker TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker，同时接收 JobTracker 发送过来的命令并执行相应的操作 (如启动新任务、杀死任务等) TaskTracker 使用 “slot” 等量划分本节点上的资源量 (CPU、内存等)。一个 Task 获取到一个 slot 后才有机会运行，而 Hadoop 调度器的作用就是将各个TaskTracker 上的空闲 slot 分配给 Task 使用。slot 分为 Map slot 和 Reduce slot 两种，分别供 MapTask 和 Reduce Task 使用 Task Task 分为 Map Task 和 Reduce Task 两种，均由 TaskTracker 启动 MapReduce 工作流程工作流程概述 MapReduce 的核心思想是“分而治之”。如上图所示，把一个大的数据拆分成许多小的数据块再多台机器上并行处理，对于一个大 MapReduce 作业： 首先会被拆分成许多个 Map 任务在多台机器上并行执行，每个 Map 任务通常运行在数据存储的节点上，以避免额外的数据传输开销。 Map 任务结束后，会生成以 $&lt;key, value&gt;$ 形式的中间结果。 这些中间结果会被分到多个 Reduce 任务在多台机器上执行，具有相同 $key$ 的 $&lt;key, value&gt;$ 会被发送到同一个 Reduce 任务那里 Reduce 任务会对中间结果进行汇总计算得到最后结果，并输出到分布式文件系统中。 需要指出： 不同的 Map 任务之间不会进行通信 不同的 Reduce 任务之间也不会发生任何信息交换 用户不能显式地从一台机器向另一台机器发送消息 所有的数据交换都是通过 MapReduce 框架自身去实现的 MapReduce 各个执行阶段 下面是 MapReduce 算法的执行过程 MapReduce 框架使用 InputFormat 模块做 Map 前的预处理，比如验证输入的格式是否符合输入的定义；然后，将输入文件切分为逻辑上的多个 InputSplit，InputSlit 是 MapReduce 对文件进行处理和运算的输入单位，只是一个逻辑概念。 由于 InputSplit 并非物理切分，因此需要 RecordReader (RR) 根据 InputSplit 中的信息来处理 InputSplit 中的具体记录，加载数据并转换为适合 Map 任务读取的键值对，输入给 Map 任务 Map 任务会根据用户自定义的映射规则，输出一系列的 $&lt;key, value&gt;$ 作为中间结果。 对 Map 输出的中间结果 $&lt;key, value&gt;$ 进行 Shuffle 操作得到有序的 $&lt;key, value-list&gt;$ Reduce 以一系列 $&lt;key, value-list&gt;$ 中间结果作为输入，执行用户定义的逻辑，输出结果给 OutputFormat 模块。 OutputFormat 模块会验证输出目录是否已经存在以及输出结果类型是否符合配置文件中的配置类型，如果都满足，就输出 Reduce 的结果到分布式文件系统。 Split (分片) 的解释 执行过程中的第 1 步提到 InputSplit 只是一个逻辑概念，并非物理切分: HDFS 以固定大小的 block 为基本单位存储数据，而对于 MapReduce 而言，其处理单位是 split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 Map 任务的数量 Hadoop 为每个 split 创建一个 Map 任务，split 的多少决定了 Map 任务的数目。大多数情况下，理想的分片大小是一个 HDFS 块。 Reduce 任务的数量 最优的 Reduce 任务个数取决于集群中可用的 Reduce 任务槽 (Reduce slot) 的数目 通常设置比 Reduce 任务槽数目稍微小一些的 Reduce 任务个数 (这样可以预留一些系统资源处理可能发生的错误) Shuffle 过程详解Shuffle 是指对 Map 输出结果进行一系列的分区 (Portiton)、排序 (Sort)、合并 (Combine)、归并 (Merge) 等处理并交给 Reduce 的过程。 Map 端的 Shuffle 过程 Map 的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件、并清空缓存。 当启动溢写操作时，首先需要吧缓存中的数据进行分区 (Portition)，然后对每个分区的数据进行排序 (Sort) 和合并 (Combine)，之后再写入磁盘。 每次溢写操作会生成一个新的磁盘文件，随着 Map 任务的执行，会生成多个磁盘溢写文件。 在 Map 任务全部结束前，这些溢写文件会被归并 (Merge) 成一个大的磁盘文件，然后通知相应的 Reduce 任务来领取属于自己的数据。 Combine 和 Merge 的区别: 两个键值对 &lt;”a”, 1&gt; 和 &lt;”a”, 1&gt;， 如果合并，会得到 &lt;”a”, 2&gt;， 如果归并，会得到 &lt;”a”, &lt;1, 1&gt;&gt; Reduce 端的 Shuffle 过程 Reduce 任务通过 RPC 向 JobTracker 询问 Map 任务是否已经完成，若完成，则领取数据 Reduce 领取数据先放入缓存，来自不同 Map 机器，先归并 (Merge)，再合并 (Combine)，写入磁盘 多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce MapReduce 应用程序执行过程 MapRduce 过程详解 实例分析：WordCount问题描述 输入: 一个包含大量单词的文本文件 输出: 文件中每个单词及其出现次数 (频数)，并按照单词字母顺序排序，每个单词和其频数占一行，单词和频数之间有间隔 输入 输出 Hello World, Hello Hadoop, Hello MapReduce Hadoop 1Hello 3MapReduce 1World 1 WordCount 执行过程实例 MapReduce 的具体应用MapReduce可以很好地应用于各种计算问题 关系代数运算（选择、投影、并、交、差、连接） 关系的选择运算 关系的投影运算 关系的并、交、差运算 关系的自然连接运算 分组与聚合运算 矩阵-向量乘法 矩阵乘法 用 MapReduce 实现关系的自然连接 假设有关系 R(A，B) 和 S(B,C)，对二者进行自然连接操作 使用 Map 过程，把来自 R 的每个元组 $&lt;a,b&gt;$ 转换成一个键值对 $&lt;b, &lt;R, a&gt;&gt;$ ，其中的键就是属性B的值。把关系 R 包含到值中，这样做使得我们可以在 Reduce 阶段，只把那些来自 R 的元组和来自 S 的元组进行匹配。类似地，使用 Map 过程，把来自 S 的每个元组 $&lt;b,c&gt;$，转换成一个键值对 $&lt;b, &lt;S, c&gt;&gt;$ 所有具有相同B值的元组被发送到同一个 Reduce 进程中，Reduce 进程的任务是，把来自关系 R 和 S 的、具有相同属性 B 值的元组进行合并 Reduce 进程的输出则是连接后的元组 $&lt;a, b, c&gt;$，输出被写到一个单独的输出文件中 MapReduce 编程实践 (Hadoop 3.1.3) 编程实践来自于厦门大学数据库实验室林子雨老师编写的案例：http://dblab.xmu.edu.cn/blog/2481-2/MapReduce是谷歌公司的核心计算模型，Hadoop开源实现了MapReduce。MapReduce将复杂的、运行于大规模集群上的并行计算过程高度抽象到了两个函数：Map和Reduce，并极大地方便了分布式编程工作，编程人员在不会分布式并行编程的情况下，也可以很容易将自己的程序运行在分布式系统上，完成海量数据的计算。本教程以一个词频统计任务为主线，详细介绍MapReduce基础编程方法。环境是Ubuntu18.04（或Ubuntu16.04或Ubuntu14.04）、Hadoop3.1.3，开发工具是Eclipse。 词频统计任务要求首先，在 Linux 系统本地创建两个文件，即文件 wordfile1.txt 和 wordfile2.txt。在实际应用中，这两个文件可能会非常大，会被分布存储到多个节点上。但是，为了简化任务，这里的两个文件只包含几行简单的内容。需要说明的是，针对这两个小数据集样本编写的 MapReduce 词频统计程序，不作任何修改，就可以用来处理大规模数据集的词频统计。 文件 wordfile1.txt 的内容如下：12I love SparkI love Hadoop 文件 wordfile2.txt 的内容如下：12Hadoop is goodSpark is fast 假设 HDFS 中有一个 /user/hadoop/input 文件夹，并且文件夹为空，请把文件 wordfile1.txt 和 wordfile2.txt 上传到 HDFS中 的 input 文件夹下。现在需要设计一个词频统计程序，统计 input 文件夹下所有文件中每个单词的出现次数，也就是说，程序应该输出如下形式的结果：1234567fast 1good 1Hadoop 2I 2is 2love 2Spark 2 WordCount 程序代码: wordcount.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.io.IOException;import java.util.Iterator;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount { public WordCount() { } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs(); if(otherArgs.length &lt; 2) { System.err.println(\"Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;\"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(WordCount.TokenizerMapper.class); job.setCombinerClass(WordCount.IntSumReducer.class); job.setReducerClass(WordCount.IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for(int i = 0; i &lt; otherArgs.length - 1; ++i) { FileInputFormat.addInputPath(job, new Path(otherArgs[i])); } FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); System.exit(job.waitForCompletion(true)?0:1); } public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private static final IntWritable one = new IntWritable(1); private Text word = new Text(); public TokenizerMapper() { } public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while(itr.hasMoreTokens()) { this.word.set(itr.nextToken()); context.write(this.word, one); } } }public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public IntSumReducer() { } public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException { int sum = 0; IntWritable val; for(Iterator i$ = values.iterator(); i$.hasNext(); sum += val.get()) { val = (IntWritable)i$.next(); } this.result.set(sum); context.write(key, this.result); } }} 详细过程请访问林子雨老师的教程——MapReduce编程实践(Hadoop3.1.3)。","link":"/2020/05/28/intro2BigData7/"},{"title":"大数据技术原理与应用 - (8). Hive - 基于 Hadoop 的数据仓库","text":"【第三篇】 - 大数据处理与分析, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据处理与分析的相关技术，包括 第7章 - MapReduce 第8章 - Hive - 基于 Hadoop 的数据仓库 第9章 - Hadoop 的优化与发展 第10章 - Spark 第11章 - 流计算 第12章 - 图计算 第13章 - 数据可视化 Hive 是一个基于 Hadoop 的数据仓库平台。通过 Hive，我们可以方便地进行 ETL 的工作。Hive 定义了一个类似于 SQL 的查询语言: HiveQL，能够将用户编写的 HiveQL 转化为相应的 Mapreduce 程序基于 Hadoop 执行，可以说 Hive 实质就是一款基于 HDFS 的 MapReduce 计算框架，对存储在 HDFS 中的数据进行分析和管理。 Hive 概述数据仓库概念数据仓库 (Data Warehouse) 是一个面向主题的 (Subject Oriented)、集成的 (Integrated)、相对稳定的 (Non-Volatile)、反映历史变化 (Time Variant) 的数据集合，用于支持管理决策。 传统数据仓库面临的挑战 无法满足快速增长的海量数据存储需求 无法有效处理不同类型的数据 计算和处理能力不足 Hive 简介 Hive 是一个构建于 Hadoop 顶层的数据仓库工具 支持大规模数据存储、分析，具有良好的可扩展性 某种程度上可以看作是用户编程接口，本身不存储和处理数据 依赖分布式文件系统 HDFS 存储数据 依赖分布式并行计算模型 MapReduce 处理数据 定义了简单的类似 SQL 的查询语言 —— HiveQL 用户可以通过编写的 HiveQL 语句运行 MapReduce 任务 可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到 Hadoop 平台上 是一个可以提供有效、合理、直观组织和使用数据的分析工具 Hive 特点Hive 具有的特点非常适用于数据仓库 采用批处理方式处理海量数据 Hive 需要把 HiveQL 语句转换成 MapReduce 任务进行运行 数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化 提供适合数据仓库操作的工具 Hive 本身提供了一系列对数据进行提取、转换、加载 (ETL) 的工具，可以存储、查询和分析存储在 Hadoop 中的大规模数据 这些工具能够很好地满足数据仓库各种应用场景 优点： 可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器 cpu i7-6700k 4 核心 8 线程，8 核心 16 线程，内存 64G =&gt; 128G 延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数 良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行 缺点： Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中 (当前选择的 hive-2.3.2 的版本支持记录级别的插入操作) Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。 Hive 不支持事务 (因为不没有增删改，所以主要用来做 OLAP (联机分析处理)，而不是 OLTP (联机事务处理)，这就是数据处理的两大级别)。 Hive 和 RDBMS 的对比 总结： Hive 具有 SQL 数据库的外表，但应用场景完全不同，Hive 只适合用来做海量离线数 据统计分析，也就是数据仓库。 Hive在企业中的部署和应用 Hive 批处理，时延高，报表不需要实时出结果。 HBase 可以实时反馈。 Mahout 中实现和提供了很多机器学习的 api，帮助企业分析人员能够快速构建商业智能应用。 HDFS：分布式文件存储；MapReduce：分布式数据处理 Hive DesignHive Architecture 用户接口 (UI) 模块包括 CLI、HWI、JDBC、ODBC、Thrift Server 驱动模块 (Driver) 包括 Compiler、Optimizer、Execution Engine 等，负责把 HiveSQL 语句转换成一系列 MapReduce 作业 元数据 (metadata) 存储模块 (Metastore) 是一个独立的关系型数据库 (自带 derby 数据库，或 MySQL 数据库) Source 中文翻译 Apache Hive wiki: https://cwiki.apache.org/confluence/display/Hive/Design Figure above shows the major components of Hive and its interactions with Hadoop. As shown in that figure, the main components of Hive are: UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed. Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces. Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore. Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored. Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components. Figure also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table‘s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9). 上图展示了 Hive 的的主要组件以及它们与 Hadoop 的交互过程，这些 Hive 的主要组件包括: UI - 用户界面。用于向系统提交查询和其他操作。截至2011年，该系统具有命令行界面，并且正在开发基于Web的GUI。 Driver - 接收查询的组件。该组件实现了 session handle 的概念，并提供了以 JDBC / ODBC 接口为模型的 API 的执行和获取。 Compiler - 解析查询的组件。对不同的查询块和查询表达式进行语义分析，并最终借助 table 和从 metastore 查找的 partition metadata (分区元数据) 来生成执行计划。 Metastore - 该组件用于存储所有在仓库中的各种的 tables 和 partitons 的结构信息，包括列和列类型信息，读写数据所需的 serializers 和 deserializers 以及存储数据的相应 HDFS 文件。 Execution Engine - 执行由 Compiler 创建的执行计划的组件。该计划是阶段的 DAG。Execution Engine 管理 execution plan 不同阶段之间的 dependencies，并在适当的系统组件上执行这些阶段。 该图还展示了一个典型的查询如何在系统中 flow。UI调用调用 Driver 的执行接口（步骤1）。Driver 为 query 创建 session handle，并将 query 发送到 Compiler 以生成执行计划（步骤2）。Compiler 从 metastore 中获取必要的 metadata（步骤3和4）。该 metadata 用于对查询树中 (query tree) 的表达式进行类型检查，以及基于查询谓词 (query predicates) 修剪 partition。编译器生成的计划（步骤5）是阶段的DAG，每个阶段是一个 Map / Reduce 作业，metadata 操作或对 HDFS 的操作。对于 Map / Reduce阶段，该 plan 包含 Map Operator Trees（在 Mapper 上执行的 Operator Trees）和 Reduce Operator Tree（用于需要 Reducers 的运算）。Execution Engine 将这些 stages 将这些阶段提交给适当的组件（步骤6、6.1、6.2和6.3）。在每个任务（Mapper/ Reducer）中，与 table 或中间输出关联的 deserializer 用于从 HDFS 文件读取行，并将这些行通过关联的 operator tree 传递。生成输出后，将通过 serializer 将其写入临时 HDFS 文件中（如果不需要 Reduce 操作，则在 Napper 发生）。临时文件用于向计划的后续 Map/ Reduce 阶段提供数据。对于 DML 操作，最终的临时文件将移动到 table 的位置。此方案用于确保脏数据 (dirty data) 不会被读取（文件重命名是 HDFS 中的 atomic 操作）。对于查询 (query)，Execution Engine直接从 HDFS 读取临时文件的内容，作为 Driver 进行提取调用的一部分（步骤7、8和9）。 .content .tabs ul { margin: 0; } .tab-content { display: none; } function onTabClick (event) { var tabTitle = $(event.currentTarget).children('span:last-child').text(); $('.article .content .tab-content').css('display', 'none'); $('.article .content .tabs li').removeClass('is-active'); $('#' + tabTitle).css('display', 'block'); $(event.currentTarget).parent().addClass('is-active'); } Hive Data ModelData in Hive is organized into: Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases. Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the &lt;table location&gt;/ds=&lt;date&gt; directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = ‘2008-09-01’ would only have to look at files in &lt;table location&gt;/ds=2008-09-01/ directory in HDFS. Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table). Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1. Hive 工作原理 SQL 语句转换成 MapReduce 作业的基本原理 Hive 中 SQL 查询转换成 MapReduce 作业的过程 SQL 语句转换成 MapReduce 的基本原理 join 的实现原理 过程与第 7 章 —— 用 MapReduce 实现关系的自然连接类似，请参见上一章 MapReduce。 group by 的实现原理 存在一个分组 (Group By) 操作，其功能是把表 Score 的不同片段按照 rank 和 level 的组合值进行合并，计算不同 rank 和 level 的组合值分别有几条记录：1select rank, level ,count(*) as value from score group by rank, level Hive 中 SQL 查询转换成 MapReduce 作业的过程当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作: 驱动模块接收该命令或查询编译器 对该命令或查询进行解析编译 由优化器对该命令或查询进行优化计算 该命令或查询通过执行器进行执行 请参见上面 Sec 2.1 - Hive Architecture 介绍的图及过程。 由 Hive 驱动模块中的编译器对用户输入的 SQL 语言进行词法和语法解析，将 SQL 语句转化为抽象语法树的形式 抽象语法树的结构仍很复杂，不方便直接翻译为 MapReduce 算法程序，因此，把抽象语法书转化为查询块 把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符 重写逻辑查询计划，进行优化，合并多余操作，减少 MapReduce 任务数量 将逻辑操作符转换成需要执行的具体 MapReduce 任务 对生成的 MapReduce 任务进行优化，生成最终的 MapReduce 任务执行计划 由 Hive 驱动模块中的执行器，对最终的 MapReduce 任务进行执行输出 几点说明 当启动 MapReduce 程序时，Hive 本身是不会生成 MapReduce 算法程序的 需要通过一个表示 “Job 执行计划” 的 XML 文件驱动执行内置的、原生的 Mapper 和 Reducer 模块 Hive 通过和 JobTracker 通信来初始化 MapReduce 任务，不必直接部署在 JobTracker 所在的管理节点上执行 通常在大型集群上，会有专门的网关机来部署 Hive 工具。网关机的作用主要是远程操作和管理节点上的 JobTracker 通信来执行任务 数据文件通常存储在 HDFS 上，HDFS 由名称节点管理 Hive High Availability (Hive HA) 将若干 Hive 实例纳入一个资源池，并由 HAProxy 提供一个统一的对外接口 对于程序开发人员，就把它认为是一台超强 “hive” 就可以。每次它接收到一个 Hive 查询连接后，都会轮询资源池里可用的 Hive 资源。这样，能充分使用每个 Hive server，减少压力。在拿到 Hive 连接后，Hive HA 会首先进行逻辑可用测试，这个逻辑规则可自行配置。 如果逻辑可用，则直接把客户端的HIVE 查询连接 relay到该hive server。 若逻辑不可用，则将该hiveserver放入黑名单，然后继续读取池里其他hive server进行连接测试。 Hive HA 每隔一段时间 (可配置)，对黑名单中的 Hive server 进行处理，通过和节点管理服务器通讯，重启该 Hive server。如果重启后可用，则将该 Hive 从黑名单中移除，加入资源池。","link":"/2020/05/29/intro2BigData8/"},{"title":"大数据技术原理与应用 - (9). Hadoop 的优化与发展","text":"【第三篇】 - 大数据处理与分析, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据处理与分析的相关技术，包括 第7章 - MapReduce 第8章 - Hive - 基于 Hadoop 的数据仓库 第9章 - Hadoop 的优化与发展 第10章 - Spark 第11章 - 流计算 第12章 - 图计算 第13章 - 数据可视化 介绍 Hadoop 2.0 对 1.0 不足与局限的解决方案，介绍 Hadoop 2.0 的新特性以及新一代资源管理调度框架 YARN 框架。 Hadoop 局限与改进Hadoop 的局限与不足Hadoop 1.0 的核心组件 (仅指 MapReduce 和 HDFS，不包括 Hadoop 生态系统内的 Pig、Hive、HBase 等其他组件)，主要存在以下不足： 抽象层次低。有时候为了实现一个简单的功能，也需要编写大量的代码。 表达能力有限。MapReduce 把复杂的分布式编程工作高度抽象到两个函数上，即 Map 和 Reduce，但是实际生产环境中的一些应用是无法用简单的 Map 和 Reduce 来完成。 开发者自己管理作业（Job）之间的依赖关系。一个 Job 只包含 Map 和 Reduce 两个阶段，通常的实际应用问题需要大量的 job 进行协作才能顺利解决，这些 job 之间往往存在复杂的依赖关系，但是 MapReduce 框架本身并没有提供相关的机制对这些依赖关系进行有效管理。 难以看到程序整体逻辑。 执行迭代操作效率低。对于一些机器学习、数据挖掘任务，往往需要多轮迭代才能得到结果，如遗传算法。但是 Map、Reduce 的过程需要对 HDFS 的数据进行读写，反复读写 HDFS 文件中的数据，大大降低了迭代操作的效率。 资源浪费（Map和Reduce分两阶段执行）。MapReduce 的框架设计中，Reduce 任务需要等待所有 Map 任务都完成才可以开始，造成了不必要的资源浪费。 实时性差（适合批处理，不支持实时交互式）。 两方面改进 一方面是 Hadoop 自身两大核心组件 MapReduce 和 HDFS 的架构设计改进 另一方面是 Hadoop 生态系统其它组件的不断丰富，加入了 Pig、Tez、Spark 和 Kafka 等新组件 改进 Hadoop 框架 完善 Hadoop 生态 组件 功能 解决 Hadoop 中存在的问题 Pig 处理大规模数据的脚本语言，用户只需要编写几条简单的语句，系统会自动转换为MapReduce作业 抽象层次低，需要手工编写大量代码 Spark 基于内存的分布式并行编程框架，具有较高的实时性，并且较好支持迭代计算 延迟高，而且不适合执行迭代计算 Oozie 工作流和协作服务引擎，协调Hadoop上运行的不同任务 没有提供作业（Job）之间依赖关系管理机制，需要用户自己处理作业之间依赖关系 Tez 支持DAG作业的计算框架，对作业的操作进行重新分解和组合，形成一个大的DAG作业，减少不必要操作 不同的MapReduce任务之间存在重复操作，降低了效率 Kafka 分布式发布订阅消息系统，一般作为企业大数据分析平台的数据交换枢纽，不同类型的分布式系统可以统一接入到Kafka，实现和Hadoop各个组件之间的不同类型数据的实时高效交换 Hadoop生态系统中各个组件和其他产品之间缺乏统一的、高效的数据交换中介 HDFS2.0 的新特性HDFS HAHDFS 1.0 的不足 HDFS 1.0 只存在一个 NameNode，存在单点故障问题。 第二名称节点 (SecondaryNameNode) 无法解决单点故障问题，非 NameNode 的热备份。 HDFS HA（High Availability）是为了解决单点故障问题，是 NameNode 的热备份解决方案。 HA 集群设置两个 NameNode，“活跃 (Active)”和“待命 (Standby)”。 两种 NameNode 的状态同步，可以借助于一个共享存储系统来实现。 一旦活跃 NameNode 出现故障，就可以立即切换到待命 NameNode。 Zookeeper 确保只有一个 NameNode 在对外服务。 NameNode 维护映射信息，数据节点同时向两个 NameNode 汇报信息。 HDFS FederationHDFS 1.0 的不足 单点故障问题 不可以水平扩展（是否可以通过纵向扩展来解决？） 系统整体性能受限于单个 NameNode 的吞吐量 单个 NameNode 难以提供不同程序之间的隔离性 HDFS HA 是热备份，提供高可用性，但是无法解决可扩展性、系统性能和隔离性 HDFS Federation 的设计 在 HDFS Federation 中，设计了多个相互独立的 NameNode，使得 HDFS 的命名服务能够水平扩展，这些 NameNode 分别进行各自命名空间和块的管理，相互之间是联盟 (Federation) 关系，不需要彼此协调。并且向后兼容。 HDFS Federation 中，所有 NameNode 会共享底层的 DataNode 存储资源，DataNode 向所有 NameNode 汇报。 属于同一个命名空间的块构成一个 “块池”。 HDFS Federation 的访问方式 对于 Federation 中的多个命名空间，可以采用客户端挂载表 （lient SideMount Table) 方式进行数据共享和访问。 客户可以访问不同的挂载点来访问不同的子命名空间 把各个命名空间挂载到全局 “挂载表” (mount-table) 中，实现数据全局共享 同样的命名空间挂载到个人的挂载表中，就成为应用程序可见的命名空间 HDFS Federation 相对于 HDFS1.0 的优势 HDFS集群扩展性。多个 NameNode 各自分管一部分目录，使得一个集群可以扩展到更多节点，不再像 HDFS1.0 中那样由于内存的限制制约文件存储数目。 性能更高效。多个 NameNode 管理不同的数据，且同时对外提供服务，将为用户提供更高的读写吞吐率。 良好的隔离性。用户可根据需要将不同业务数据交由不同 NameNode 管理，这样不同业务之间影响很小。 需要注意: HDFS Federation 并不能解决单点故障问题，也就是说，每个 NameNode 都存在在单点故障问题，需要为每个 NameNode 部署一个后备 NameNode，以应对 NameNode 挂掉对业务产生的影响。 YARNMapReduce1.0 的缺陷 存在单点故障 JobTracker “大包大揽”导致任务过重 (任务多时内存开销大，上限4000节点) 容易出现内存溢出 (分配资源只考虑 MapReduce 任务数，不考虑 CPU、内存) 资源划分不合理 (强制划分为 slot ，包括 Map slot 和 Reduce slot) YARN 设计思路 MapReduce1.0 既是一个计算框架，也是一个资源管理调度框架 到了 Hadoop2.0 以后，MapReduce1.0 中的资源管理调度功能，被单独分离出来形成了 YARN，它是一个纯粹的资源管理调度框架，而不是一个计算框架 被剥离了资源管理调度功能的 MapReduce 框架就变成了 MapReduce2.0，它是运行在 YARN 之上的一个纯粹的计算框架，不再自己负责资源调度管理服务，而是由 YARN 为其提供资源管理调度服务 YARN 体系结构 ResourceManager: 处理客户端请求 启动/监控 ApplicationMaster 监控 NodeManager 资源分配与调度 ApplicationMaster: 为应用程序申请资源，并分配给内部任务 任务调度、监控与容错 NodeManager: 单个节点上的资源管理 处理来自 ResourceManger 的命令 处理来自 ApplicationMaster 的命令 YARN 工作流程 用户编写客户端 (Clinet) 应用程序，向 YARN 提交应用程序，提交的内容包括 ApplicationMaster 程序、启动 ApplicationMaster 的命令、用户程序等 YARN 中的 ResourceManager 负责接收和处理来自 Clinet 的请求，为应用程序分配一个容器，在该容器中启动一个 ApplicationMaster ApplicationMaster 被创建后会首先向 ResourceManager 注册 ApplicationMaster 采用轮询的方式向 ResourceManager 申请资源 ResourceManager 以“容器”的形式向提出申请的 ApplicationMaster 分配资源 在容器中启动任务（运行环境、脚本） 各个任务向 ApplicationMaster 汇报自己的状态和进度 应用程序运行完成后，ApplicationMaster 向 ResourceManager 的应用程序管理器注销并关闭自己 Hadoop 生态中代表性功能组件PigPig 是什么 Pig 是一个基于 Apache Hadoop 的大规模 High-Level 数据分析平台。Pig 可以 在MapReduce，Apache Tez 或 Apache Spark 中执行其 Hadoop 作业，它提供的 SQL-LIKE 语言 - Pig Latin，该语言的编译器会把类 SQL 的数据分析请求转换为一系列经过优化处理的 MapReduce 运算。Pig 为复杂的海量数据并行计算提供了一个简单的操作和编程接口，Pig Latin 可以使用 user-defined functions (UDF) 进行扩展，用户可以使用 Java，Python，JavaScript，Ruby 或 Groovy 编写这些函数，然后直接从该语言调用。 Pig 可以加载数据、表达转换数据以及存储最终结果。 Pig Latin Pig Latin是这样的一个操作：通过对关系进行处理产生另外一组关系。Pig Latin语言在书写一条语句的时候能够跨越多行，但是必须以半角的分号来结束。 Pig Latin语言通常按照下面的流程来编写： 通过一条 LOAD语句 从文件系统中读取数据； 通过一系列 转换语句 对数据进行处理； 通过一条 STORE语句 把处理结果输出到文件系统中，或者使用一条DUMP语句把处理结果输出在屏幕上。 LOAD和STORE语句有严格的语法规定，用户很容易就能掌握，关键是如何灵活的使用转换语句对数据进行处理。 Pig Latin 例子 example123456789101112131415161718input_lines = LOAD '/tmp/my-copy-of-all-pages-on-internet' AS (line:chararray);-- Extract words from each line and put them into a pig bag-- datatype, then flatten the bag to get one word on each rowwords = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;-- filter out any words that are just white spacesfiltered_words = FILTER words BY word MATCHES '\\\\w+';-- create a group for each wordword_groups = GROUP filtered_words BY word;-- count the entries in each groupword_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;-- order the records by countordered_word_count = ORDER word_count BY count DESC;STORE ordered_word_count INTO '/tmp/number-of-words-on-internet'; 上面的程序将生成并行的可执行任务，这些任务可以分布在 Hadoop 集群中的多台计算机上，以计算数据集（例如互联网上的所有网页）中的单词数。 TezTez 是 Apache 开源的支持 DAG作业的计算框架，它直接源于 MapReduce 框架，核心思想是将 Map 和 Reduce 两个操作进一步拆分。 Map 被拆分成 Input、Processor、Sort、Merge 和 Output Reduce 被拆分成 Input、Shuffle、Sort、Merge、Processor 和 Output 等 分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的 DAG 作业。通过 DAG 作业的方式运行 MapReduce 作业，提供了程序运行的整体处理逻辑，就可以去除工作流当中多余的 Map 阶段，减少不必要的操作，提升数据处理的性能。 MapReduce 和 Tez 对比 Tez的优化主要体现在，去除了 连续两个作业之间的 “写入HDFS” 每个工作流中多余的 Map 阶段 比如以下 Hive SQL 会翻译成四个MR作业，而采用 Tez 则生成一个 DAG 作业，可大大减少磁盘 IO： 12345SELECT a.state, COUNT(*),AVERAGE(c.price)FROM aJOIN b ON (a.id = b.id)JOIN c ON (a.itemId = c.itemId)GROUP BY a.state Tez 作用 在 Hadoop2.0 生态系统中 (参考上图: Hadoop 1.0 到 2.0)，MapReduce、Hive、Pig 等计算框架，都需要最终以 MapReduce 任务的形式执行数据分析，因此 Tez 框架可以发挥重要的作用 借助于 Tez 框架实现对 MapReduce、Pig 和 Hive 等的性能优化 可以解决现有 MapReduce 框架在迭代计算 (如 PageRank algorithms) 和交互式计算方面的问题 (Tez+Hive) 与 Impala、Dremel 和 Drill 的区别？ Tez 在解决 Hive、Pig 延迟大、性能低等问题的思路，是和那些支持实时交互式查询分析的产品（如 Impala、Dremel 和 Drill 等）是不同的。 Impala、Dremel 和 Drill 的解决问题思路是抛弃 MapReduce 计算框架，不再将类似 SQL 语句的 HiveQL 或者 Pig 语句翻译成 MapReduce 程序，而是采用与商用并行关系数据库类似的分布式查询引擎，可以直接从 HDFS 或者 HBase 中用 SQL 语句查询数据，而不需要把 SQL 语句转化成 MapReduce 任务来执行，从而大大降低了延迟，很好地满足了实时查询的要求。 Tez 则不同，比如，针对 Hive 数据仓库进行优化的 “Tez+Hive” 解决方案，仍采用 MapReduce 计算框架，但是对 DAG 的作业依赖关系进行了裁剪，并将多个小作业合并成一个大作业，这样，不仅计算量减少了，而且写HDFS次数也会大大减少。 SparkHadoop 缺陷: 其 MapReduce 计算模型延迟过高，无法胜任实时、快速计算的需求，因而只适用于离线批处理的应用场景 中间结果写入磁盘，每次运行都从磁盘读数据 在前一个任务执行完成之前，其他任务无法开始，难以胜任复杂、多阶段的计算任务 Spark 最初诞生于伯克利大学的 APM 实验室，是一个可应用于大规模数据处理的快速、通用引擎，如今是 Apache 软件基金会下的顶级开源项目之一。Spark 在借鉴Hadoop MapReduce 优点的同时，很好地解决了 MapReduce 所面临的问题: 内存计算，带来了更高的迭代运算效率 基于 DAG 的任务调度执行机制，优于 MapReduce 的迭代执行机制 详见下一章 - 第10章: Spark 内容。 Kafka Kafka 是一种高吞吐量的分布式发布订阅消息系统，用户通过 Kafka 系统可以发布大量的消息，同时也能实时订阅消费消息 Kafka 可以同时满足在线实时处理和批量离线处理 在公司的大数据生态系统中，可以把 Kafka 作为数据交换枢纽，不同类型的分布式系统（关系数据库、NoSQL 数据库、流处理系统、批处理系统等），可以统一接入到Kafka，实现和 Hadoop 各个组件之间的不同类型数据的实时高效交换","link":"/2020/06/01/intro2BigData9/"},{"title":"大数据技术原理与应用 - (10). Spark","text":"【第三篇】 - 大数据处理与分析, 《大数据技术原理与应用, 林子雨》 本篇介绍大数据处理与分析的相关技术，包括 第7章 - MapReduce 第8章 - Hive - 基于 Hadoop 的数据仓库 第9章 - Hadoop 的优化与发展 第10章 - Spark 第11章 - 流计算 第12章 - 图计算 第13章 - 数据可视化 Spark 最初诞生于伯克利大学的 APM 实验室，是一个可应用于大规模数据处理的快速、通用引擎，如今是 Apache 软件基金会下的顶级开源项目之一。Spark 在借鉴Hadoop MapReduce 优点的同时，很好地解决了 MapReduce 所面临的问题。 Spark 概述Spark 简介Spark 最初诞生于 University of California, Berkeley 的 APM 实验室，是一个可应用于大规模数据处理的快速、通用引擎，如今是 Apache 软件基金会下的顶级开源项目之一。 Spark 特点: 运行速度快：使用 DAG 执行引擎以支持循环数据流与内存计算。 容易使用：支持使用 Scala、Java、Python 和 R 语言进行编程，可以通过 Spark Shell 进行交互式编程。 通用性：Spark 提供了完整而强大的技术栈，包括 SQL 查询、流式计算、机器学习和图算法组件。 运行模式多样：可运行于独立的集群模式中，可运行于 Hadoop 中，也可运行于 Amazon EC2 等云环境中，并且可以访问 HDFS、Cassandra、HBase、Hive 等多种数据源。 Spark 和 Hadoop 对比Hadoop 缺点: 表达能力有限。计算必须要转化成 Map 和 Reduce 两个操作，但这并不适合所有情况。 磁盘 IO 开销大。每次执行和结束都需分别从磁盘中读取和写入数据或中间结果。 延迟高。 任务之间的衔接涉及 IO 开销 在前一个任务执行完成之前，其他任务就无法开始，难以胜任复杂、多阶段的计算任务 Spark 在借鉴Hadoop MapReduce 优点的同时，很好地解决了 Hadoop MapReduce 所面临的问题。相较于 Hadoop，Spark 的优点有: Spark 的计算模式也属于 MapReduce，但不局限于 Map 和 Reduce 操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活。 提供 内存计算，可将中间结果放到内存中，带来了更高的迭代运算效率。 基于 DAG 的任务调度执行机制，优于 MapReduce 的迭代执行机制。 使用 Hadoop 进行迭代计算非常耗资源，因为频繁从磁盘中读写数据。 Spark 将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据。 Spark 生态系统在实际应用中，大数据处理主要包括一下三大类型 (应用场景)。 复杂的批量数据处理: 时间跨度通常在数十分钟到数小时之间。(Hadoop) 基于历史数据的交互式查询: 时间跨度通常在数十秒到几分钟之间。(Impala) 基于实时数据流的数据处理: 时间跨度通常在数百毫秒到数秒之间。(Storm) 对于互联网公司而言，通常会同时存在以上三种场景，就同时需要部署三种不同的软件，多软件难免带来一些问题。 不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换。 不同软件需要不同的开发和维护团队，带来了较高的使用成本。 比较难以对同一个集群中的各个系统进行统一的资源协调和分配。 Spark 生态系统已经成为伯克利数据分析软件栈 BDAS (Berkeley Data Analytics Stack) 的重要组成部分。 Spark 生态系统中提供的许多组件，可以满足不同的应用场景，如 Spark Core, Spark SQL, Spark Streaming, MLLib, GraphX 等。 Spark Core 包含 Spark 的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等，主要面向批数据处理。 Spark SQL 允许开发人员直接处理 RDD (Resillient Distributed Dataset)，同时也可以查询 Hive, HBase 等外部数据源。其能够统一处理关系表和 RDD，使得开发人员不需要自己编写 Spark 应用程序。 Spark Streaming 支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流数据分解成一系列短小的批处理作业，每个批处理作业都可以使用 Spark Core 进行快速处理。 MLLib 提供常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等。 GraphX 是 Spark 中用于图计算的 API，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。 应用场景 时间跨度 其他框架 Spark 生态系统中的组件 复杂的批量数据处理 小时级 MapReduce, Hive Spark 基于历史数据的交互式查询 分钟级、秒级 Impala, Dremel, Drill Spark SQL 基于实时数据流的数据处理 毫秒、秒级 Storm, S4 Spark Streaming 基于历史数据的数据挖掘 - Mahout MLlib 图结构数据的处理 - Pregel, Hama GraphX Spark 运行架构基本概念 RDD：是 Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。 DAG：是 Directed Acyclic Graph（有向无环图）的简称，反映 RDD 之间的依赖关系。 Executor：是运行在工作节点 （WorkerNode）的一个进程，负责运行 Task。 Application：用户编写的 Spark 应用程序。 Task：运行在 Executor 上的工作单元。 Job：一个 Job 包含多个 RDD 及作用于相应 RDD 上的各种操作。 Stage：是 Job 的基本调度单位，一个 Job 会分为多组 Task，每组 Task 被称为 Stage，或者也被称为 TaskSet，代表了一组关联的、相互之间没有 Shuffle 依赖关系的任务组成的任务集。 架构设计 Spark 运行架构包括集群资源管理器 (Cluster Manager)、运行作业任务的工作节点 (Worker Node)、每个应用的任务控制节点 (Driver) 和每个工作节点上负责具体任务的执行进程 (Executor)。 资源管理器可以自带或 Mesos 或 YARN。 与 Hadoop MapReduce 计算框架相比，Spark 所采用的 Executor 有两个优点： 利用多线程来执行具体的任务，减少任务的启动开销。 Executor 中有一个 BlockManager 存储模块，会将内存和磁盘共同作为存储设备，有效减少 IO 开销。 一个 Application 由一个 Driver 和若干个 Job 构成，一个 Job 由多个 Stage 构成，一个 Stage 由多个没有 Shuffle 关系的 Task 组成。 当执行一个 Application 时，Driver 会向集群管理器申请资源，启动 Executor，并向 Executor 发送应用程序代码和文件，然后在 Executor 上执行 Task，运行结束后，执行结果会返回给 Driver，或者写到 HDFS 或者其他数据库中。 Spark 运行基本流程 首先为应用构建起基本的运行环境，即由 Driver 创建一个 SparkContext，进行资源的申请、任务的分配和监控 资源管理器为 Executor 分配资源，并启动 Executor 进程 SparkContext 根据 RDD 的依赖关系构建 DAG 图，DAG 图提交给 DAGScheduler 解析成 Stage，然后把一个个 TaskSet 提交给底层调度器 TaskScheduler 处理；Executor 向 SparkContext 申请 Task，Task Scheduler 将 Task 发放给 Executor 运行，并提供应用程序代码 Task 在 Executor 上运行，把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完毕后写入数据并释放所有资源 总体而言，Spark 运行架构具有以下特点： 每个 Application 都有自己专属的 Executor 进程，并且该进程在 Application 运行期间一直驻留。Executor 进程以多线程的方式运行 Task Spark 运行过程与资源管理器无关，只要能够获取 Executor 进程并保持通信即可 Task 采用了数据本地性和推测执行等优化机制 RDD 运行原理设计背景 许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，共同之处是，不同计算阶段之间会重用中间结果。 目前的 MapReduce 框架都是把中间结果写入到 HDFS 中，带来了大量的数据复制、磁盘 IO 和序列化开销。 RDD 就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同 RDD 之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储。 RDD 概念 一个 RDD 就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个 RDD 可分成多个分区，每个分区就是一个数据集片段，并且一个 RDD 的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。 RDD 提供了一种高度受限的共享内存模型，即 RDD 是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建 RDD，或者通过在其他 RDD 上执行确定的转换操作（如 map、join 和 group by）而创建得到新的 RDD。 RDD 提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型。 RDD 提供的转换接口都非常简单，都是类似 map、filter、groupBy、join 等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（不适合网页爬虫）。 表面上 RDD 的功能很受限、不够强大，实际上 RDD 已经被实践证明可以高效地表达许多框架的编程模型（比如 MapReduce、SQL、Pregel）。 Spark 用 Scala 语言实现了 RDD 的 API，程序员可以通过调用 API 实现对 RDD 的各种操作。 RDD 典型的执行过程如下： RDD 读入外部数据源进行创建 RDD 经过一系列的转换（Transformation）操作，每一次都会产生不同的 RDD，供给下一个转换操作使用 最后一个 RDD 经过“动作”操作进行转换，并输出到外部数据源图 10-8 RDD 执行过程的一个实例这一系列处理称为一个 Lineage（血缘关系），即 DAG 拓扑排序的结果 优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单 RDD 特性Spark 采用 RDD 以后能够实现高效计算的原因主要在于： 高效的容错性 现有容错机制：数据复制或者记录日志 RDD：血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作 中间结果持久化到内存，数据在内存中的多个 RDD 操作之间进行传递，避免了不必要的读写磁盘开销 存放的数据可以是 Java 对象，避免了不必要的对象序列化和反序列化 RDD 之间的依赖关系 窄依赖表现为一个父 RDD 的分区对应于一个子 RDD 的分区或多个父 RDD 的分区对应于一个子 RDD 的分区 宽依赖则表现为存在一个父 RDD 的一个分区对应一个子 RDD 的多个分区 Stage 的划分Spark 通过分析各个 RDD 的依赖关系生成了 DAG，再通过分析各个 RDD 中的分区之间的依赖关系来决定如何划分 Stage，具体划分方法是： 在 DAG 中进行反向解析，遇到宽依赖就断开 遇到窄依赖就把当前的 RDD 加入到 Stage 中 将窄依赖尽量划分在同一个 Stage 中，可以实现流水线计算 如上图，被分成三个 Stage，在 Stage2 中，从 map 到 union 都是窄依赖，这两步操作可以形成一个流水线操作。 流水线操作实例： 分区 7 通过 map 操作生成的分区9，可以不用等待分区8到分区10这个 map 操作的计算结束，而是继续进行 union 操作，得到分区13，这样流水线执行大大提高了计算的效率。 Stage的类型包括两种：ShuffleMapStage 和 ResultStage，具体如下： ShuffleMapStage：不是最终的 Stage，在它之后还有其他 Stage，所以，它的输出一定需要经过 Shuffle 过程，并作为后续 Stage 的输入；这种 Stage 是以 Shuffle 为输出边界，其输入边界可以是从外部获取数据，也可以是另一个 ShuffleMapStage 的输出，其输出可以是另一个 Stage 的开始；在一个 Job 里可能有该类型的 Stage，也可能没有该类型 Stage； ResultStage：最终的 Stage，没有输出，而是直接产生结果或存储。这种 Stage 是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个 ShuffleMapStage 的输出。在一个 Job 里必定有该类型 Stage。因此，一个 Job 含有一个或多个 Stage，其中至少含有一个 ResultStage。 RDD 运行过程 通过上述对 RDD 概念、依赖关系和 Stage 划分的介绍，结合之前介绍的 Spark 运行基本流程，再总结一下 RDD 在 Spark 架构中的运行过程： 创建 RDD 对象； SparkContext 负责计算 RDD 之间的依赖关系，构建 DAG； DAGScheduler 负责把 DAG 图分解成多个 Stage，每个 Stage 中包含了多个 Task，每个 Task 会被 TaskScheduler 分发给各个 WorkerNode 上的 Executor 去执行。","link":"/2020/06/03/intro2BigData10/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"CoursesReview","slug":"CoursesReview","link":"/tags/CoursesReview/"},{"name":"Data Mining","slug":"Data-Mining","link":"/tags/Data-Mining/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"Big Data","slug":"Big-Data","link":"/tags/Big-Data/"},{"name":"Cheat Sheet","slug":"Cheat-Sheet","link":"/tags/Cheat-Sheet/"},{"name":"Image Classification","slug":"Image-Classification","link":"/tags/Image-Classification/"},{"name":"Reading Note","slug":"Reading-Note","link":"/tags/Reading-Note/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Statistical Learning","slug":"Statistical-Learning","link":"/tags/Statistical-Learning/"},{"name":"Image Segmentation","slug":"Image-Segmentation","link":"/tags/Image-Segmentation/"},{"name":"Medical Imaging","slug":"Medical-Imaging","link":"/tags/Medical-Imaging/"},{"name":"FrontEnd","slug":"FrontEnd","link":"/tags/FrontEnd/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"}],"categories":[{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"University of Edinburgh","slug":"University-of-Edinburgh","link":"/categories/University-of-Edinburgh/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/categories/Computer-Vision/"},{"name":"Big Data","slug":"Big-Data","link":"/categories/Big-Data/"},{"name":"Deep Learning","slug":"Machine-Learning/Deep-Learning","link":"/categories/Machine-Learning/Deep-Learning/"},{"name":"Pose Estimation","slug":"Computer-Vision/Pose-Estimation","link":"/categories/Computer-Vision/Pose-Estimation/"},{"name":"Data Mining","slug":"Big-Data/Data-Mining","link":"/categories/Big-Data/Data-Mining/"},{"name":"Networks Architecture","slug":"Computer-Vision/Networks-Architecture","link":"/categories/Computer-Vision/Networks-Architecture/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Statistical Learning","slug":"Machine-Learning/Statistical-Learning","link":"/categories/Machine-Learning/Statistical-Learning/"}]}