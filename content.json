{"pages":[{"title":"About Me 关于我","text":"展航(Zhanhang)，即将毕业的学生； 一个统计学学生正在想着计算机方向挣扎。 学习学习 之前一直没有把学过的内容总结起来的意识，最近搭了这个GitHub Page，希望能慢慢总结起来方便自己翻查自己的笔记。‘…’ 代表还未接触到的内容，希望自己能持续学习。 数学基础 微积分（Calculus） 线性代数（Linear Algebra） 概率论与数理统计（Statistics） 统计分析 回归分析（Regression Analysis） 时间序列分析（Time Series Analysis） 主成分分析（Principal Component Analysis） 因子分析（Factor Analysis） 聚类分析（Cluster Analysis） 相关分析（Correlation Analysis） 对应分析（Correspondence Analysis） 方差分析(ANOVA/Analysis of Variance) 数据可视化 R-ggplot2; plotly Python-Matplotlib; seaborn; plotly Dashboard 数据挖掘 标准化 + 异常值检测（预处理） 数据降维（PCA, kernel PCA, MDS, Isomap） 模型评估（Generalisation, over/under-fitting, Cross-validation…） 机器学习 k近邻法（k-Nearest Neighbors） 朴素贝叶斯（naïve Bayes） 支持向量机（SVM） 决策树（Decision tree） 集成学习（Ensemble Learning） k均值聚类（k-Means Clustering） 关联规则 EM算法（Expectation–maximization algorithm） 隐马尔可夫模型（HMM） 条件随机场（CRF） … 推荐系统 深度学习 DNN CNN RNN（LSTM） GAN … Python for Machine Learning scikit-learn Keras PyTorch … 计算机视觉 OpenCV … 自然语言处理 …（mark： Stanford CS224n Natural Language Processing with Deep Learning：Bilibili, YouTube Stanford CS224u Natural Language Understanding：Bilibili, YouTube） 强化学习（Reinforcement Learning） …（mark：UCL Reinforcement Learning：Biliblili, YouTube） 提升效率的工具 Git（推荐：廖雪峰Git教程） Linux（Cheat Sheet） LaTeX Resources: MIT 6.S191: Intro to Deep Learning. Official website: http://introtodeeplearning.com/#schedule （Bilibili; YouTube）","link":"/about/index.html"}],"posts":[{"title":"Keras 笔记","text":"To take notes about the essential Keras elements to build basic neural networks. 1. Single Layer Neural Network (Linear Regression) 单层神经网络相当于（非）线性回归模型，第一个例子是构建一个最简单一元线性回归模型。 创建数据 单层神经网络模型需要数据进行训练，因此我们使用 numpy 创建一些人造数据，且我们的 $y$ 为 $y = ax+b$ 。 1234567891011121314import numpy as npimport tensorflow as tffrom tensorflow.keras import layersimport matplotlib.pyplot as pltplt.style.use('seaborn')# create dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) #randomize the dataY = 2*X + 10 + np.random.normal(0, 0.05, (200,))# plot dataplt.scatter(X, Y)plt.show() 构建神经网络 tf.keras.models.Sequential() 1234tf.keras.models.Sequential( layers=None, name=None) 用Keras创建神经网络，我们首先需要用 tf.keras.models.Sequential() 来建立网络，这里面只有一个argument，即 layers。这里添加神经层的方法有两种，一种是建立 model 的时候直接放入神经层，另一种是通过 model.add 来添加。如： 12345678# option 1model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1, activation = None, use_bias = True)])# option 2model = tf.keras.models.Sequential()model.add(tf.keras.layers.Dense(1, activation = None, use_bias = True)) tf.keras.layers.Dense() 上面的例子中我们给 model 添加了一个 tf.keras.layers.Dense(), 它代表最典型的全连接神经网络层，即每个输入节点连接到每个输出节点。 123456789101112tf.keras.layers.Dense( units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Arguments: units: 正整数，输出空间的维数。 activation：要使用的激活功能。如果未指定任何内容，则不应用激活（即 “linear” activation：a（x）= x）。 use_bias：Boolean，该层是否使用bias vector。 …其他看文档: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense models.Sequential().compile() 给 model 添加完神经层后，必须进行 model.compile() 才能继续后续的模型训练，并且在 model.compile 的时候需要指定 optimizer 和 loss。 12345678910model.compile( optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, distribute=None,) Arguments: optimizer：String（优化器名称）或优化器实例。请参阅tf.keras.optimizers。 loss：String（目标函数的名称）或目标函数。见tf.losses。 metrics：模型在训练和测试期间要评估的度量列表。通常，您将使用 metrics = ['accuracy']。 …其他看文档：https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential 完整例子 2. Multiple Layer Neural Network 3. Convolutional Neural Network 4. Recurrent Neural Network4.1 RNN 4.2 LSTM 5. Generative Adversarial Network Reference TensorFlow Tutorial：https://www.tensorflow.org/tutorials/ TensorFlow Guide：https://www.tensorflow.org/guide/","link":"/2019/06/12/Keras-notes/"},{"title":"OpenCV Cheat Sheet (Python)","text":"Opencv-python是OpenCv的python API，包括数百种计算机视觉算法。这个页面记录了一些常用的Opencv-python函数，以便作为我的快速参考。 1. 安装和使用 Installation and Usage 安装 1pip install opencv-python 事实上一共有四种不同的packages，安装其中一个即可，四个packages都用同一个名字cv2（对于其他的packages，详见Documentation）。 2. OpenCV中的GUI特性2.1 图像基本操作(读取，显示，保存) 三个函数cv.imread(), cv.imshow(), cv.imwrite()分别用于读取；显示和保存图像。 cv.imread()：读取图像 1retval = cv.imread(filename[, flags]) 参数 Parameters: filename: 文件名 (Name of file to be loaded.) flags: 定义读取图片的方式，如彩色或灰色等 (takes values of cv::ImreadModes specified the way image should be read.){cv::IMREAD_UNCHANGED = -1,cv::IMREAD_GRAYSCALE = 0,cv::IMREAD_COLOR = 1,cv::IMREAD_ANYDEPTH = 2,cv::IMREAD_ANYCOLOR = 4,cv::IMREAD_LOAD_GDAL = 8,…} 例子 Example： 1234import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0) cv.imshow()：显示图像 1None = cv.imshow(winname, mat) 参数 parameters: winname：显示窗的名字 (Name of the window.) mat： 要显示的图像名 (Image to be shown.) 例子 Example: 1cv.imshow('image',img) cv.imwrite()：保存图像 1retval = cv.imwrite(filename, img[, params]) 参数 Parameters： filename：文件名（Name of the file.） img：要保存的图像名（Image to be saved.） params：Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, … .) see cv::ImwriteFlags 例子 Example： 1cv.imwrite('messigray.png',img) 2.2 色彩通道转换(Color conversions)：RGB - Gray RGB ↔ GRAY 除了RGB channel到灰度Y channel的转换，还有其他通道的转换，例如RGB ↔ CIE XYZ.Rec 709 with D65 white point； RGB ↔ YCrCb JPEG (or YCC)等等，详见Documentation。 RGB空间内的变换，例如添加/删除Alpha通道，反转通道顺序，转换为16位RGB颜色（R5：G6：B5或R5：G5：B5），以及转换为灰度/从灰度转换使用以下方式： $$ \\text{RGB[A] to Gray: Y←0.299⋅R+0.587⋅G+0.114⋅B} $$ and $$ \\text{Gray to RGB[A]: R←Y,G←Y,B←Y,A←max(}ChannelRange\\text{)} $$ 1cvtColor(src, dst, code, dstCn = 0) 参数 Parameters： src：输入图像 (input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC… ), or single-precision floating-point.) dst：输出图像 (output image of the same size and depth as src.) code：转换方式代码 (color space conversion code (see cv::ColorConversionCodes).){cv2.COLOR_RGB2GRAY,cv2.COLOR_GRAY2RGB,cv2.COLOR_BGR2GRAY,cv2.COLOR_RGB2GRAY,…} dstCn：目标图像中的通道数;如果参数为0，则从src和代码自动导出通道数 (number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code.) 例子 Examples： 123456import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0)# Convert image from Y channel to RGB channelimg2 = cv.cvtColor(img, cv.COLOR_GRAY2BGR) Additional ReadingOpenCV官方教程中文版（For Python），段力辉 译（搬运自：https://www.linuxidc.com/Linux/2015-08/121400.htm）","link":"/2019/05/12/OpenCV-Cheat-Sheet/"},{"title":"Numpy&Pandas Tutorial","text":"Numpy和Pandas对python中的数据处理很重要。尤其对于数据分析/挖掘，Pandas几乎不可或缺。写tutorial的起因是因为一次面试中被问到numpy中去重用哪个函数，发现自己对numpy的不熟悉，所以希望以此加深印象…(haven’t started yet) 1. NumpyNumpy Cheat Sheet Numpy Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF. 2. PandasPandas Cheat Sheet Pandas Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF.","link":"/2019/05/10/Numpy-Pandas-Tutorial/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/05/10/hello-world/"},{"title":"统计学习 - Statistical Learning","text":"统计学习方法笔记总结。haven’t finished yet 1. k近邻法（k-Nearest Neighbors） 直观理解: 分类：在数据中找到与某个点（目标）最近的k个点，把该点（目标）的类分为k个点中多数的类。 回归：在数据中找到与某个点（目标）最近的k个点，k个点的均值为目标点的预测值。 优点： $k$ 近邻法是个非参数学习算法，它没有任何参数（ $k$ 是超参数，而不是需要学习的参数）。 近邻模型具有非常高的容量，这使得它在训练样本数量较大时能获得较高的精度。 缺点： 计算成本很高。因为需要构建一个 $N \\times N$ 的距离矩阵，其计算量为 $O(N^2)$，其中 $N$ 为训练样本的数量。 当数据集是几十亿个样本时，计算量是不可接受的。 在训练集较小时，泛化能力很差，非常容易陷入过拟合。 无法判断特征的重要性。 1.1 k近邻模型 模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间 一般是$n$维实数向量空间$\\mathbb{R}^d$。使用的距离是欧氏距离，但也可以是其他距离，如更一般的$L_p$距离（$L_p$ distance）或Minkowski距离（Minkowski distance）。 $$ L_p(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^p \\right)^{1/p}\\ , \\quad \\quad p \\geqslant 1\\\\ \\overrightarrow{\\boldsymbol{x}_i}, \\overrightarrow{\\boldsymbol{x}_j} \\in \\mathcal{X}, \\quad \\overrightarrow{\\boldsymbol{x}_i} = (x_i^{(1)}, x_i^{(2)}, \\dots, x_i^{(d)})^T ,\\quad {\\boldsymbol{x}_j} = (x_j^{(1)}, x_j^{(2)}, \\dots, x_j^{(d)})^T $$ 当 $p＝2$ 时，称为欧氏距离(Euclidean distance)：$L_2(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^2 \\right)^{1/2}$ 当 $p＝1$ 时，称为曼哈顿距离(Manhattan distance)：$L_1(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|$ 当 $p＝\\infty$ 时，它是各个坐标距离的最大：$L_{\\infty}(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\underset{i}{\\text{max}}|x_{i,l} - x_{j,l}|$ 不同的距离度量所确定的最近邻点是不同的。 k值的选择 k值的选择会对k近邻法的结果产生重大影响。 较小的k值：k值的减小就意味着整体模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，“学习”的近似 误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对 预测结果起作用。但是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。 优点：减少”学习”的偏差。 缺点：增大”学习”的方差（即波动较大）。 较大的k值：k值的增大就意味着整体的模型变得简单。相当于用较大邻域中的训练实例进行预测。其优点是可以减 少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似 的）训练实例也会对预测起作用，使预测发生错误。 优点：减少”学习”的方差（即波动较小）。 缺点：增大”学习”的偏差。 应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则 分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。也可以基于距离的远近进行加权投票。 多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为 $$ \\mathcal{f} : \\mathbb{R}^d \\rightarrow \\{c_1, c_2, \\dots, c_K \\} $$ 对给定的实例$\\overrightarrow{x} \\in \\mathcal{X}$，其最近邻的k个训练实例点构成集合$\\mathcal{N}_k (\\overrightarrow{x})$。如果涵盖$\\mathcal{N}_k (\\overrightarrow{x})$的区域的类别是cj，那么误分类率是 $$ L = \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i \\neq c_m) = 1 - \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m) $$ $L$ 就是训练数据的经验风险。要使经验风险最小，则使得 $\\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$ 最大。即多数表决：$c_m = \\underset{c_m}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$。所以多数表决规则等价于经验风险最小化。 1.2 k近邻算法 k近邻的分类算法： 输入：训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$ 为实例的特征向量，$y_i \\in \\overrightarrow{\\boldsymbol{y}} = \\left\\{ c_1, c_2, \\dots, c_k \\right\\} $为实例的类别，$i = 1, 2, \\dots, n$ 。 输出： 实例 $\\overrightarrow{\\boldsymbol{x}}$ 所属的类 y 。 步骤： 根据给定的距离度量，在训练集 $D$ 中找出与 $\\overrightarrow{\\boldsymbol{x}}$ 最邻近的k个点，涵盖这k个点的 $\\overrightarrow{\\boldsymbol{x}}$ 的邻域记作 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ ; 在 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ 中根据分类决策规则（如多数表决）决定 $\\overrightarrow{\\boldsymbol{x}}$ 的类别 $y$ ： $$ y = \\underset{c_j}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_j), \\quad i = 1, 2, \\dots, n; \\quad j = 1, 2, \\dots, k $$ $I$ 为指示函数，即当$y_i＝c_j$ 时 $I$ 为1，否则 $I$ 为0。 2. 朴素贝叶斯 直观理解: 朴素贝叶斯和贝叶斯分类（回归）器都是基于贝叶斯理论进行预测或者分类的模型。过程是利用训练数据学习 $P(X|Y)$ 和 $P(Y)$ 的估计，得到联合概率分布 $P(X,Y)＝P(Y)P(X|Y)$ ，然后利用贝叶斯定理进行预测，将输入 $\\overrightarrow{\\boldsymbol{x}}$ 分到后验概率最大的类。 $$ P(Y|X) = \\frac{P(X, Y)}{P(X)} = \\frac{P(Y)P(X|Y)}{\\sum_{Y}P(Y)P(X|Y)} \\propto P(Y)P(X|Y) \\\\ \\hat{y} = \\underset{c_k}{\\text{argmax }}P(Y=c_k) \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) $$ 条件独立假设: $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}}) & = \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) \\end{aligned} $$ 优点： 性能相当好，它速度快，可以避免维度灾难。 支持大规模数据的并行学习，且天然的支持增量学习。 缺点： 无法给出分类概率，因此难以应用于需要分类概率的场景。 2.1 贝叶斯定理 全概率公式 (Law of Total Probability Theorem)： $$ P(A) = \\sum_{j=1}^{n}P(A|B_j)P(B_j) $$ 贝叶斯定理 (Bayes’ theorem) 在数学上表示为以下等式： $$ \\begin{aligned} P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{\\sum_{j=1}^{d}P(B|A_j)P(A_j)} \\propto P(B|A) \\cdot P(A) \\end{aligned} $$ 2.2 朴素贝叶斯朴素贝叶斯（naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。 2.2.1 基本方法及解释2.2.1.1 基本方法 设输入空间 $X \\subseteq R_n$ 为 $d$ 维向量的集合，输出空间为类标记集合 $y ＝{c_1, c_2, \\dots, c_K}$。输入为特征向量$\\overrightarrow{\\boldsymbol{x}} \\in X$ ，输出为类标记（class label）$y \\in Y $ 。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布，训练数据集 $D$ 由 $P(X,Y)$ 独立同分布产生。 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 步骤： 学习以下先验概率分布及条件概率分布。 $$ \\begin{aligned} \\text{(先验概率)} & \\quad \\quad P(Y=c_k), \\quad \\ k=1,2,\\dots, k\\\\ \\text{(条件概率)} & \\quad \\quad P(X = \\overrightarrow{\\boldsymbol{x}} | P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\end{aligned} $$ 条件独立性的假设： $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k) & = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\\\ & = \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ 条件独立假设等于 是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。 朴素贝叶斯法分类时，对给定的输入 $\\overrightarrow{\\boldsymbol{x}}$ ，通过学习到的模型计算后验概率分布 $P(Y＝ c_k|X＝\\overrightarrow{\\boldsymbol{x}})$。 $$ \\begin{aligned} P(Y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) & = \\frac{P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}{\\sum_{k} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}\\\\ \\text{(独立性假设)} & = \\frac{P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}{\\sum_{k} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}, \\quad k = 1,2,\\dots,K \\end{aligned} $$ 将后验概率最大的类作为x的类输出: $$ \\begin{aligned} \\hat{y} = f(\\overrightarrow{\\boldsymbol{x}}) = \\underset{c_k}{\\text{argmax }} P(Y = c_k) \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ Note：因为后验概率的分母都相同，因此在这可以忽略。 2.2.1.2 后验概率最大的含义等价于期望风险最小化 (Optional) 假设选择 0-1损失函数： $$ \\begin{aligned} L(Y, f(X)) = \\begin{cases} 0, \\quad \\ Y = f(X)\\\\ 1, \\quad \\ Y \\neq f(x) \\end{cases} \\end{aligned} $$ 期望风险函数 $$ \\begin{aligned} R_{exp}(f) = \\mathbb{E} \\left[ L(Y, f(X)) \\right] = \\sum_{\\overrightarrow{\\boldsymbol{x}}}\\sum{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(\\overrightarrow{\\boldsymbol{x}}, y) = \\mathbb{E}_X \\left[ \\sum_{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(y | \\overrightarrow{\\boldsymbol{x}}) \\right] \\end{aligned} $$ 为了使期望风险最小化，只需对 $\\overrightarrow{\\boldsymbol{x}}$ 逐个极小化 $$ \\begin{aligned} f(\\overrightarrow{\\boldsymbol{x}}) & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} L(c_k, y) P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} P(y \\neq c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} (1 - P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}))\\\\ & = \\underset{y \\in Y}{\\text{argmax}} \\sum_{k} P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) \\end{aligned} $$ 2.2.1.3 参数估计 - 极大似然估计朴素贝叶斯中可以利用极大似然估计先验概率和条件概率。 $$ \\begin{aligned} P(Y = c_k) \\frac{1}{n} \\sum_{i=1}^{n} I(y_i = c_k)\\\\ P(X^{(j)} = a_{jl} \\ |\\ Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} \\ |\\ y_i = c_k \\right)}{\\sum_{i=1}^{n} I(y_i=c_k)}\\\\ j = 1, 2, \\dots, d; \\quad l = 1, 2, \\dots, S_j; \\quad k = 1, 2, \\dots, K \\end{aligned} $$ 其中，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征；$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值；$I$ 为指示函数。 2.2.1.4 贝叶斯估计用极大似然估计可能会出现所要估计的概率值为0的情况。解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计（or also known as add-alpha smoothing）是 $$ \\begin{aligned} P_{\\alpha}(X^{(j)} = a_{jl} | Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} | y_i = c_k \\right) + \\alpha}{\\sum_{i=1}^{n} I(y_i=c_k) + S_j \\alpha}\\\\ \\end{aligned} $$ 其中，$\\alpha \\geqslant 0$ 当 $\\alpha = 0$ 为极大似然估计，当 $\\alpha = 1$为拉普拉斯平滑（Laplace smoothing, also known as add-one smoothing）。 2.2.2 算法 朴素贝叶斯算法（naïve Bayes algorithm） 利用极大使然估计先验概率 $P(Y = c_k)$ 和条件概率 $P(X^{(j)} = a_{jl} \\ |\\ Y = c_k)$，如Sec 2.2.1.2 对于给定的实例 $\\overrightarrow{\\boldsymbol{x}}＝(x^{(1)},x^{(2)},…,x^{(d)})^T$ ，计算 $$ \\begin{aligned} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k), \\quad \\ k = 1, 2, \\dots, K \\end{aligned} $$ 确定实例 $\\overrightarrow{\\boldsymbol{x}}$ 的类 $$ \\hat{y} = \\underset{c_k}{\\text{argmax}} \\ P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k) $$ 例子： 3. 支持向量机 直观理解: 支持向量机（support vector machines，SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习算法是求解凸二次规划的最优化算法。 间隔最大使它有别于感知机； 感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 支持向量机还支持核技巧，从而使它成为实质上的非线性分类器。 支持向量机学习方法包含构建由简至繁的模型： 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机； 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机； 非线性支持向量机；当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机 特征向量之间的内积就是核函数，使用核函数可以学习非线性支持向量机。 非线性支持向量机等价于隐式的在高维的特征空间中学习线性支持向量机，这种方法称作核技巧。 3.1 线性可分支持向量机 假设给定一个特征空间上的训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$, $y_i \\in {-1, +1}, \\; i=1,2,\\dots,n$ .$\\overrightarrow{\\boldsymbol{x}}_i$ 为第 $i$ 个(d维)特征向量，也称为实例，$(x_i，\\ y_i)$ 称为样本点。 假设数据集线性可分。则学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$\\boldsymbol{W}_{k \\times d} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}}$，它由法向量 $\\boldsymbol{W}$ 和截距 $\\overrightarrow{\\boldsymbol{b}}$ 决定，可用 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 来表示。例如： 给定线性可分训练数据集，通过间隔最大化或等 价地求解相应的凸二次规划问题学习得到的分离超平面为 $\\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} = 0$ 以及相应的分类决策函数 $f(\\overrightarrow{\\boldsymbol{x}}) = sign( \\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} )$ 3.1.1 函数间隔 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i)$ 的函数间隔为 $$\\hat{\\gamma}_i = y_i (\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的函数间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 函数间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 可以将一个点距离分离超平面的远近来表示分类预测的可靠程度： 一个点距离分离超平面越远，则该点的分类越可靠。 一个点距离分离超平面越近，则该点的分类则不那么确信。 在超平面 $\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}} = 0$ 确定的情况下： $| \\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}}| $ 能够相对地表示点 $\\overrightarrow{\\boldsymbol{x}}_i$ 距离超平面的远近。 3.1.2 几何间隔 因为只要成比例地改变 \\boldsymbol{W} 和 $b$ ，例如将它们改为 $2w$ 和 $2b$ ，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量 \\boldsymbol{W} 加某些约束，如规范化， $||\\boldsymbol{W}||＝ 1$ ，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i$ 的几何间隔为 $$\\hat{\\gamma}_i = y_i (\\frac{\\boldsymbol{W}}{||\\boldsymbol{W}||} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\frac{\\overrightarrow{\\boldsymbol{b}}}{||\\boldsymbol{W}||} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的几何间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 几何间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 由定义可知函数间隔和几何间隔有下列的关系：$$\\gamma_i = \\frac{\\hat{\\gamma}_i}{||W||}; \\quad \\gamma = \\frac{\\hat{\\gamma}}{||W||}$$ 当 $||\\boldsymbol{W}|| = 1$ 时，函数间隔和几何间隔相等。 如果超平面参数 $\\boldsymbol{W}$ 和 $\\overrightarrow{\\boldsymbol{b}}$ 成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 Reference 李航. 统计学习方法[M]. 清华大学出版社， 2012年3月. Ai算法工程师手册","link":"/2019/05/31/StatisticalLearning/"},{"title":"DME - Data Mining and Exploration (INF11007) Revision","text":"Remeber to read the ‘Lab’ section of each chapter 1. Exploratory Data Analysis1.1 Numberical Data Description1.1.1 Location Non-robust Measure Sample Mean (arithmetic mean or average): $\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ for random variable: $\\mathbb{E}[x] = \\int xp(x) dx$ Robust Measure Median: $$ median(x) = \\begin{cases} x_{[(n+1)\\mathbin{/}2]}& \\text{; if $n$ is odd}\\\\ \\frac{1}{2}[x_{(n\\mathbin{/}2)}+x_{(n\\mathbin{/}2)+1}]& \\text{; if $n$ is even} \\end{cases} $$ Mode: Value that occurs most frequent $\\alpha_{th}$ Sample Quantile (rough data point, i.e. $q_{\\alpha} \\approx x_{([n\\alpha])}$) $Q_{1} = q_{0.25}$, $Q_{2} = q_{0.5}$, $Q_{3} = q_{0.75}$ ExampleData_1=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9]Data_2=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9000] DataSet Mean Median $Q_{1}$ $Q_{3}$ Data 1 3.0 2.5 1.0 4.0 Data 2 902.1 2.5 1.0 4.0 1.1.2 Scale Non-robust Measure Sample Variance: $Var(x) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x})^2$ for random variable: $Var[x] = \\int [x-\\mathbb{E}[x]]^2 dx$ Standard Deviation: $Std(x) = \\sqrt{Var(x)}$ Robust Measure Median Absolute Deviation(MAD): $$MAD(x) = median[|x_{i} - median(x)|]$$ IQR(interquartile range): $$IQR = Q_{3} - Q_{1}$$ 1.1.3 Shape: Non-robust Measure Skewness: measures the asymmetry of data $$skew(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{3}$$ Kurtosis: measures how heavy the tails of distribution are, in other word, measures how often x takes on values that are considerable larger or smaller than its standard deviation.$$kurt(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{4}$$ Robust Measure Galtons’s measure of skewness: $$skew(x) = \\frac{(Q_{3}-Q_{2})-(Q_{2}-Q_{1})}{Q_{3}-Q_{1}}$$ Robust kurtosis: $$kurt(x) = \\frac{(q_{7/8}-q_{5/8})-(q_{3/8}-q_{1/8})}{Q_{3}-Q_{1}}$$ 1.1.4 Multivariate Measure: Sample Covariance: $$Cov(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x}) (y_{i} - \\hat{y})$$ for random variable: $Cov[x, y] = \\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])] = \\mathbb{E}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]$ Pearson’s Correlation Coefficient:$$\\rho(x,y) = \\frac{\\text{cov}(x,y)}{Std(x) Std(y)}$$ $\\rho=0$ doesn’t mean statistical independent, since it only measures linear correlation $-1 \\le \\rho \\le 1$ Simple way to measure non-linear correlation: $\\rho(g(x),g(y)) = \\frac{\\text{cov}(g(x),g(y))}{Std(g(x)) Std(g(y))}$ Covariance Matrix: $$Cov[X] = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{T}]$$ Eigenvalue decomposition: $Cov[X] = U\\Lambda U^{T}$ $\\sum_{i=1}^{d}Var[x_{i}]=trace(Var[X])=\\sum_{i=1}^{d} \\lambda_{i}$ $cov[Ax+b] = Acov[x]A^{T}$ Correlation Matrix:$$\\rho(X) = diag\\left( \\frac{1}{std(X)} \\right) Cov[X]diag\\left( \\frac{1}{std(X)} \\right)$$ Rank Correlation - Kendall’s $\\tau$: $$\\tau(x,y) = \\frac{n_{c}(x,y) - n_{d}(x,y)}{n(n-1)/2}$$ $n_c$: total number of concordant pairs, $n_d$: total number of disconcordant pairs 1.2 Data Visualisation1.3 Data Preprocessing:1.3.1 Standardisation:Normalising data to have 0 (sample) mean and unit (sample) variance: Centering Matrix: $$C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}$$ Where, $1_n = [1, 1, \\dots, 1]^T$ Multiplying it from right: removes sample mean of each row, i.e., $X = \\tilde{X}C_{n}$ left: removes sample mean of each column 1.3.2 Outlier Detection: Tukey’s fences: $[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)] = [Q_1 - k \\times IQR, Q_3 + k \\times IQR]$ Typically, $k = 1.5$ for outlier removal 1.4 Lab for Chapter.1 2. Principal Component Analysis(PCA)2.1 PCA by Variance Maximisation2.1.1 Sequential ApproachPrincipal Component(PC) direction: $\\boldsymbol{w}$, projected data: $\\boldsymbol{w}^{T} \\boldsymbol{x}$ The First Principal Component Direction: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{1}}}{\\text{maximise}} & & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = Var(z_{1}) \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{1}} = 1|| \\end{aligned} $$ According to the eigenvalue decomposition of convariance matrix $\\Sigma$: $\\Sigma = U \\Lambda U^{T}$ Let $\\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i} \\boldsymbol{u_{i}} = U \\boldsymbol{a}$, then $$ \\begin{aligned} & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & ||\\boldsymbol{w_{1}}|| = \\boldsymbol{w_{1}}^{T} \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ Thus, the optimisation problem can be written as: $$ \\begin{aligned} & {\\text{maximise}} & & \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & \\text{subject to} & & \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ $\\boldsymbol{a} = (1, 0, \\dots, 0)^T$ is the unique solution, if $lambda_{1} &gt; \\lambda{i}$. So the first PC direction is $$\\boldsymbol{w_{1}} = U \\boldsymbol{a} = \\boldsymbol{u_{1}}$$ , where the first PC direction given by the first eigen vector, $\\boldsymbol{u_{1}}$, of $\\Sigma$ corresponding to the first(largest) eigen value $\\lambda_{1}$. $Var(z_{1})= \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\lambda_{1}$ $\\mathbb{E}(z_{1}) = \\mathbb{E}(\\boldsymbol{w_{1}}^{T} \\boldsymbol{x}) = \\boldsymbol{w_{1}}^{T} \\mathbb{E}(\\boldsymbol{x}) = 0$ First PC scores: $\\boldsymbol{z_{1}}^{T} = \\boldsymbol{w_{1}}^{T} X_{d \\times n}$ Subsequent PC Direction $\\boldsymbol{w_{m}}$: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{m}}}{\\text{maximise}} & & \\boldsymbol{w_{m}}^T \\Sigma \\boldsymbol{w_{m}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{m}} = 1|| \\\\ & & & \\boldsymbol{w_{m}}^{T}\\boldsymbol{w_{i}} = 0 & & i = 1, 2, \\dots, m-1 \\end{aligned} $$ Solution: similar to the previous procedure $\\boldsymbol{w_{m}} = \\boldsymbol{u_{m}}$ is the m-th PC direction given by the m-th eigen vector of $\\Sigma$ corresponding to the m-th largest eigen value $\\lambda_{m}$. $Var(z_{m}) = \\lambda_{m}$, $\\mathbb{E}(z_{m}) = 0$ PCs (scores) uncorrelated: $$ \\begin{aligned} Cov(z_i, z_j) & = \\mathbb{E}(z_i z_j) - \\mathbb{E}(z_i) \\mathbb{E}(z_j)\\\\ & = \\mathbb{E}(\\boldsymbol{w_{i}}^{T} \\boldsymbol{x} \\boldsymbol{w_{j}}^{T} \\boldsymbol{x}) - 0\\\\ & = \\boldsymbol{w_{j}}^{T} \\mathbb{E}(\\boldsymbol{x} \\boldsymbol{x}) \\boldsymbol{w_{j}}^{T}\\\\ & = \\boldsymbol{w_{j}}^{T} \\Sigma \\boldsymbol{w_{j}}^{T} \\\\ & = \\boldsymbol{e_{i}}^T U^T U \\Lambda U^T U \\boldsymbol{e_{j}} \\\\ & = 0 \\end{aligned} $$ Fraction of variance explained $= \\frac{\\sum_{i}^{k} \\lambda_{i}}{\\sum_{i}^{d} \\lambda_{i}}$ how much variability in data is captured by the first k principal components. 2.1.2 Simultaneous Approach $$ \\begin{aligned} & \\text{maximise} & & \\sum_{i=1}^{k}\\boldsymbol{w_{i}}^T \\Sigma \\boldsymbol{w_{i}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, m-1\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ Subtle technical point: the sequential approach corresponds to solving this optimisation problem in greedy manner(algorithm), which doesn’t guarantee to yield optimal solution. However, sequential approach and simultaneous yield same results. 2.2 PCA by Minimisation of Approximation Error Projection Matrix: $$ P = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} = W_{k} W_{k}^{T} $$ , where $W_{k} = (\\boldsymbol{w_{1}}, \\dots, \\boldsymbol{w_{k}})$ is $d \\times k$ matrix . Approximating $\\boldsymbol{x}$ into subspace $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} \\boldsymbol{x}$ Approximation Error: $\\mathbb{E}||\\boldsymbol{x} - P \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - W_{k} W_{k}^T \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2$ Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2 \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, k\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ So, the optimal PC directions $\\boldsymbol{w_{i}}$ are the first k eigen vectors $\\boldsymbol{u_{i}}$ of $\\Sigma$ The optimal projection matrix is $P = U_k U_{k}^{T}$ $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = U_{k} U_{k}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} \\boldsymbol{u_{i}}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} z_{i}$ $\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2 = \\sum_{i=1}^{d} \\lambda_{i} - \\sum_{i=1}^{k} \\lambda_{i} = \\sum_{i=k+1}^{d} \\lambda_{i}$, which means minimising expected error = maximising variance explained. Relative Approximation Error: $$ \\frac{\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2}{\\mathbb{E}||\\boldsymbol{x}||^2} = 1 - \\frac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{i=1}^{d} \\lambda_{i}} = 1 - \\text{fraction of variance explained} $$ 2.3 PCA by Low Rank Matrix Approximation2.3.1 Approximation from Data Matrix Let $X_{d \\times n} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})$, where $\\boldsymbol{x}$ is $d \\times 1$ matrix (d-dimension). Express $X$ via its Singular Value Decomposition(SVD): $X = U S V^{T}$ , where $U_{d \\times d}$ and $V_{n \\times n}$ are orthonormal. $S$ is zero everwhere, but first r diagonal elements. Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\sum_{ij} \\left[ (X)_{ij} - (M)_{ij} \\right]^2 = ||X - \\hat{X}||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ \\end{aligned} $$ So, Optimal solution: $\\hat{X} = \\sum_{i=1}^{k} \\boldsymbol{u_i} \\boldsymbol{s_i} \\boldsymbol{v_i}^{T} = U_K S_K V_K^T$ ((truncated singular value decomposition). left singular vectors $\\boldsymbol{u_i}$ are eigen vectors of $\\Sigma$, so $\\boldsymbol{u_i}$ are PC directions. $s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$: $\\lambda_i = \\frac{s_i^2}{n}$. (Proof in Appendix A) PC scores: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = s_i \\boldsymbol{v_i}^T$ Proof: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = \\boldsymbol{u_i}^T U S V^T = \\boldsymbol{u_i}^T \\sum_{j=1}^{r}\\boldsymbol{u_j} s_j \\boldsymbol{v_j}^T = s_i \\boldsymbol{v_i}^T$ 2.3.2 Approximation from Sample Covariance Matrix Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & ||\\Sigma - M||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ & & & M^T = M \\end{aligned} $$ Optimal solution: $M = \\hat{\\Sigma} = U_k \\Lambda_k U_k^T = \\Sigma^T$, i.e., $\\sum_{i=1}^{k}\\lambda_i \\boldsymbol{u_i} \\boldsymbol{u_i}^T$ 2.3.3 Approximation from Gram Matrix Gram Matrix:$$G = X^T X \\text{, where} (G)_{ij} = \\boldsymbol{x_i}^T\\boldsymbol{x_j}$$ Gram Matrix is positive semi-definite According the SVD of $X$: $$ G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T = \\sum_{i=1}^{n} s_i^2 \\boldsymbol{v_i} \\boldsymbol{v_i}^T $$ Thus, the best rank k approximation of $G$ is $\\hat{G} = \\sum_{i=1}^{k} \\boldsymbol{v_i} s_i^2 \\boldsymbol{v_i}^T$. Denote $\\tilde{\\Lambda} = S^T S$ is the top k eigen value of $G$, $V_k = (\\boldsymbol{v_1}, \\boldsymbol{v_2}, \\dots, \\boldsymbol{v_k})_{n \\times k}$ $$ Z_k = \\sqrt{\\tilde{\\Lambda}_k} V_k^T $$ 2.3.4 Probabilistic PCA (PPCA) Advantages: PPCA can samples artificial data points (generative model). Formulation allows us to deal with missing data. Probabilistic Model: $$ Z \\sim \\mathcal{N}(0,\\,I_k)\\\\ \\epsilon \\sim \\mathcal{N}(0, \\, \\sigma^2 I_d)\\\\ \\underset{d \\times 1}{\\boldsymbol{x}} = \\underset{d \\times k}{W} \\; \\underset{k \\times 1}{\\boldsymbol{z}} + \\underset{d \\times 1}{\\boldsymbol{\\mu}} + \\underset{d \\times 1}{\\boldsymbol{\\epsilon}} $$ Joint, Conditional and Observation Distribution Conditional Distribution: $$ p(\\boldsymbol{x}|\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{\\mu},\\; \\sigma^2I_{d}) $$ Joint Distribution: $$ \\begin{aligned} p(\\boldsymbol{z},\\; \\boldsymbol{x}) & = p(\\boldsymbol{x}|\\boldsymbol{z})p(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{u},\\; \\sigma^2I_{d}) \\mathcal{N}(\\boldsymbol{z};\\; 0,\\; I_k)\\\\ & = \\frac{1}{const}exp \\left[ -\\frac{1}{2} [(\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu})^{T} (\\frac{1}{\\sigma^2}I_{d}) (\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu}) + \\boldsymbol{z}^{T} \\boldsymbol{z}] \\right] \\end{aligned} $$ Important Equations: For multivariate normal distribution: $$ \\begin{aligned} -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu}) & = -\\frac{1}{2}\\boldsymbol{x}^T \\Sigma^{-1} \\boldsymbol{x} + \\boldsymbol{x}^{T} \\Sigma^{-1}\\mu + const\\\\ & = -\\frac{1}{2}\\boldsymbol{x}^T A \\boldsymbol{x} + \\boldsymbol{x}^{T} \\xi + const \\end{aligned} $$ Thus, $\\Sigma = A^{-1}$ and $\\boldsymbol{\\mu} = \\Sigma \\ \\xi$ . Observation Distribution: $$ p(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}; \\; \\boldsymbol{\\mu}, \\; W W^{T} + \\sigma^2 I) $$ Maximum Likelihood:The maximum likelihood solutions are shown by Tipping and Bishop, 1999: $$ W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}} R\\\\ \\sigma_{ML}^2 = \\frac{1}{d-k} \\sum_{i=k+1}^{d}\\lambda_{i} $$ $U_k$ are $k$ principal eigenvectors of $\\hat{\\Sigma} = Cov(X) = \\frac{1}{n}X X^T$ . $\\Lambda_k$ is diagonal matrix with eighenvalues. $R$ is arbitrary orthogonal matrix, interpreted as a rotation in the latent space, indicating not unique solutions. Another option to find $W$ and $\\sigma^2$ is EM algorithm. Relation to PCA: The closest thing to PCA mapping is the posterior distribution $p(\\boldsymbol{z}| \\; \\boldsymbol{x})$. To find it, we can fix $\\boldsymbol{x}$ as a constant in the joint distribution $p(\\boldsymbol{z},\\; \\boldsymbol{x})$ and use the important equation just mentioned above. $$ p(\\boldsymbol{z}| \\; \\boldsymbol{x} = \\mathcal{N}(\\boldsymbol{z}; \\; M^{-1} W^{T} (\\boldsymbol{x} - \\boldsymbol{\\mu}), \\; \\sigma^2 M^{-1}) $$ , where $M = W^T W + \\sigma^2 I$ . PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\hat{\\boldsymbol{x}} = W_{ML} \\mathbb{E}(\\boldsymbol{z}|\\; \\boldsymbol{x}) = W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} $$ , where $M_{ML} = W_{ML}^{T} W_{ML} + \\sigma^{2}I \\;$ and $\\; W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}}$ . For $\\sigma^2 \\rightarrow 0$, we recover the PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\begin{aligned} W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} & = U_k \\Lambda_k^{1/2} ((U_k \\Lambda_k^{1/2})^T (U_k \\Lambda_k^{1/2}))^{-1} (U_k \\Lambda_k^{1/2})^{T} \\boldsymbol{x}\\\\ & = U_k U_k^T \\boldsymbol{x} \\end{aligned} $$ 2.4 Lab for Chapter.2 3. Dimensionality Reduction3.1 Linear Dimensionality Reduction3.1.1 From Data Matrix Observed (uncentered) data: $\\tilde{X} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})_{d \\times n}$ Center data: $X = \\tilde{X} C_n$ , where $C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}\\ $ . Option 1 - compute PC scores via eigen values decomposition: $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T = U \\Lambda U^T \\end{aligned} $$ Denote $U_k$ with the first $k$ eigen vectors of $\\Sigma$ corresponding to the top $k$ eigen values: $U_k = (\\boldsymbol{u_1}, \\boldsymbol{u_2}, \\dots, \\boldsymbol{u_k})_{d \\times k}$ PC scores: $$ \\begin{aligned} \\underset{k \\times 1}{\\boldsymbol{z}_i} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times 1}{\\boldsymbol{x}_i} , & & \\underset{k \\times n}{Z} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times n }{X} \\end{aligned} $$ Option 2 - compute PC scores via Gram Matrix: $$ \\begin{aligned} G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T \\end{aligned}\\\\ \\begin{aligned} \\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\underset{k \\times n}{V_k^T}, & & V_k = (\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_k) \\end{aligned} $$ 3.1.2 From Inner Product $$ \\begin{aligned} (G)_{ij} = \\boldsymbol{x}_i^T \\boldsymbol{x}_j & & X = \\tilde{X} C_n & & \\tilde{G} = \\tilde{X}^T \\tilde{X} \\end{aligned}\\\\ G = X^T X = C_n \\tilde{X}^T \\tilde{X} C_n = C_n \\tilde{G} C_n $$ 3.1.3 From Distance Matrix If only given squared distance $\\delta_{ij}^2$ between data points $\\tilde{\\boldsymbol{x_i}}$ and $\\tilde{\\boldsymbol{x_j}} \\ $. $$ \\delta_{ij}^2 = ||\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}||^2 = (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}})^T (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}) $$ Distance Matrix $\\Delta$ contains elements $\\delta_{ij} \\ $. $$ \\delta_{ij}^2 = ||(\\tilde{\\boldsymbol{x_i}} -\\mu) - (\\tilde{\\boldsymbol{x_j}} - \\mu)||^2 = ||\\boldsymbol{x_i} - \\boldsymbol{x_j}||^2 = (\\boldsymbol{x_i} - \\boldsymbol{x_j})^T(\\boldsymbol{x_i} - \\boldsymbol{x_j})\\\\ \\delta_{ij}^2 = ||\\boldsymbol{x_i}||^2 + ||\\boldsymbol{x_j}||^2 -2\\boldsymbol{x_i}^T \\boldsymbol{x_j} $$ Center the distance: $$ (C_n \\Delta C_n)_{ij} = (\\Delta C_n)_{ij} - \\frac{1}{n} \\sum_{i} (\\Delta C_n)_{ij} = - 2\\boldsymbol{x_i}^T \\boldsymbol{x_j}\\\\ G = -\\frac{1}{2}C_n \\Delta C_n $$ 3.2 (Non-linear) Dimensionalisty Reduction via Kernel PCA To obtain new data matrix $\\Phi$ using the transforming function $\\phi(\\boldsymbol{x}_i)$. $$ \\Phi = (\\phi_1, \\phi_2, \\dots, \\phi_n) = (\\phi(\\boldsymbol{x}_1), \\phi(\\boldsymbol{x}_2), dots, \\phi(\\boldsymbol{x}_n)) $$ Kernel Trick: inner product of some functions can be computed as: $$ \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ uncentered Gram Matrix $G$ of $\\Phi$ with elements $(\\tilde{G})_{ij}$: $$ \\tilde{G})_{ij} = \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ Polynomial kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = (\\boldsymbol{x}_i^T \\boldsymbol{x}_j)^\\alpha$Gaussian kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = exp \\left( - \\frac{||\\boldsymbol{x_i} - \\boldsymbol{x}_j||^2}{2 \\sigma^2} \\right)$ Then applying methods in Sec 3.1.2 and Sec 3.1.1 to compute PC scores. 3.3 Multidimensional Scaling (MDS)3.3.1 Metric MDS Assumption: the numerical values of dissimilarities (e.g. Euclidean distance) carry information. Optimisation Problem: $$ \\begin{aligned} \\text{minimise}& & w_{ij}(||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - \\delta_{ij})^2 \\end{aligned} $$ $\\delta_{ij}$ are dissimilarities between two data items, e.g. Euclidean Distance. $||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| \\ $ is Euclidean distance betweeen $\\boldsymbol{z}_i \\ $ and $\\ \\boldsymbol{z}_j \\ $, i.e., $\\ \\sqrt{(\\boldsymbol{z}_i - \\boldsymbol{z}_j)^T (\\boldsymbol{z}_i - \\boldsymbol{z}_j)} \\ $. $w_{ij} \\ $ are some weights specified by users. if $\\ w_{ij} = \\frac{1}{\\delta_{ij}} \\ $, the MDS is called Sammon nonlinear mapping emphasing the faithful representation of samll dissimilarities. Solved by gradient descent. 3.3.2 Non-metric MDS Assumption: only relationship between $\\ \\delta_{ij} \\ $ matters, i.e., whether $\\ \\delta_{12} &gt; \\delta_{13}\\ $ or $\\ \\delta_{12} &lt; \\delta_{13}\\ $ . Optimisation Problem: $$ \\begin{aligned} \\underset{\\boldsymbol{z_1}, \\boldsymbol{z_2}, \\dots, \\boldsymbol{z_n}, f}{\\text{minimise}}& & \\sum_{i \\le j} w_{ij} (||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - f(\\delta_{ij}))^2 \\end{aligned} $$ Actual values of $\\ \\delta_{ij} \\ $ do not matter. $f \\ $ is monotonic (non-decreasing) function converting dissimilarities to distances. Solved by iterating between optimisation w.r.t $\\ \\boldsymbol{z}_i \\ $ and optimisation w.r.t $\\ f \\ $, which can be done by regression. 3.3.3 Classical MDS: Assumption: numerical values of $\\ \\delta_{ij} \\ $ matter. Dissimilarities $\\ \\delta_{ij} \\ $ are (squared) Eucldiean distance between some unknown vectors. Distance matrix $\\ \\Delta \\ $ is formed by $\\ \\delta_{ij} \\ $ Using the method in Sec 3.1.3: Compute hypothetical Gram matrix $\\ G’ \\ $ of unknown centered data points. $$ \\begin{aligned} G = -\\frac{1}{2}C_n \\Delta C_n ,& & C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T} \\end{aligned} $$ Compute top k eigen values $\\ \\sigma_k^2 \\ $ and corresponding eigen vectors $\\ \\boldsymbol{v}_k \\ $ of $\\ G \\ $ and form $\\ \\tilde{\\Lambda}_k = diag(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_k^2) \\ $ and $\\ V_k = (\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_k)_{n \\times k}$ $\\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\; \\underset{k \\times n}{V_k^T}$ $\\Delta \\ $ is not necessary positive semi-definite, thus, some eigen values might be negative. Solution: choose $\\ k \\ $small enough to avoid negative eigen values. Classical MDS solution for $\\ k’ &lt; k \\ $ is directly given by the first $\\ k’ \\ $ corordinates of $\\ k \\ $ dimensional $\\ \\boldsymbol{z} \\ $. Alternative approximate negative definite $\\ \\Delta \\ $ by: $$ \\begin{aligned} & \\text{minimise}& & ||(-\\frac{1}{2}C_n \\Delta C_n) - M^T M||_F\\\\ & \\text{subject to}& & rank(M^T M) = k \\end{aligned} $$ 3.3.4 Isometric Features Mapping (Isomap) Steps of Isomap Construct the neighbourhood graph via ‘k nearest neighbour‘ or all data points within a certain (Euclidean) distance. Construct the shortest path (distances) as geodesic distance Construct the low dimensional embeding of these data via MDS so as to represent these data. Geodesic distance is measured by the shortest distance between them when only allowed to travel on the data manifold from one neighbouring data point to the next. Isomap well represents the circular structure when learned graph is connected. 3.4 Lab for Chapter.3 4. Predictive Modelling and Generalization4.1 Prediction and Training Loss4.1.1 Prediction Loss $$ \\mathcal{J}(h) = \\mathbb{E}_{\\hat{y}, \\ y} \\left[ \\mathcal{L}(\\hat{y}, \\ y) \\right] = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(h(\\boldsymbol{x}), \\ y) \\right] $$ The term $\\ \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\ $ means expectation w.r.t $\\ p(\\boldsymbol{x},\\ y) \\ $ . 4.1.2 Training Loss $$ \\mathcal{J}_{\\lambda}^{*} = \\underset{\\theta}{min} \\ \\mathcal{J}_{\\lambda}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\mathcal{L}(h(\\boldsymbol{x}_i; \\ \\theta), \\ y_i) \\right] $$ 4.2 Generalisation Performance4.2.1 Generalisation Loss For prediction function $$ \\mathcal{J}(\\hat{h}) = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(\\hat{h}(\\boldsymbol{x}), \\ y) \\right] $$ Done with held-out data For algorithm $$ \\bar{\\mathcal{J}}(\\mathcal{A}) = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\hat{h}) \\right] = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\mathcal{A}(D^{train})) \\right] $$ See DME Lecture Notes for more details. 4.2.2 Overfitting and Underfitting Overfitting: Reducing the model complexity, the prediction loss decreases. Underfitting: Increasing the model complexity, the prediction loss decreases. Solutions: Model Selection or Regularisation . Regularisation: $$ \\begin{aligned} & \\text{minimise} & & \\mathcal{J}_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\theta}) + \\lambda_{reg} R(\\boldsymbol{\\theta}) \\end{aligned} $$ L2 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} \\theta_i^2 \\; $ L1 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} |\\theta_i| \\; $ Either model complexity and size of training data matter generalisation performance, See 4.2.3 Example on DME Lecture Notes. 4.3 Estimating the Generalisation PerformanceWe typically need to estimate the generalisation performance twice: Once for hyperparameter selection, and once for ﬁnal performance evaluation. 4.3.1 Methods for Estimating the Generalisation PerformanceHeld-out Approach Prediction function: $$ \\begin{aligned} \\hat{h} = \\mathcal{A}(D^{train}) \\end{aligned} $$ Prediction Loss on Testing/ Validation Sets $\\ \\tilde{D} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}}(\\hat{h}: \\ \\tilde{D}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L} \\left( \\hat{h}(\\tilde{\\boldsymbol{x}}_i, \\ \\tilde{y}_i) \\right) \\end{aligned} $$ Common split ratios $\\ n/ \\tilde{n} \\ $: 60/40, 70/30 or 80/20 . If the number of (hyper-)parameters is large, let more data on training set. Split randomly. Stratification: classes are presented in same proportion in both sets. Drawback: estimated prediction loss may varies strongly in different $\\ \\tilde{D} \\ $, unless $\\ \\tilde{n} \\ $ is large. Solve by Cross-Validation Cross-Validation Approach K-fold: Construct k pairs of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} & D^{train} = D_{i \\neq k} & & D^{val} = D_k \\end{aligned} $$ K Prediction functions: obtained by using k training sets . $$ \\begin{aligned} \\hat{h}_k = \\mathcal{A}(D_{k}^{train}) \\end{aligned} $$ K performance Estimations: evaluated on k validation sets . $$ \\begin{aligned} \\hat{\\mathcal{J}}_k = \\hat{\\mathcal{J}}(\\hat{h}_k : \\ D_k^{val}) \\end{aligned} $$ Cross Validation (CV) Score: averaging all k $\\ \\hat{\\mathcal{J}}_k \\ $ $$ \\begin{aligned} CV = \\frac{1}{K} \\sum_{k=1}^{K}\\hat{\\mathcal{J}}_k \\left(\\mathcal{A} (D_k^{train}: D_k^{val}) \\right) = \\hat{{\\bar{\\mathcal{J}}}} (\\mathcal{A}) \\end{aligned} $$ Estimate Variability of CV score $$ \\begin{aligned} Var(CV) \\approx \\frac{1}{k} Var(\\hat{\\mathcal{J}}_k), & & Var{\\hat{\\mathcal{J}}} = \\frac{1}{k} = (\\hat{\\mathcal{J}}_k - CV) ^2 \\end{aligned} $$ LOOCV (Leave-One-Out Cross-Validation): $\\ D^{val} \\ $ contains only one data point. Generally expensive, but for some problems, the computation can be done quickly. For a further discussion of the choice of K, see e.g. Section 7.10 in the textbook by Hastie, Tibshirani, and Friedman (2009). 4.3.2 Hyperparameters Selection and Performance Evaluation:Option 1 - Two Times Held-out Split off some testing data to evaluate the final performance., e.g. typically, $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Split remaining data into $\\ D^{train} \\ $, $\\ D^{val} \\ $, e.g. 80/20 ratio. Tuning parameters $\\ \\boldsymbol{\\lambda} \\ $ on $\\ D^{train} \\ $, return a set of $\\ \\hat{\\boldsymbol{\\lambda}} \\ $ . $$ \\begin{aligned} \\hat{h}_{\\boldsymbol{\\lambda}} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) \\end{aligned} $$ Compute prediction loss $\\ PL({\\boldsymbol{\\lambda}}) \\ $ on $\\ D^{val} \\ $. $$ \\begin{aligned} PL(\\boldsymbol{\\lambda}) = \\hat{\\mathcal{J}} (\\hat{h}_{\\boldsymbol{\\lambda}}: \\ D^{val}) \\end{aligned} $$ and choosing the $\\ \\boldsymbol{\\lambda} \\ $ by minimising $\\ PL(\\boldsymbol{\\lambda}) \\ $ $$ \\begin{aligned} \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }} PL(\\boldsymbol{\\lambda}) \\end{aligned} $$ Using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $, re-estimate $\\ \\boldsymbol{\\theta} \\ $ on the union of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} \\hat{h} = \\mathcal{A}_{\\hat{\\boldsymbol{\\lambda}}} = \\left( D^{train} U D^{val} \\right) \\end{aligned} $$ Compute prediction loss on $\\ D^{test} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}} = \\hat{\\mathcal{J}}(\\hat{h}:\\ D^{test}) \\end{aligned} $$ Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ Option 2 - Cross-validation + Held-out Split of $\\ D^{test} \\ $, e.g. $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Compute CV score on remaining data $\\ D^{train} \\ $. $$ EPL(\\boldsymbol{\\lambda}) = CV $$ Choose $\\ \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }}EPL(\\boldsymbol{\\lambda}) \\ $ Re-estimate $\\ \\boldsymbol{\\theta} \\ $ on $\\ D^{train} \\ $ using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $. $$ \\hat{h} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) $$ Compute prediction loss on $\\ D^{test} \\ $. Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ 4.4 Loss Functions in Predictive Models.4.4.1 Regression $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = | \\hat{y} - y | & & \\text{(Absolute Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & \\text{if } | \\hat{y} - y |< \\delta\\\\ \\delta | \\hat{y} - y | - \\frac{1}{2} \\delta^2 & \\text{otherwise} \\end{cases} & & \\text{(Huber Loss)} \\end{aligned} $$ 4.4.2 Classification4.4.2.1 Non-differentiable Loss Function Assume k different classes, loss function $\\ L(\\hat{y}, \\ y) \\ $ can be represented as $\\ k \\times k \\ $ matrix. $$ L(\\hat{y}, \\ y) = \\begin{bmatrix} L(1,1) & L(1,2) & \\dots & L(1,k) \\\\ L(2,1) & L(2,2) & \\dots & L(2,k) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ L(k,1) & L(k,2) & \\dots & L(k,k) \\end{bmatrix} $$ The diagonal $\\ L(i,i) \\ $ are zero as correct prediction. The off-diagonal $\\ L(i,j) \\ $ are positive: loss incurred when predicting ‘i’ instead of ‘j’ Zero-One loss: If $\\ L(i,\\ j) = 1 \\ $ for $\\ i \\neq j \\ $, and 0 otherwise $$ L(\\hat{y}, \\ y) = \\begin{cases} 1 & i \\neq j\\\\ 0 & otherwise \\end{cases} $$ The expected prediction loss: $$ \\begin{aligned} \\mathcal{J}(h) & = \\mathbb{E}_{\\boldsymbol{x}, \\ y} L \\left(h(\\boldsymbol{x}), \\ y) \\right)\\\\ & = \\mathbb{E}_{\\hat{y}, \\ y} L \\left(\\hat{y}, \\ y) \\right)\\\\ & = \\sum_{i, j} L(i,\\ j) p(i,\\ j)\\\\ & = \\sum_{i \\neq j} p(i, \\ j)\\\\ & = \\mathbb{P}(y \\neq \\hat{y}) \\end{aligned} $$ , where $\\ p(i,\\ j) = p(\\hat{y} = i, \\ y = j) \\ $ Known as ‘missclassification rate’ Binary Classification receiver operating characteristic curve (ROC curve) Minimising the false-positive (or false-negative) rate alone is not a very meaningful strategy: The reason is that the trivial classiﬁer $\\ h(x) = \\hat{y} = −1 \\ $ would be the optimal solution. But for such a classiﬁer the true-positive rate would be zero. ROC curve visualise a generally a trade-oﬀ between true-positive rate (TPR) and false-positive rates (FPR). 4.4.2.2 Diﬀerentiable Loss FunctionsFor simplicity, we consider here binary classiﬁcation only. Let us assume that $\\ \\hat{y} ∈{−1,1} \\ $ is given by $$ \\hat{y}(\\boldsymbol{x}) = sign(h(\\boldsymbol{x})) $$ , where $\\ h(\\boldsymbol{x})\\ $ is real-valued. $$ \\text{correct classiﬁcation of } \\boldsymbol{x} ⇐⇒ yh(\\boldsymbol{x}) > 0. $$ Loss Function: $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\begin{cases} 1 & \\text{if } y h(\\boldsymbol{x} < 0)\\\\ 0 & \\text{otherwise.} \\end{cases} & & \\text{(Zero-One Loss)}\\\\ & L(\\hat{y},\\ y) = (h(\\boldsymbol{x}) - y)^2 = (1 - y h(\\boldsymbol{x}))^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = log \\left( 1 + exp(- y h(\\boldsymbol{x})) \\right) & & \\text{(Log Loss)}\\\\ & L(\\hat{y},\\ y) = exp(- y h(\\boldsymbol{x})) & & \\text{(Exponential Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right) & & \\text{(Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & & \\text{(Square Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} - 4 y h(\\boldsymbol{x}) & \\text{if } y h(\\boldsymbol{x}) < -1\\\\ max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & \\text{otherwise} \\end{cases} & & \\text{(Huberised Square Hinge Loss)} \\end{aligned} $$ 4.5 Lab for Chapter.4 Appendix A$s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$Assume $X$ centered, then, according the SVD of $X$, the covariance matrix is $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T \\\\ & = \\frac{1}{n}U S V^T (U S V^T)^T = \\frac{1}{n} U (\\frac{1}{n}S S^T) U^T\\\\ & = U \\Lambda U^T \\end{aligned} $$ Reference[1]: Michael E Tipping and Christopher M Bishop. “Probabilistic principal component analysis”. In: Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61.3 (1999), pp. 611–622 [2]: T. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning. Springer, 2009.","link":"/2019/05/14/DME-Data-Mining-and-Exploration-Revision/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"DataMining","slug":"DataMining","link":"/tags/DataMining/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"CoursesRevision","slug":"CoursesRevision","link":"/tags/CoursesRevision/"}],"categories":[{"name":"DeepLearning","slug":"DeepLearning","link":"/categories/DeepLearning/"},{"name":"CV","slug":"CV","link":"/categories/CV/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"CheatSheet","slug":"Python/CheatSheet","link":"/categories/Python/CheatSheet/"},{"name":"DataMining","slug":"DataMining","link":"/categories/DataMining/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"University of Edinburgh","slug":"University-of-Edinburgh","link":"/categories/University-of-Edinburgh/"}]}