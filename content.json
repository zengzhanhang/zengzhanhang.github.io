{"pages":[{"title":"About Me 关于我","text":"展航(Zhanhang)，即将毕业的学生； 一个统计学学生正在想着计算机方向挣扎。 学习学习 之前一直没有把学过的内容总结起来的意识，最近搭了这个GitHub Page，希望能慢慢总结起来方便自己翻查自己的笔记。‘…’ 代表还未接触到的内容，希望自己能持续学习。 数学基础 微积分（Calculus） 线性代数（Linear Algebra） 概率论与数理统计（Statistics） 统计分析 回归分析（Regression Analysis） 时间序列分析（Time Series Analysis） 主成分分析（Principal Component Analysis） 因子分析（Factor Analysis） 聚类分析（Cluster Analysis） 相关分析（Correlation Analysis） 对应分析（Correspondence Analysis） 方差分析(ANOVA/Analysis of Variance) 数据可视化 R-ggplot2; plotly Python-Matplotlib; seaborn; plotly Dashboard 数据挖掘 标准化 + 异常值检测（预处理） 数据降维（PCA, kernel PCA, MDS, Isomap） 模型评估（Generalisation, over/under-fitting, Cross-validation…） 机器学习 k近邻法（k-Nearest Neighbors） 朴素贝叶斯（naïve Bayes） 支持向量机（SVM） 决策树（Decision tree） 集成学习（Ensemble Learning） k均值聚类（k-Means Clustering） 关联规则 EM算法（Expectation–maximization algorithm） 隐马尔可夫模型（HMM） 条件随机场（CRF） … 推荐系统 深度学习 DNN CNN RNN（LSTM） GAN … Python for Machine Learning scikit-learn Keras PyTorch … 计算机视觉 OpenCV … 自然语言处理 …（mark： Stanford CS224n Natural Language Processing with Deep Learning：Bilibili, YouTube Stanford CS224u Natural Language Understanding：Bilibili, YouTube） 强化学习（Reinforcement Learning） …（mark：UCL Reinforcement Learning：Biliblili, YouTube） 提升效率的工具 Git（推荐：廖雪峰Git教程） Linux（Cheat Sheet） LaTeX Resources: MIT 6.S191: Intro to Deep Learning. Official website: http://introtodeeplearning.com/#schedule （Bilibili; YouTube）","link":"/about/index.html"}],"posts":[{"title":"Keras 笔记","text":"To take notes about the essential Keras elements to build basic neural networks. The explainations of each section haven’t finished yet. 1. Single Layer Neural Network (Linear Regression) 单层神经网络相当于（非）线性回归模型，第一个例子是构建一个最简单一元线性回归模型。 创建数据 单层神经网络模型需要数据进行训练，因此我们使用 numpy 创建一些人造数据，且我们的 $y$ 为 $y = ax+b$ 。 1234567891011121314import numpy as npimport tensorflow as tffrom tensorflow.keras import layersimport matplotlib.pyplot as pltplt.style.use('seaborn')# create dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) #randomize the dataY = 2*X + 10 + np.random.normal(0, 0.05, (200,))# plot dataplt.scatter(X, Y)plt.show() 构建神经网络 tf.keras.models.Sequential() 1234tf.keras.models.Sequential( layers=None, name=None) 用Keras创建神经网络，我们首先需要用 tf.keras.models.Sequential() 来建立网络，这里面只有一个argument，即 layers。这里添加神经层的方法有两种，一种是建立 model 的时候直接放入神经层，另一种是通过 model.add 来添加。如： 12345678# option 1model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1, activation = None, use_bias = True)])# option 2model = tf.keras.models.Sequential()model.add(tf.keras.layers.Dense(1, activation = None, use_bias = True)) tf.keras.layers.Dense() 上面的例子中我们给 model 添加了一个 tf.keras.layers.Dense(), 它代表最典型的全连接神经网络层，即每个输入节点连接到每个输出节点。 123456789101112tf.keras.layers.Dense( units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Arguments: units: 正整数，输出空间的维数。 activation：要使用的激活功能。如果未指定任何内容，则不应用激活（即 “linear” activation：a（x）= x）。 use_bias：Boolean，该层是否使用bias vector。 …其他看文档: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense models.Sequential().compile() 给 model 添加完神经层后，必须进行 model.compile() 才能继续后续的模型训练，并且在 model.compile 的时候需要指定 optimizer 和 loss。 12345678910model.compile( optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, distribute=None,) Arguments: optimizer：String（优化器名称）或优化器实例。请参阅tf.keras.optimizers。 loss：String（目标函数的名称）或目标函数。见tf.losses。 metrics：模型在训练和测试期间要评估的度量列表。通常，您将使用 metrics = ['accuracy']。 …其他看文档：https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential 完整例子 2. Multiple Layer Neural Network 3. Convolutional Neural Network 4. Recurrent Neural Network4.1 RNN 4.2 LSTM 5. Generative Adversarial Network Reference TensorFlow Tutorial：https://www.tensorflow.org/tutorials/ TensorFlow Guide：https://www.tensorflow.org/guide/","link":"/2019/06/12/Keras-notes/"},{"title":"综述：Human Pose Estimation Literature Review","text":"【导读】：本综述将会以时间顺序总结一些基于 Deep Learning 的人体姿态估计 (Human Pose Estimation) 有代表意义的论文。这些文章最早从 Google 提出的 DeepPose 开始，代表了 Pose Estimation 领域的发展。 Declaration：写这个 blog 是因为目前在做 pose estimation 领域的 literature review，并且读到了两篇觉得不错的 blog，分别是：A 2019 guide to Human Pose Estimation with Deep Learning 和 A 2019 guide to 3D Human Pose Estimation。","link":"/2019/12/31/Human-Pose-Estimation-Review/"},{"title":"Numpy&Pandas Tutorial","text":"Numpy和Pandas对python中的数据处理很重要。尤其对于数据分析/挖掘，Pandas几乎不可或缺。写tutorial的起因是因为一次面试中被问到numpy中去重用哪个函数，发现自己对numpy的不熟悉，所以希望以此加深印象…(haven’t started yet) 1. NumpyNumpy Cheat Sheet Numpy Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF. 2. PandasPandas Cheat Sheet Pandas Cheat Sheet taken from https://www.datacamp.com/community/data-science-cheatsheets This browser does not support PDFs. Please download the PDF to view it: Download PDF.","link":"/2019/05/10/Numpy-Pandas-Tutorial/"},{"title":"OpenCV Cheat Sheet (Python)","text":"Opencv-python是OpenCv的python API，包括数百种计算机视觉算法。这个页面记录了一些常用的Opencv-python函数，以便作为我的快速参考。 1. 安装和使用 Installation and Usage 安装 1pip install opencv-python 事实上一共有四种不同的packages，安装其中一个即可，四个packages都用同一个名字cv2（对于其他的packages，详见Documentation）。 2. OpenCV中的GUI特性2.1 图像基本操作(读取，显示，保存) 三个函数cv.imread(), cv.imshow(), cv.imwrite()分别用于读取；显示和保存图像。 cv.imread()：读取图像 1retval = cv.imread(filename[, flags]) 参数 Parameters: filename: 文件名 (Name of file to be loaded.) flags: 定义读取图片的方式，如彩色或灰色等 (takes values of cv::ImreadModes specified the way image should be read.){cv::IMREAD_UNCHANGED = -1,cv::IMREAD_GRAYSCALE = 0,cv::IMREAD_COLOR = 1,cv::IMREAD_ANYDEPTH = 2,cv::IMREAD_ANYCOLOR = 4,cv::IMREAD_LOAD_GDAL = 8,…} 例子 Example： 1234import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0) cv.imshow()：显示图像 1None = cv.imshow(winname, mat) 参数 parameters: winname：显示窗的名字 (Name of the window.) mat： 要显示的图像名 (Image to be shown.) 例子 Example: 1cv.imshow('image',img) cv.imwrite()：保存图像 1retval = cv.imwrite(filename, img[, params]) 参数 Parameters： filename：文件名（Name of the file.） img：要保存的图像名（Image to be saved.） params：Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, … .) see cv::ImwriteFlags 例子 Example： 1cv.imwrite('messigray.png',img) 2.2 色彩通道转换(Color conversions)：RGB - Gray RGB ↔ GRAY 除了RGB channel到灰度Y channel的转换，还有其他通道的转换，例如RGB ↔ CIE XYZ.Rec 709 with D65 white point； RGB ↔ YCrCb JPEG (or YCC)等等，详见Documentation。 RGB空间内的变换，例如添加/删除Alpha通道，反转通道顺序，转换为16位RGB颜色（R5：G6：B5或R5：G5：B5），以及转换为灰度/从灰度转换使用以下方式： $$ \\text{RGB[A] to Gray: Y←0.299⋅R+0.587⋅G+0.114⋅B} $$ and $$ \\text{Gray to RGB[A]: R←Y,G←Y,B←Y,A←max(}ChannelRange\\text{)} $$ 1cvtColor(src, dst, code, dstCn = 0) 参数 Parameters： src：输入图像 (input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC… ), or single-precision floating-point.) dst：输出图像 (output image of the same size and depth as src.) code：转换方式代码 (color space conversion code (see cv::ColorConversionCodes).){cv2.COLOR_RGB2GRAY,cv2.COLOR_GRAY2RGB,cv2.COLOR_BGR2GRAY,cv2.COLOR_RGB2GRAY,…} dstCn：目标图像中的通道数;如果参数为0，则从src和代码自动导出通道数 (number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code.) 例子 Examples： 123456import numpy as npimport cv2 as cv# Load an color image in grayscaleimg = cv.imread('messi5.jpg', 0)# Convert image from Y channel to RGB channelimg2 = cv.cvtColor(img, cv.COLOR_GRAY2BGR) Additional ReadingOpenCV官方教程中文版（For Python），段力辉 译（搬运自：https://www.linuxidc.com/Linux/2015-08/121400.htm）","link":"/2019/05/12/OpenCV-Cheat-Sheet/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/05/10/hello-world/"},{"title":"DeepMedic - multi-sacle 3D CNN with CRF for brain lesion segmentation","text":"论文阅读笔记，如果我有什么理解错误的地方，欢迎大家指正。论文：Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation 摘要：作者提出一种双路径，11层深的3D卷积神经网络用于brain lesion segmentation任务，主要解决了医学图像处理上的三个方面的问题。一是作者设计了一种dense training scheme的方案，采用全卷积神经网络，一次把相邻像素的image segments传入网络输出dense prediction (dense-inference)，从而节省了计算代价。作者使用更深的网络使模型判别能力更强。作者使用一种dual pathway architecture，即同时训练两个网络，使模型同时对高/低分辨率图像进行处理。最后作者采用全连接条件随机场(Fully connected Conditional Random Field)对网络output进行后处理(post-processing)改善图像类之间的边缘信息。 1. Dense Training在传统patch-wise的分类中，输入patch的尺寸和cnn最后一层神经元的感受野大小相同，这样网络得到一个single prediction对应输入patch的中心像素的值。而用全卷积实现的神经网络，因为其输入的patch的尺寸可以大于最后一层神经元的感受野，因此模型可以同时输出多个prediction，即dense-inference，而每一个prediction对应cnn’s receptive field的在输入patch上的每一步stride。 同时作者认为，这样dense-inference得到的每一个prediction都是可信的，只要感受野是完全在input patch上扫过，并且只捕捉到原始信息，因此没有使用padding。 Dense Training Scheme 的优点 节省计算代价和内存消耗 灵活：作者提到最佳的性能为将整个图像送入网络，但是这个做法不现实。如果GPU内存限制不允许，例如在需要缓存大量FM的大型3D网络的情况下，则将图像分成多个image-segments，这样会比单个segment大，但是可以去fit内存。（原文：If GPU memory constraints do not allow it, such as in the case of large 3D networks where a large number of FMs need to be cached, the volume is tiled in multiple image-segments, which are larger than individual patches, but small enough to fit into memory.） CNNs are trained patch-by-patch: 个人认为论文中提到的这个操作是在图像上randomly crop出一个individual patch作为网络的输入来训练网络，以此计算loss和进行gradient descent。 2. Class Balance(这里不太确定自己理解对不对) 在原文的Section 2.2中作者提到，这种dense training scheme的方案中的sampling input segments (即上面提到的randomly crop individual patches) 提供了一种灵活的方式去平衡training samples中的segmentation classes（正负类）的分布，而不同的分布对模型的性能有很大的影响。 从上图和原文Section 3.2的实验结果来看，感觉有点类似过采样的意思。上图说明，如果是crop一个以病变为中心 (lesion-centered) 的image segments，分别用 $7 \\times 7$ 和 $9 \\times 9$ 的框去crop下来，可以看到用 $9 \\times 9$ 的框去crop出这个image segment的话，绿色的内容会相对更多一点。 作者在Section 3.2的最后一段也提到，这个segments size在模型中是一个超参数，提到segment size会提升模型的性能，但是很快这种性能的提升久会达到平稳(level off)，并且在一系列的segment size中都会得到相似的性能。 3. 构建更深的网络 该部分总结：Deeper Networks + Smaller Kernel + Batch Norm + (p)ReLU + $ \\mathcal{N}\\left(0,\\ \\sqrt{2/n^{in}_{l}} \\right)$ 更深的网络有更好的判别能力，但是更深的网络同时意味着更高的计算代价和更吃内存。所以在这里作者采用的策略是层数增加，但是把kernel size从 $5^3$ 缩小到 $3^3$ 从而减少计算代价和节省内存。作者认为这里kernel size减小说明需要训练的parameters的数量减少，一定程度上有正则的效果。（但是原文好像只是给了个简单的 $\\frac{5^3}{3^3} \\approx 4.6$，但是层数的增加会增加参数的数量，不知道作者有没有算对，我也没去算它….） 因为更深的网络变得容易train不下去，所以作者采用了 ReLU-based 的网络 (在github文件看，DeepMedic的网络好像采用的是pReLU)。同时，作者不采用标准正态分布去初始化kernel weights，才是采用了 $ \\mathcal{N}\\left(0,\\ \\sqrt{2/n^{in}_{l}} \\right)$ 。 Batch Normalisation 4. Dual Pathways Networks 作者为上述deeper networks增加了第二个pathway networks，对down-sampling的图像进行操作。这种dual pathways 3D CNN 同时对高/低分辨率图像进行训练，文中提到第二个网络(低分辨率)用于捕捉一些high level的信息，例如 location。而第一个网络（高分辨率）用于捕捉一些细节信息。为了使得最后输出的feature maps尺寸一致，要对第二个网络的输出进行上采样以匹配第一个网络输出的feature maps的尺寸。 5. 3D fully connected CRF作者认为CNN网络输出的结果偏向与smooth（如何smooth可以见下图），因此对DeepMedic网络（11层，dual pathways）网络的输出认为是soft segmentation。最后作者用CRF对soft segmentations 进行后处理（post-processing）改善其边缘信息。 5.1 为什么CRF可以用于处理图像任务？首先对于图像任务，我们可以把每个像素认为是一个单独的节点（node），像素与像素之间就构成了边（edge），同时，相邻像素之间相互影响，而这种影响是对称的，相当于edge，因此构成了一个概率无向图。 5.2 全连接条件随机场对于每个像素 $i$ 具有类别标签 $x_i$ 还有对应的观测值 $y_i$，这样每个像素点作为节点，像素与像素间的关系作为边，即构成了一个条件随机场。而且我们通过观测变量 $y_i$ 来推测像素 $i$ 对应的类别标签 $x_i$。条件随机场如下： 条件随机场符合吉布斯分布：(此处的 $x$ 即上面说的观测值) $$P(\\boldsymbol{X} = \\mathbf{x} | \\boldsymbol{I}) = \\frac{1}{Z(\\boldsymbol{I})} exp\\left( -\\mathbb{E}(\\mathbf{x}|\\boldsymbol{I}) \\right) $$ 其中的 $\\mathbb{E}(\\mathbf{x}|\\boldsymbol{I})$ 是能量函数，为了简便，以下省略全局观测 $\\boldsymbol{I}$： $$ E(\\mathbf{x})=\\sum_i{\\Psi_u(x_i)}+\\sum_{i,j}\\Psi_p(x_i, x_j) $$ 其中的一元势函数 $\\sum_i{\\Psi_u(x_i)}$ 即来自于前端FCN的输出。而二元势函数如下： $$\\Psi_p(x_i, x_j)=u(x_i, x_j)\\sum_{m=1}^M{\\omega^{(m)}k_G^{(m)}(\\boldsymbol{f_i, f_j)}}$$ 二元势函数就是描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关。所以这样CRF能够使图片尽量在边界处分割。 而全连接条件随机场的不同就在于，二元势函数描述的是每一个像素与其他所有像素的关系，所以叫“全连接”。 Reference Harvard Referencing: Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon, D.K., Rueckert, D. and Glocker, B., 2017. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation. Medical image analysis, 36, pp.61-78. https://yq.aliyun.com/articles/232455 https://zhuanlan.zhihu.com/p/22308032","link":"/2019/07/18/deepmedic/"},{"title":"综述：DenseNet—Dense卷积网络（图像分类）","text":"【导读】本文对Dense卷积网络的发展进行了综述。这是2017年的CVPR最佳论文奖，并拥有2000多篇论文引用。它由康威尔大学、清华大学和facebook AI共同合作完成。 作者 | Sik-Ho Tsang (原文：Review: DenseNet — Dense Convolutional Network (Image Classification))编译 | Xiaowen 该译文转载自：https://mp.weixin.qq.com/s/F2GRDen0v2zbnHlB-xHioA 目录 Dense Block DenseNet 结构 DenseNet 的优势 CIFAR &amp; SVHN 小规模数据集结果 ImageNet 大规模数据集结果 特征复用的进一步分析 1. Dense Block 在 andard ConvNet 中，输入图像经过多次卷积，得到高层次特征。 在 ResNet 中，提出了恒等映射（ identity mapping ）来促进梯度传播，同时使用使用 Element-wise addition 。它可以看作是将状态从一个 ResNet 模块传递到另一个 ResNet 模块的算法。 (It can be viewed as algorithms with a state passed from one ResNet module to another one.) 在 DenseNet 中，每个层从前面的所有层获得额外的输入，并将自己的特征映射传递到后续的所有层，使用级联(Concatenation)方式，每一层都在接受来自前几层的“集体知识（collective knowledge）”。(Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.) 由于每个层从前面的所有层接收特征映射，所以网络可以更薄、更紧凑，即信道数可以更少。增长速率k是每个层的附加信道数。 因此，它具有较高的计算效率和存储效率。下图显示了前向传播中级联的概念： 2. DenseNet 结构2.1 基础 DenseNet 组成层 对于每个组成层使用 Pre-Activation Batch Norm (BN) 和 ReLU，然后用k通道的输出特征映射进行 $\\boldsymbol{3 \\times 3}$ 卷积，例如，将$x_0$、$x_1$、$x_2$、$x_3$ 转换为 $x_4$。这是 Pre-Activation ResNet 的想法。 2.2 DenseNet-B (Bottleneck 层) 为了降低模型的复杂度和规模，在 BN-ReLU-3×3 conv 之前进行了 BN-ReLU-1×1 conv . 2.3 具有转换层（transition layer）的多Dense块 采用1×1 Conv和2×2平均池化作为相邻 dense block 之间的转换层。 特征映射大小在 dense block 中是相同的，因此它们可以很容易地连接在一起。 在最后一个 dense block 的末尾，执行一个全局平均池化，然后附加一个 Softmax 分类器。 2.4 DenseNet-BC (进一步压缩) 如果 Dense Block 包含 $m$ 个特征映射，则转换层（transition layer）生成 $\\theta_m$ 输出特征映射，其中 $0 &lt; \\theta \\leq 1$ 称为压缩因子。 当 $\\theta = 1$ 时，跨转换层的特征映射数保持不变。$\\boldsymbol{\\theta &lt; 1}$ 的 DenseNet 称为 DenseNet-C，在实验中采用 $\\theta = 0.5$ 。 当同时使用 bottleneck 和 $\\boldsymbol{\\theta &lt; 1}$ 时的转换层时，该模型称为 DenseNet-BC 模型。 最后，训练 with/without B/C 和不同 L 层 和 k growth rate 的 DenseNet。 3. DenseNet 的优势3.1 Strong Gradient Flow 误差信号可以更直接地传播到早期的层中。这是一种隐含的深度监督，因为早期的层可以从最终的分类层直接获得监督。 3.2 Parameter &amp; Computational Efficiency 对于每个层，RetNet 中的参数与c×c成正比，而 DenseNet 中的参数与1×k×k成正比。 由于 k &lt;&lt; C, 所以 DenseNet 比 ResNet 的size更小。 3.3 More Diversified Features 由于 DenseNet 中的每一层都接收前面的所有层作为输入，因此特征更加多样化，并且倾向于有更丰富的模式。 3.4 Maintains Low Complexity Features 在标准ConvNet中，分类器使用最复杂的特征。 在 DenseNet 中，分类器使用所有复杂级别的特征。它倾向于给出更平滑的决策边界。它还解释了为什么 DenseNet 在训练数据不足时表现良好。 4. CIFAR &amp; SVHN 小规模数据集结果4.1 CIFAR-10 Pre-Activation ResNet is used in detailed comparison. 数据增强（C10+），测试误差： Small-size ResNet-110: 6.41% Large-size ResNet-1001 (10.2M parameters): 4.62% State-of-the-art (SOTA) 4.2% Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 4.5% Large-size DenseNet (L=250, k=24): 3.6% 无数据增强（C10），测试误差： Small-size ResNet-110: 11.26% Large-size ResNet-1001 (10.2M parameters): 10.56% State-of-the-art (SOTA) 7.3% Small-size DenseNet-BC (L=100, k=12) (Only 0.8M parameters): 5.9% Large-size DenseNet (L=250, k=24): 4.2% 在 Pre-Activation ResNet 中出现严重的过拟合，而 DenseNet 在训练数据不足时表现良好，因为DenseNet 使用了复杂的特征。 左：DenseNet-BC 获得最佳效果。 中：Pre-Activation ResNet 已经比 alexnet 和 vggnet 获得更少的参数，DenseNet-BC(k=12)的参数比 Pre-Activation ResNet 少3×10，测试误差相同。 右：与 Pre-Activation ResNet-1001有10.2m 参数相比，0.8 参数的 DenseNet-BC-100 具有相似的测试误差。 4.2 CIFAR-100CIFAR-100类似的趋势如下： 4.3 Detailed Results SVHN 是街景房屋编号的数据集。蓝色代表最好的效果。DenseNet-BC 不能得到比基本 DenseNet 更好的结果，作者认为 SVHN 是一项相对容易的任务，非常深的模型可能会过拟合。 5. ImageNet 大规模数据集结果 左：20M参数的DenseNet-201与大于40M参数的ResNet-101产生类似的验证错误。 右：相似的计算次数趋势(GOLOPS)。 底部：DenseNet-264(k=48)最高误差为20.27%，前5误差为5.17%。 6. 特征复用的进一步分析 从非常早期的层中提取的特征被同一 Dense Block 中的较深层直接使用。 转换层的权重也分布在前面的所有层中。 第二和第三 dense block 内的各层一贯地将最小权重分配给转换层的输出。(第一行) 在最终分类层，权重似乎集中在最终 feature map 上。一些更高级的特性在网络中产生得很晚。 Reference[2017 CVPR] [DenseNet]Densely Connected Convolutional Networks 原文链接：https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","link":"/2019/07/21/densenet/"},{"title":"统计学习 - Statistical Learning","text":"统计学习方法笔记总结。haven’t finished yet 1. k近邻法（k-Nearest Neighbors） 直观理解: 分类：在数据中找到与某个点（目标）最近的k个点，把该点（目标）的类分为k个点中多数的类。 回归：在数据中找到与某个点（目标）最近的k个点，k个点的均值为目标点的预测值。 优点： $k$ 近邻法是个非参数学习算法，它没有任何参数（ $k$ 是超参数，而不是需要学习的参数）。 近邻模型具有非常高的容量，这使得它在训练样本数量较大时能获得较高的精度。 缺点： 计算成本很高。因为需要构建一个 $N \\times N$ 的距离矩阵，其计算量为 $O(N^2)$，其中 $N$ 为训练样本的数量。 当数据集是几十亿个样本时，计算量是不可接受的。 在训练集较小时，泛化能力很差，非常容易陷入过拟合。 无法判断特征的重要性。 1.1 k近邻模型 模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间 一般是$n$维实数向量空间$\\mathbb{R}^d$。使用的距离是欧氏距离，但也可以是其他距离，如更一般的$L_p$距离（$L_p$ distance）或Minkowski距离（Minkowski distance）。 $$ L_p(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^p \\right)^{1/p}\\ , \\quad \\quad p \\geqslant 1\\\\ \\overrightarrow{\\boldsymbol{x}_i}, \\overrightarrow{\\boldsymbol{x}_j} \\in \\mathcal{X}, \\quad \\overrightarrow{\\boldsymbol{x}_i} = (x_i^{(1)}, x_i^{(2)}, \\dots, x_i^{(d)})^T ,\\quad {\\boldsymbol{x}_j} = (x_j^{(1)}, x_j^{(2)}, \\dots, x_j^{(d)})^T $$ 当 $p＝2$ 时，称为欧氏距离(Euclidean distance)：$L_2(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\left( \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|^2 \\right)^{1/2}$ 当 $p＝1$ 时，称为曼哈顿距离(Manhattan distance)：$L_1(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\sum_{l=1}^{n} |x_{i,l} - x_{j,l}|$ 当 $p＝\\infty$ 时，它是各个坐标距离的最大：$L_{\\infty}(\\overrightarrow{\\boldsymbol{x}_i},\\ \\overrightarrow{\\boldsymbol{x}_j}) = \\underset{i}{\\text{max}}|x_{i,l} - x_{j,l}|$ 不同的距离度量所确定的最近邻点是不同的。 k值的选择 k值的选择会对k近邻法的结果产生重大影响。 较小的k值：k值的减小就意味着整体模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，“学习”的近似 误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对 预测结果起作用。但是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。 优点：减少”学习”的偏差。 缺点：增大”学习”的方差（即波动较大）。 较大的k值：k值的增大就意味着整体的模型变得简单。相当于用较大邻域中的训练实例进行预测。其优点是可以减 少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似 的）训练实例也会对预测起作用，使预测发生错误。 优点：减少”学习”的方差（即波动较小）。 缺点：增大”学习”的偏差。 应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则 分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。也可以基于距离的远近进行加权投票。 多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为 $$ \\mathcal{f} : \\mathbb{R}^d \\rightarrow \\{c_1, c_2, \\dots, c_K \\} $$ 对给定的实例$\\overrightarrow{x} \\in \\mathcal{X}$，其最近邻的k个训练实例点构成集合$\\mathcal{N}_k (\\overrightarrow{x})$。如果涵盖$\\mathcal{N}_k (\\overrightarrow{x})$的区域的类别是cj，那么误分类率是 $$ L = \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i \\neq c_m) = 1 - \\frac{1}{k} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m) $$ $L$ 就是训练数据的经验风险。要使经验风险最小，则使得 $\\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$ 最大。即多数表决：$c_m = \\underset{c_m}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_m)$。所以多数表决规则等价于经验风险最小化。 1.2 k近邻算法 k近邻的分类算法： 输入：训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$ 为实例的特征向量，$y_i \\in \\overrightarrow{\\boldsymbol{y}} = \\left\\{ c_1, c_2, \\dots, c_k \\right\\} $为实例的类别，$i = 1, 2, \\dots, n$ 。 输出： 实例 $\\overrightarrow{\\boldsymbol{x}}$ 所属的类 y 。 步骤： 根据给定的距离度量，在训练集 $D$ 中找出与 $\\overrightarrow{\\boldsymbol{x}}$ 最邻近的k个点，涵盖这k个点的 $\\overrightarrow{\\boldsymbol{x}}$ 的邻域记作 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ ; 在 $\\mathcal{N}_k(\\overrightarrow{\\boldsymbol{x}})$ 中根据分类决策规则（如多数表决）决定 $\\overrightarrow{\\boldsymbol{x}}$ 的类别 $y$ ： $$ y = \\underset{c_j}{\\text{argmax}} \\sum_{\\overrightarrow{x}_i \\in \\mathcal{N}_k (\\overrightarrow{x})} I(\\tilde{y}_i = c_j), \\quad i = 1, 2, \\dots, n; \\quad j = 1, 2, \\dots, k $$ $I$ 为指示函数，即当$y_i＝c_j$ 时 $I$ 为1，否则 $I$ 为0。 2. 朴素贝叶斯 直观理解: 朴素贝叶斯和贝叶斯分类（回归）器都是基于贝叶斯理论进行预测或者分类的模型。过程是利用训练数据学习 $P(X|Y)$ 和 $P(Y)$ 的估计，得到联合概率分布 $P(X,Y)＝P(Y)P(X|Y)$ ，然后利用贝叶斯定理进行预测，将输入 $\\overrightarrow{\\boldsymbol{x}}$ 分到后验概率最大的类。 $$ P(Y|X) = \\frac{P(X, Y)}{P(X)} = \\frac{P(Y)P(X|Y)}{\\sum_{Y}P(Y)P(X|Y)} \\propto P(Y)P(X|Y) \\\\ \\hat{y} = \\underset{c_k}{\\text{argmax }}P(Y=c_k) \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) $$ 条件独立假设: $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}}) & = \\prod_{j=1}^{d} P(X^{(j)} = x^{(j)} | Y = c_k) \\end{aligned} $$ 优点： 性能相当好，它速度快，可以避免维度灾难。 支持大规模数据的并行学习，且天然的支持增量学习。 缺点： 无法给出分类概率，因此难以应用于需要分类概率的场景。 2.1 贝叶斯定理 全概率公式 (Law of Total Probability Theorem)： $$ P(A) = \\sum_{j=1}^{n}P(A|B_j)P(B_j) $$ 贝叶斯定理 (Bayes’ theorem) 在数学上表示为以下等式： $$ \\begin{aligned} P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B|A)P(A)}{\\sum_{j=1}^{d}P(B|A_j)P(A_j)} \\propto P(B|A) \\cdot P(A) \\end{aligned} $$ 2.2 朴素贝叶斯朴素贝叶斯（naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。 2.2.1 基本方法及解释2.2.1.1 基本方法 设输入空间 $X \\subseteq R_n$ 为 $d$ 维向量的集合，输出空间为类标记集合 $y ＝{c_1, c_2, \\dots, c_K}$。输入为特征向量$\\overrightarrow{\\boldsymbol{x}} \\in X$ ，输出为类标记（class label）$y \\in Y $ 。$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率分布，训练数据集 $D$ 由 $P(X,Y)$ 独立同分布产生。 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 步骤： 学习以下先验概率分布及条件概率分布。 $$ \\begin{aligned} \\text{(先验概率)} & \\quad \\quad P(Y=c_k), \\quad \\ k=1,2,\\dots, k\\\\ \\text{(条件概率)} & \\quad \\quad P(X = \\overrightarrow{\\boldsymbol{x}} | P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\end{aligned} $$ 条件独立性的假设： $$ \\begin{aligned} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k) & = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\dots, X^{(d)} = x^{(d)} | y = c_k) \\\\ & = \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ 条件独立假设等于 是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。 朴素贝叶斯法分类时，对给定的输入 $\\overrightarrow{\\boldsymbol{x}}$ ，通过学习到的模型计算后验概率分布 $P(Y＝ c_k|X＝\\overrightarrow{\\boldsymbol{x}})$。 $$ \\begin{aligned} P(Y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) & = \\frac{P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}{\\sum_{k} P(X = \\overrightarrow{\\boldsymbol{x}} | Y = c_k)}\\\\ \\text{(独立性假设)} & = \\frac{P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}{\\sum_{k} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k)}, \\quad k = 1,2,\\dots,K \\end{aligned} $$ 将后验概率最大的类作为x的类输出: $$ \\begin{aligned} \\hat{y} = f(\\overrightarrow{\\boldsymbol{x}}) = \\underset{c_k}{\\text{argmax }} P(Y = c_k) \\prod_{j=1}^{d} P(X_j = x^{(j)} | Y = c_k) \\end{aligned} $$ Note：因为后验概率的分母都相同，因此在这可以忽略。 2.2.1.2 后验概率最大的含义等价于期望风险最小化 (Optional) 假设选择 0-1损失函数： $$ \\begin{aligned} L(Y, f(X)) = \\begin{cases} 0, \\quad \\ Y = f(X)\\\\ 1, \\quad \\ Y \\neq f(x) \\end{cases} \\end{aligned} $$ 期望风险函数 $$ \\begin{aligned} R_{exp}(f) = \\mathbb{E} \\left[ L(Y, f(X)) \\right] = \\sum_{\\overrightarrow{\\boldsymbol{x}}}\\sum{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(\\overrightarrow{\\boldsymbol{x}}, y) = \\mathbb{E}_X \\left[ \\sum_{y \\in Y} L(y, f(\\overrightarrow{\\boldsymbol{x}})) P(y | \\overrightarrow{\\boldsymbol{x}}) \\right] \\end{aligned} $$ 为了使期望风险最小化，只需对 $\\overrightarrow{\\boldsymbol{x}}$ 逐个极小化 $$ \\begin{aligned} f(\\overrightarrow{\\boldsymbol{x}}) & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} L(c_k, y) P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} P(y \\neq c_k | X = \\overrightarrow{\\boldsymbol{x}})\\\\ & = \\underset{y \\in Y}{\\text{argmin}} \\sum_{k} (1 - P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}))\\\\ & = \\underset{y \\in Y}{\\text{argmax}} \\sum_{k} P(y = c_k | X = \\overrightarrow{\\boldsymbol{x}}) \\end{aligned} $$ 2.2.1.3 参数估计 - 极大似然估计朴素贝叶斯中可以利用极大似然估计先验概率和条件概率。 $$ \\begin{aligned} P(Y = c_k) \\frac{1}{n} \\sum_{i=1}^{n} I(y_i = c_k)\\\\ P(X^{(j)} = a_{jl} \\ |\\ Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} \\ |\\ y_i = c_k \\right)}{\\sum_{i=1}^{n} I(y_i=c_k)}\\\\ j = 1, 2, \\dots, d; \\quad l = 1, 2, \\dots, S_j; \\quad k = 1, 2, \\dots, K \\end{aligned} $$ 其中，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征；$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值；$I$ 为指示函数。 2.2.1.4 贝叶斯估计用极大似然估计可能会出现所要估计的概率值为0的情况。解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计（or also known as add-alpha smoothing）是 $$ \\begin{aligned} P_{\\alpha}(X^{(j)} = a_{jl} | Y = c_k) = \\frac{\\sum_{i=1}^{n} I\\left( x_i^{(j)} = a_{jl} | y_i = c_k \\right) + \\alpha}{\\sum_{i=1}^{n} I(y_i=c_k) + S_j \\alpha}\\\\ \\end{aligned} $$ 其中，$\\alpha \\geqslant 0$ 当 $\\alpha = 0$ 为极大似然估计，当 $\\alpha = 1$为拉普拉斯平滑（Laplace smoothing, also known as add-one smoothing）。 2.2.2 算法 朴素贝叶斯算法（naïve Bayes algorithm） 利用极大使然估计先验概率 $P(Y = c_k)$ 和条件概率 $P(X^{(j)} = a_{jl} \\ |\\ Y = c_k)$，如Sec 2.2.1.2 对于给定的实例 $\\overrightarrow{\\boldsymbol{x}}＝(x^{(1)},x^{(2)},…,x^{(d)})^T$ ，计算 $$ \\begin{aligned} P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k), \\quad \\ k = 1, 2, \\dots, K \\end{aligned} $$ 确定实例 $\\overrightarrow{\\boldsymbol{x}}$ 的类 $$ \\hat{y} = \\underset{c_k}{\\text{argmax}} \\ P(Y = c_k)\\prod_{j=1}^{d} P(X_j = x^{(j)} \\ |\\ Y = c_k) $$ 例子： 3. 支持向量机 直观理解: 支持向量机（support vector machines，SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习算法是求解凸二次规划的最优化算法。 间隔最大使它有别于感知机； 感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 支持向量机还支持核技巧，从而使它成为实质上的非线性分类器。 支持向量机学习方法包含构建由简至繁的模型： 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机； 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机； 非线性支持向量机；当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机 特征向量之间的内积就是核函数，使用核函数可以学习非线性支持向量机。 非线性支持向量机等价于隐式的在高维的特征空间中学习线性支持向量机，这种方法称作核技巧。 3.1 线性可分支持向量机 假设给定一个特征空间上的训练数据集 $$ D = \\left\\{ (\\overrightarrow{\\boldsymbol{x}}_1, y_1), (\\overrightarrow{\\boldsymbol{x}}_2, y_2), \\dots, (\\overrightarrow{\\boldsymbol{x}}_n, y_n) \\right\\} $$ 其中，$\\overrightarrow{\\boldsymbol{x}}_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d$, $y_i \\in {-1, +1}, \\; i=1,2,\\dots,n$ .$\\overrightarrow{\\boldsymbol{x}}_i$ 为第 $i$ 个(d维)特征向量，也称为实例，$(x_i，\\ y_i)$ 称为样本点。 假设数据集线性可分。则学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$\\boldsymbol{W}_{k \\times d} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}}$，它由法向量 $\\boldsymbol{W}$ 和截距 $\\overrightarrow{\\boldsymbol{b}}$ 决定，可用 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 来表示。例如： 给定线性可分训练数据集，通过间隔最大化或等 价地求解相应的凸二次规划问题学习得到的分离超平面为 $\\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} = 0$ 以及相应的分类决策函数 $f(\\overrightarrow{\\boldsymbol{x}}) = sign( \\boldsymbol{W}^{*} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}^{*}} )$ 3.1.1 函数间隔 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i)$ 的函数间隔为 $$\\hat{\\gamma}_i = y_i (\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的函数间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 函数间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 可以将一个点距离分离超平面的远近来表示分类预测的可靠程度： 一个点距离分离超平面越远，则该点的分类越可靠。 一个点距离分离超平面越近，则该点的分类则不那么确信。 在超平面 $\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}} + \\overrightarrow{\\boldsymbol{b}} = 0$ 确定的情况下： $| \\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}}| $ 能够相对地表示点 $\\overrightarrow{\\boldsymbol{x}}_i$ 距离超平面的远近。 3.1.2 几何间隔 因为只要成比例地改变 $\\boldsymbol{W}$ 和 $b$ ，例如将它们改为 $2w$ 和 $2b$ ，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量 $\\boldsymbol{W}$ 加某些约束，如规范化，$||\\boldsymbol{W}||＝ 1$ ，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。 对于给定的训练数据集 $D$ 和超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ ，定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于样本点 $(\\overrightarrow{\\boldsymbol{x}}_i,y_i$ 的几何间隔为 $$\\hat{\\gamma}_i = y_i (\\frac{\\boldsymbol{W}}{||\\boldsymbol{W}||} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\frac{\\overrightarrow{\\boldsymbol{b}}}{||\\boldsymbol{W}||} )$$ 定义超平面 $(\\boldsymbol{W},\\overrightarrow{\\boldsymbol{b}})$ 关于训练数据集 $D$ 的几何间隔为超平面关于 $D$ 中所有样本点 $(x_i，y_i)$ 几何间隔之最小值，即 $$\\hat{\\gamma} = \\underset{i = 1, \\dots , n}{\\text{min}} \\hat{\\gamma}_i$$ 由定义可知函数间隔和几何间隔有下列的关系：$$\\gamma_i = \\frac{\\hat{\\gamma}_i}{||W||}; \\quad \\gamma = \\frac{\\hat{\\gamma}}{||W||}$$ 当 $||\\boldsymbol{W}|| = 1$ 时，函数间隔和几何间隔相等。 如果超平面参数 $\\boldsymbol{W}$ 和 $\\overrightarrow{\\boldsymbol{b}}$ 成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 3.1.3 硬间隔最大化 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。 几何间隔最大化的物理意义：不仅将正负实例点分开，而且对于最难分辨的实例点（距离超平面最近的那些点），也有足够大的确信度来将它们分开。 求解一个几何间隔最大的分离超平面的问题可以表示为下面的约束最优化问题： $$ \\begin{aligned} & \\underset{\\boldsymbol{W}, \\overrightarrow{\\boldsymbol{b}}}{\\text{max}} & & \\gamma \\\\ & \\text{s.t.} & & y_i (\\frac{\\boldsymbol{W}}{||\\boldsymbol{W}||} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\frac {\\overrightarrow{\\boldsymbol{b}}}{||\\boldsymbol{W}||} ) \\geqslant \\gamma ; \\quad i=1,2,\\dots,n \\end{aligned} $$ 考虑几何间隔和函数间隔的关系式（7.8），可将这个问题改写为 $$ \\begin{aligned} & \\underset{\\boldsymbol{W}, \\overrightarrow{\\boldsymbol{b}}}{\\text{max}} & & \\frac{\\hat{\\gamma}}{||\\boldsymbol{W}||} \\\\ & \\text{s.t.} & & y_i (\\boldsymbol{W} \\cdot \\overrightarrow{\\boldsymbol{x}}_i + \\overrightarrow{\\boldsymbol{b}} ) \\geqslant \\hat{\\gamma} ; \\quad i=1,2,\\dots,n \\end{aligned} $$ Reference 李航. 统计学习方法[M]. 清华大学出版社， 2012年3月. Ai算法工程师手册","link":"/2019/05/31/StatisticalLearning/"},{"title":"DME - Data Mining and Exploration (INF11007) Revision","text":"Remeber to read the ‘Lab’ section of each chapter 1. Exploratory Data Analysis1.1 Numberical Data Description1.1.1 Location Non-robust Measure Sample Mean (arithmetic mean or average): $\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ for random variable: $\\mathbb{E}[x] = \\int xp(x) dx$ Robust Measure Median: $$ median(x) = \\begin{cases} x_{[(n+1)\\mathbin{/}2]}& \\text{; if $n$ is odd}\\\\ \\frac{1}{2}[x_{(n\\mathbin{/}2)}+x_{(n\\mathbin{/}2)+1}]& \\text{; if $n$ is even} \\end{cases} $$ Mode: Value that occurs most frequent $\\alpha_{th}$ Sample Quantile (rough data point, i.e. $q_{\\alpha} \\approx x_{([n\\alpha])}$) $Q_{1} = q_{0.25}$, $Q_{2} = q_{0.5}$, $Q_{3} = q_{0.75}$ ExampleData_1=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9]Data_2=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9000] DataSet Mean Median $Q_{1}$ $Q_{3}$ Data 1 3.0 2.5 1.0 4.0 Data 2 902.1 2.5 1.0 4.0 1.1.2 Scale Non-robust Measure Sample Variance: $Var(x) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x})^2$ for random variable: $Var[x] = \\int [x-\\mathbb{E}[x]]^2 dx$ Standard Deviation: $Std(x) = \\sqrt{Var(x)}$ Robust Measure Median Absolute Deviation(MAD): $$MAD(x) = median[|x_{i} - median(x)|]$$ IQR(interquartile range): $$IQR = Q_{3} - Q_{1}$$ 1.1.3 Shape: Non-robust Measure Skewness: measures the asymmetry of data $$skew(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{3}$$ Kurtosis: measures how heavy the tails of distribution are, in other word, measures how often x takes on values that are considerable larger or smaller than its standard deviation.$$kurt(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{4}$$ Robust Measure Galtons’s measure of skewness: $$skew(x) = \\frac{(Q_{3}-Q_{2})-(Q_{2}-Q_{1})}{Q_{3}-Q_{1}}$$ Robust kurtosis: $$kurt(x) = \\frac{(q_{7/8}-q_{5/8})-(q_{3/8}-q_{1/8})}{Q_{3}-Q_{1}}$$ 1.1.4 Multivariate Measure: Sample Covariance: $$Cov(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x}) (y_{i} - \\hat{y})$$ for random variable: $Cov[x, y] = \\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])] = \\mathbb{E}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]$ Pearson’s Correlation Coefficient:$$\\rho(x,y) = \\frac{\\text{cov}(x,y)}{Std(x) Std(y)}$$ $\\rho=0$ doesn’t mean statistical independent, since it only measures linear correlation $-1 \\le \\rho \\le 1$ Simple way to measure non-linear correlation: $\\rho(g(x),g(y)) = \\frac{\\text{cov}(g(x),g(y))}{Std(g(x)) Std(g(y))}$ Covariance Matrix: $$Cov[X] = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{T}]$$ Eigenvalue decomposition: $Cov[X] = U\\Lambda U^{T}$ $\\sum_{i=1}^{d}Var[x_{i}]=trace(Var[X])=\\sum_{i=1}^{d} \\lambda_{i}$ $cov[Ax+b] = Acov[x]A^{T}$ Correlation Matrix:$$\\rho(X) = diag\\left( \\frac{1}{std(X)} \\right) Cov[X]diag\\left( \\frac{1}{std(X)} \\right)$$ Rank Correlation - Kendall’s $\\tau$: $$\\tau(x,y) = \\frac{n_{c}(x,y) - n_{d}(x,y)}{n(n-1)/2}$$ $n_c$: total number of concordant pairs, $n_d$: total number of disconcordant pairs 1.2 Data Visualisation1.3 Data Preprocessing:1.3.1 Standardisation:Normalising data to have 0 (sample) mean and unit (sample) variance: Centering Matrix: $$C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}$$ Where, $1_n = [1, 1, \\dots, 1]^T$ Multiplying it from right: removes sample mean of each row, i.e., $X = \\tilde{X}C_{n}$ left: removes sample mean of each column 1.3.2 Outlier Detection: Tukey’s fences: $[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)] = [Q_1 - k \\times IQR, Q_3 + k \\times IQR]$ Typically, $k = 1.5$ for outlier removal 1.4 Lab for Chapter.1 2. Principal Component Analysis(PCA)2.1 PCA by Variance Maximisation2.1.1 Sequential ApproachPrincipal Component(PC) direction: $\\boldsymbol{w}$, projected data: $\\boldsymbol{w}^{T} \\boldsymbol{x}$ The First Principal Component Direction: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{1}}}{\\text{maximise}} & & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = Var(z_{1}) \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{1}} = 1|| \\end{aligned} $$ According to the eigenvalue decomposition of convariance matrix $\\Sigma$: $\\Sigma = U \\Lambda U^{T}$ Let $\\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i} \\boldsymbol{u_{i}} = U \\boldsymbol{a}$, then $$ \\begin{aligned} & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & ||\\boldsymbol{w_{1}}|| = \\boldsymbol{w_{1}}^{T} \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ Thus, the optimisation problem can be written as: $$ \\begin{aligned} & {\\text{maximise}} & & \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & \\text{subject to} & & \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ $\\boldsymbol{a} = (1, 0, \\dots, 0)^T$ is the unique solution, if $lambda_{1} &gt; \\lambda{i}$. So the first PC direction is $$\\boldsymbol{w_{1}} = U \\boldsymbol{a} = \\boldsymbol{u_{1}}$$ , where the first PC direction given by the first eigen vector, $\\boldsymbol{u_{1}}$, of $\\Sigma$ corresponding to the first(largest) eigen value $\\lambda_{1}$. $Var(z_{1})= \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\lambda_{1}$ $\\mathbb{E}(z_{1}) = \\mathbb{E}(\\boldsymbol{w_{1}}^{T} \\boldsymbol{x}) = \\boldsymbol{w_{1}}^{T} \\mathbb{E}(\\boldsymbol{x}) = 0$ First PC scores: $\\boldsymbol{z_{1}}^{T} = \\boldsymbol{w_{1}}^{T} X_{d \\times n}$ Subsequent PC Direction $\\boldsymbol{w_{m}}$: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{m}}}{\\text{maximise}} & & \\boldsymbol{w_{m}}^T \\Sigma \\boldsymbol{w_{m}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{m}} = 1|| \\\\ & & & \\boldsymbol{w_{m}}^{T}\\boldsymbol{w_{i}} = 0 & & i = 1, 2, \\dots, m-1 \\end{aligned} $$ Solution: similar to the previous procedure $\\boldsymbol{w_{m}} = \\boldsymbol{u_{m}}$ is the m-th PC direction given by the m-th eigen vector of $\\Sigma$ corresponding to the m-th largest eigen value $\\lambda_{m}$. $Var(z_{m}) = \\lambda_{m}$, $\\mathbb{E}(z_{m}) = 0$ PCs (scores) uncorrelated: $$ \\begin{aligned} Cov(z_i, z_j) & = \\mathbb{E}(z_i z_j) - \\mathbb{E}(z_i) \\mathbb{E}(z_j)\\\\ & = \\mathbb{E}(\\boldsymbol{w_{i}}^{T} \\boldsymbol{x} \\boldsymbol{w_{j}}^{T} \\boldsymbol{x}) - 0\\\\ & = \\boldsymbol{w_{j}}^{T} \\mathbb{E}(\\boldsymbol{x} \\boldsymbol{x}) \\boldsymbol{w_{j}}^{T}\\\\ & = \\boldsymbol{w_{j}}^{T} \\Sigma \\boldsymbol{w_{j}}^{T} \\\\ & = \\boldsymbol{e_{i}}^T U^T U \\Lambda U^T U \\boldsymbol{e_{j}} \\\\ & = 0 \\end{aligned} $$ Fraction of variance explained $= \\frac{\\sum_{i}^{k} \\lambda_{i}}{\\sum_{i}^{d} \\lambda_{i}}$ how much variability in data is captured by the first k principal components. 2.1.2 Simultaneous Approach $$ \\begin{aligned} & \\text{maximise} & & \\sum_{i=1}^{k}\\boldsymbol{w_{i}}^T \\Sigma \\boldsymbol{w_{i}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, m-1\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ Subtle technical point: the sequential approach corresponds to solving this optimisation problem in greedy manner(algorithm), which doesn’t guarantee to yield optimal solution. However, sequential approach and simultaneous yield same results. 2.2 PCA by Minimisation of Approximation Error Projection Matrix: $$ P = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} = W_{k} W_{k}^{T} $$ , where $W_{k} = (\\boldsymbol{w_{1}}, \\dots, \\boldsymbol{w_{k}})$ is $d \\times k$ matrix . Approximating $\\boldsymbol{x}$ into subspace $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = \\sum_{i=1}^{k}\\boldsymbol{w_{i}} \\boldsymbol{w_{i}}^{T} \\boldsymbol{x}$ Approximation Error: $\\mathbb{E}||\\boldsymbol{x} - P \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - W_{k} W_{k}^T \\boldsymbol{x}||^2 = \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2$ Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\mathbb{E}||\\boldsymbol{x} - \\sum_{i=1}^{k}\\boldsymbol{w_k} \\boldsymbol{w_k}^T \\boldsymbol{x}||^2 \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, k\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ So, the optimal PC directions $\\boldsymbol{w_{i}}$ are the first k eigen vectors $\\boldsymbol{u_{i}}$ of $\\Sigma$ The optimal projection matrix is $P = U_k U_{k}^{T}$ $\\boldsymbol{\\hat{x}} = P \\boldsymbol{x} = U_{k} U_{k}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} \\boldsymbol{u_{i}}^{T} \\boldsymbol{x} = \\sum_{i=1}^{k} \\boldsymbol{u_{i}} z_{i}$ $\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2 = \\sum_{i=1}^{d} \\lambda_{i} - \\sum_{i=1}^{k} \\lambda_{i} = \\sum_{i=k+1}^{d} \\lambda_{i}$, which means minimising expected error = maximising variance explained. Relative Approximation Error: $$ \\frac{\\mathbb{E}||\\boldsymbol{x} - U_{k} U_{k}^T \\boldsymbol{x}||^2}{\\mathbb{E}||\\boldsymbol{x}||^2} = 1 - \\frac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{i=1}^{d} \\lambda_{i}} = 1 - \\text{fraction of variance explained} $$ 2.3 PCA by Low Rank Matrix Approximation2.3.1 Approximation from Data Matrix Let $X_{d \\times n} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})$, where $\\boldsymbol{x}$ is $d \\times 1$ matrix (d-dimension). Express $X$ via its Singular Value Decomposition(SVD): $X = U S V^{T}$ , where $U_{d \\times d}$ and $V_{n \\times n}$ are orthonormal. $S$ is zero everwhere, but first r diagonal elements. Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & \\sum_{ij} \\left[ (X)_{ij} - (M)_{ij} \\right]^2 = ||X - \\hat{X}||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ \\end{aligned} $$ So, Optimal solution: $\\hat{X} = \\sum_{i=1}^{k} \\boldsymbol{u_i} \\boldsymbol{s_i} \\boldsymbol{v_i}^{T} = U_K S_K V_K^T$ ((truncated singular value decomposition). left singular vectors $\\boldsymbol{u_i}$ are eigen vectors of $\\Sigma$, so $\\boldsymbol{u_i}$ are PC directions. $s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$: $\\lambda_i = \\frac{s_i^2}{n}$. (Proof in Appendix A) PC scores: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = s_i \\boldsymbol{v_i}^T$ Proof: $\\boldsymbol{z_i}^T = \\boldsymbol{u_i}^T X = \\boldsymbol{u_i}^T U S V^T = \\boldsymbol{u_i}^T \\sum_{j=1}^{r}\\boldsymbol{u_j} s_j \\boldsymbol{v_j}^T = s_i \\boldsymbol{v_i}^T$ 2.3.2 Approximation from Sample Covariance Matrix Optimisation Problem: $$ \\begin{aligned} & \\text{minimise} & & ||\\Sigma - M||_{F} \\\\ & \\text{subject to} & & rank(M) = k\\\\ & & & M^T = M \\end{aligned} $$ Optimal solution: $M = \\hat{\\Sigma} = U_k \\Lambda_k U_k^T = \\Sigma^T$, i.e., $\\sum_{i=1}^{k}\\lambda_i \\boldsymbol{u_i} \\boldsymbol{u_i}^T$ 2.3.3 Approximation from Gram Matrix Gram Matrix:$$G = X^T X \\text{, where} (G)_{ij} = \\boldsymbol{x_i}^T\\boldsymbol{x_j}$$ Gram Matrix is positive semi-definite According the SVD of $X$: $$ G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T = \\sum_{i=1}^{n} s_i^2 \\boldsymbol{v_i} \\boldsymbol{v_i}^T $$ Thus, the best rank k approximation of $G$ is $\\hat{G} = \\sum_{i=1}^{k} \\boldsymbol{v_i} s_i^2 \\boldsymbol{v_i}^T$. Denote $\\tilde{\\Lambda} = S^T S$ is the top k eigen value of $G$, $V_k = (\\boldsymbol{v_1}, \\boldsymbol{v_2}, \\dots, \\boldsymbol{v_k})_{n \\times k}$ $$ Z_k = \\sqrt{\\tilde{\\Lambda}_k} V_k^T $$ 2.3.4 Probabilistic PCA (PPCA) Advantages: PPCA can samples artificial data points (generative model). Formulation allows us to deal with missing data. Probabilistic Model: $$ Z \\sim \\mathcal{N}(0,\\,I_k)\\\\ \\epsilon \\sim \\mathcal{N}(0, \\, \\sigma^2 I_d)\\\\ \\underset{d \\times 1}{\\boldsymbol{x}} = \\underset{d \\times k}{W} \\; \\underset{k \\times 1}{\\boldsymbol{z}} + \\underset{d \\times 1}{\\boldsymbol{\\mu}} + \\underset{d \\times 1}{\\boldsymbol{\\epsilon}} $$ Joint, Conditional and Observation Distribution Conditional Distribution: $$ p(\\boldsymbol{x}|\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{\\mu},\\; \\sigma^2I_{d}) $$ Joint Distribution: $$ \\begin{aligned} p(\\boldsymbol{z},\\; \\boldsymbol{x}) & = p(\\boldsymbol{x}|\\boldsymbol{z})p(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\; W \\boldsymbol{z} + \\boldsymbol{u},\\; \\sigma^2I_{d}) \\mathcal{N}(\\boldsymbol{z};\\; 0,\\; I_k)\\\\ & = \\frac{1}{const}exp \\left[ -\\frac{1}{2} [(\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu})^{T} (\\frac{1}{\\sigma^2}I_{d}) (\\boldsymbol{x} - W \\boldsymbol{z} - \\boldsymbol{\\mu}) + \\boldsymbol{z}^{T} \\boldsymbol{z}] \\right] \\end{aligned} $$ Important Equations: For multivariate normal distribution: $$ \\begin{aligned} -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu}) & = -\\frac{1}{2}\\boldsymbol{x}^T \\Sigma^{-1} \\boldsymbol{x} + \\boldsymbol{x}^{T} \\Sigma^{-1}\\mu + const\\\\ & = -\\frac{1}{2}\\boldsymbol{x}^T A \\boldsymbol{x} + \\boldsymbol{x}^{T} \\xi + const \\end{aligned} $$ Thus, $\\Sigma = A^{-1}$ and $\\boldsymbol{\\mu} = \\Sigma \\ \\xi$ . Observation Distribution: $$ p(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}; \\; \\boldsymbol{\\mu}, \\; W W^{T} + \\sigma^2 I) $$ Maximum Likelihood:The maximum likelihood solutions are shown by Tipping and Bishop, 1999: $$ W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}} R\\\\ \\sigma_{ML}^2 = \\frac{1}{d-k} \\sum_{i=k+1}^{d}\\lambda_{i} $$ $U_k$ are $k$ principal eigenvectors of $\\hat{\\Sigma} = Cov(X) = \\frac{1}{n}X X^T$ . $\\Lambda_k$ is diagonal matrix with eighenvalues. $R$ is arbitrary orthogonal matrix, interpreted as a rotation in the latent space, indicating not unique solutions. Another option to find $W$ and $\\sigma^2$ is EM algorithm. Relation to PCA: The closest thing to PCA mapping is the posterior distribution $p(\\boldsymbol{z}| \\; \\boldsymbol{x})$. To find it, we can fix $\\boldsymbol{x}$ as a constant in the joint distribution $p(\\boldsymbol{z},\\; \\boldsymbol{x})$ and use the important equation just mentioned above. $$ p(\\boldsymbol{z}| \\; \\boldsymbol{x} = \\mathcal{N}(\\boldsymbol{z}; \\; M^{-1} W^{T} (\\boldsymbol{x} - \\boldsymbol{\\mu}), \\; \\sigma^2 M^{-1}) $$ , where $M = W^T W + \\sigma^2 I$ . PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\hat{\\boldsymbol{x}} = W_{ML} \\mathbb{E}(\\boldsymbol{z}|\\; \\boldsymbol{x}) = W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} $$ , where $M_{ML} = W_{ML}^{T} W_{ML} + \\sigma^{2}I \\;$ and $\\; W_{ML} = U_k (\\Lambda_k - \\sigma^2 I)^{\\frac{1}{2}}$ . For $\\sigma^2 \\rightarrow 0$, we recover the PCA projection $\\hat{\\boldsymbol{x}}$: $$ \\begin{aligned} W_{ML} M_{ML}^{-1} W_{ML}^{T} \\boldsymbol{x} & = U_k \\Lambda_k^{1/2} ((U_k \\Lambda_k^{1/2})^T (U_k \\Lambda_k^{1/2}))^{-1} (U_k \\Lambda_k^{1/2})^{T} \\boldsymbol{x}\\\\ & = U_k U_k^T \\boldsymbol{x} \\end{aligned} $$ 2.4 Lab for Chapter.2 3. Dimensionality Reduction3.1 Linear Dimensionality Reduction3.1.1 From Data Matrix Observed (uncentered) data: $\\tilde{X} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n})_{d \\times n}$ Center data: $X = \\tilde{X} C_n$ , where $C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}\\ $ . Option 1 - compute PC scores via eigen values decomposition: $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T = U \\Lambda U^T \\end{aligned} $$ Denote $U_k$ with the first $k$ eigen vectors of $\\Sigma$ corresponding to the top $k$ eigen values: $U_k = (\\boldsymbol{u_1}, \\boldsymbol{u_2}, \\dots, \\boldsymbol{u_k})_{d \\times k}$ PC scores: $$ \\begin{aligned} \\underset{k \\times 1}{\\boldsymbol{z}_i} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times 1}{\\boldsymbol{x}_i} , & & \\underset{k \\times n}{Z} = \\underset{k \\times d}{U_k^T} \\; \\underset{d \\times n }{X} \\end{aligned} $$ Option 2 - compute PC scores via Gram Matrix: $$ \\begin{aligned} G = X^T X = (USV^T)^T(USV^T) = V S^T U^T U S V^T = VS^T SV^T = V \\tilde{\\Lambda} V^T \\end{aligned}\\\\ \\begin{aligned} \\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\underset{k \\times n}{V_k^T}, & & V_k = (\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_k) \\end{aligned} $$ 3.1.2 From Inner Product $$ \\begin{aligned} (G)_{ij} = \\boldsymbol{x}_i^T \\boldsymbol{x}_j & & X = \\tilde{X} C_n & & \\tilde{G} = \\tilde{X}^T \\tilde{X} \\end{aligned}\\\\ G = X^T X = C_n \\tilde{X}^T \\tilde{X} C_n = C_n \\tilde{G} C_n $$ 3.1.3 From Distance Matrix If only given squared distance $\\delta_{ij}^2$ between data points $\\tilde{\\boldsymbol{x_i}}$ and $\\tilde{\\boldsymbol{x_j}} \\ $. $$ \\delta_{ij}^2 = ||\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}||^2 = (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}})^T (\\tilde{\\boldsymbol{x_i}} - \\tilde{\\boldsymbol{x_j}}) $$ Distance Matrix $\\Delta$ contains elements $\\delta_{ij} \\ $. $$ \\delta_{ij}^2 = ||(\\tilde{\\boldsymbol{x_i}} -\\mu) - (\\tilde{\\boldsymbol{x_j}} - \\mu)||^2 = ||\\boldsymbol{x_i} - \\boldsymbol{x_j}||^2 = (\\boldsymbol{x_i} - \\boldsymbol{x_j})^T(\\boldsymbol{x_i} - \\boldsymbol{x_j})\\\\ \\delta_{ij}^2 = ||\\boldsymbol{x_i}||^2 + ||\\boldsymbol{x_j}||^2 -2\\boldsymbol{x_i}^T \\boldsymbol{x_j} $$ Center the distance: $$ (C_n \\Delta C_n)_{ij} = (\\Delta C_n)_{ij} - \\frac{1}{n} \\sum_{i} (\\Delta C_n)_{ij} = - 2\\boldsymbol{x_i}^T \\boldsymbol{x_j}\\\\ G = -\\frac{1}{2}C_n \\Delta C_n $$ 3.2 (Non-linear) Dimensionalisty Reduction via Kernel PCA To obtain new data matrix $\\Phi$ using the transforming function $\\phi(\\boldsymbol{x}_i)$. $$ \\Phi = (\\phi_1, \\phi_2, \\dots, \\phi_n) = (\\phi(\\boldsymbol{x}_1), \\phi(\\boldsymbol{x}_2), dots, \\phi(\\boldsymbol{x}_n)) $$ Kernel Trick: inner product of some functions can be computed as: $$ \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ uncentered Gram Matrix $G$ of $\\Phi$ with elements $(\\tilde{G})_{ij}$: $$ \\tilde{G})_{ij} = \\phi(\\boldsymbol{x}_i)^T \\phi(\\boldsymbol{x}_j) = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) $$ Polynomial kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = (\\boldsymbol{x}_i^T \\boldsymbol{x}_j)^\\alpha$Gaussian kernel: $k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = exp \\left( - \\frac{||\\boldsymbol{x_i} - \\boldsymbol{x}_j||^2}{2 \\sigma^2} \\right)$ Then applying methods in Sec 3.1.2 and Sec 3.1.1 to compute PC scores. 3.3 Multidimensional Scaling (MDS)3.3.1 Metric MDS Assumption: the numerical values of dissimilarities (e.g. Euclidean distance) carry information. Optimisation Problem: $$ \\begin{aligned} \\text{minimise}& & w_{ij}(||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - \\delta_{ij})^2 \\end{aligned} $$ $\\delta_{ij}$ are dissimilarities between two data items, e.g. Euclidean Distance. $||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| \\ $ is Euclidean distance betweeen $\\boldsymbol{z}_i \\ $ and $\\ \\boldsymbol{z}_j \\ $, i.e., $\\ \\sqrt{(\\boldsymbol{z}_i - \\boldsymbol{z}_j)^T (\\boldsymbol{z}_i - \\boldsymbol{z}_j)} \\ $. $w_{ij} \\ $ are some weights specified by users. if $\\ w_{ij} = \\frac{1}{\\delta_{ij}} \\ $, the MDS is called Sammon nonlinear mapping emphasing the faithful representation of samll dissimilarities. Solved by gradient descent. 3.3.2 Non-metric MDS Assumption: only relationship between $\\ \\delta_{ij} \\ $ matters, i.e., whether $\\ \\delta_{12} &gt; \\delta_{13}\\ $ or $\\ \\delta_{12} &lt; \\delta_{13}\\ $ . Optimisation Problem: $$ \\begin{aligned} \\underset{\\boldsymbol{z_1}, \\boldsymbol{z_2}, \\dots, \\boldsymbol{z_n}, f}{\\text{minimise}}& & \\sum_{i \\le j} w_{ij} (||\\boldsymbol{z}_i - \\boldsymbol{z}_j|| - f(\\delta_{ij}))^2 \\end{aligned} $$ Actual values of $\\ \\delta_{ij} \\ $ do not matter. $f \\ $ is monotonic (non-decreasing) function converting dissimilarities to distances. Solved by iterating between optimisation w.r.t $\\ \\boldsymbol{z}_i \\ $ and optimisation w.r.t $\\ f \\ $, which can be done by regression. 3.3.3 Classical MDS: Assumption: numerical values of $\\ \\delta_{ij} \\ $ matter. Dissimilarities $\\ \\delta_{ij} \\ $ are (squared) Eucldiean distance between some unknown vectors. Distance matrix $\\ \\Delta \\ $ is formed by $\\ \\delta_{ij} \\ $ Using the method in Sec 3.1.3: Compute hypothetical Gram matrix $\\ G’ \\ $ of unknown centered data points. $$ \\begin{aligned} G = -\\frac{1}{2}C_n \\Delta C_n ,& & C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T} \\end{aligned} $$ Compute top k eigen values $\\ \\sigma_k^2 \\ $ and corresponding eigen vectors $\\ \\boldsymbol{v}_k \\ $ of $\\ G \\ $ and form $\\ \\tilde{\\Lambda}_k = diag(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_k^2) \\ $ and $\\ V_k = (\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_k)_{n \\times k}$ $\\underset{k \\times n}{Z} = \\underset{k \\times k}{\\sqrt{\\tilde{\\Lambda}}} \\; \\underset{k \\times n}{V_k^T}$ $\\Delta \\ $ is not necessary positive semi-definite, thus, some eigen values might be negative. Solution: choose $\\ k \\ $small enough to avoid negative eigen values. Classical MDS solution for $\\ k’ &lt; k \\ $ is directly given by the first $\\ k’ \\ $ corordinates of $\\ k \\ $ dimensional $\\ \\boldsymbol{z} \\ $. Alternative approximate negative definite $\\ \\Delta \\ $ by: $$ \\begin{aligned} & \\text{minimise}& & ||(-\\frac{1}{2}C_n \\Delta C_n) - M^T M||_F\\\\ & \\text{subject to}& & rank(M^T M) = k \\end{aligned} $$ 3.3.4 Isometric Features Mapping (Isomap) Steps of Isomap Construct the neighbourhood graph via ‘k nearest neighbour‘ or all data points within a certain (Euclidean) distance. Construct the shortest path (distances) as geodesic distance Construct the low dimensional embeding of these data via MDS so as to represent these data. Geodesic distance is measured by the shortest distance between them when only allowed to travel on the data manifold from one neighbouring data point to the next. Isomap well represents the circular structure when learned graph is connected. 3.4 Lab for Chapter.3 4. Predictive Modelling and Generalization4.1 Prediction and Training Loss4.1.1 Prediction Loss $$ \\mathcal{J}(h) = \\mathbb{E}_{\\hat{y}, \\ y} \\left[ \\mathcal{L}(\\hat{y}, \\ y) \\right] = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(h(\\boldsymbol{x}), \\ y) \\right] $$ The term $\\ \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\ $ means expectation w.r.t $\\ p(\\boldsymbol{x},\\ y) \\ $ . 4.1.2 Training Loss $$ \\mathcal{J}_{\\lambda}^{*} = \\underset{\\theta}{min} \\ \\mathcal{J}_{\\lambda}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\mathcal{L}(h(\\boldsymbol{x}_i; \\ \\theta), \\ y_i) \\right] $$ 4.2 Generalisation Performance4.2.1 Generalisation Loss For prediction function $$ \\mathcal{J}(\\hat{h}) = \\mathbb{E}_{\\boldsymbol{x}, \\ y} \\left[ \\mathcal{L}(\\hat{h}(\\boldsymbol{x}), \\ y) \\right] $$ Done with held-out data For algorithm $$ \\bar{\\mathcal{J}}(\\mathcal{A}) = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\hat{h}) \\right] = \\mathbb{E}_{D^{train}}\\left[ \\mathcal{J}(\\mathcal{A}(D^{train})) \\right] $$ See DME Lecture Notes for more details. 4.2.2 Overfitting and Underfitting Overfitting: Reducing the model complexity, the prediction loss decreases. Underfitting: Increasing the model complexity, the prediction loss decreases. Solutions: Model Selection or Regularisation . Regularisation: $$ \\begin{aligned} & \\text{minimise} & & \\mathcal{J}_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\theta}) + \\lambda_{reg} R(\\boldsymbol{\\theta}) \\end{aligned} $$ L2 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} \\theta_i^2 \\; $ L1 regularisation: $\\; \\; \\; R(\\boldsymbol{\\theta}) = \\sum_{i} |\\theta_i| \\; $ Either model complexity and size of training data matter generalisation performance, See 4.2.3 Example on DME Lecture Notes. 4.3 Estimating the Generalisation PerformanceWe typically need to estimate the generalisation performance twice: Once for hyperparameter selection, and once for ﬁnal performance evaluation. 4.3.1 Methods for Estimating the Generalisation PerformanceHeld-out Approach Prediction function: $$ \\begin{aligned} \\hat{h} = \\mathcal{A}(D^{train}) \\end{aligned} $$ Prediction Loss on Testing/ Validation Sets $\\ \\tilde{D} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}}(\\hat{h}: \\ \\tilde{D}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L} \\left( \\hat{h}(\\tilde{\\boldsymbol{x}}_i, \\ \\tilde{y}_i) \\right) \\end{aligned} $$ Common split ratios $\\ n/ \\tilde{n} \\ $: 60/40, 70/30 or 80/20 . If the number of (hyper-)parameters is large, let more data on training set. Split randomly. Stratification: classes are presented in same proportion in both sets. Drawback: estimated prediction loss may varies strongly in different $\\ \\tilde{D} \\ $, unless $\\ \\tilde{n} \\ $ is large. Solve by Cross-Validation Cross-Validation Approach K-fold: Construct k pairs of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} & D^{train} = D_{i \\neq k} & & D^{val} = D_k \\end{aligned} $$ K Prediction functions: obtained by using k training sets . $$ \\begin{aligned} \\hat{h}_k = \\mathcal{A}(D_{k}^{train}) \\end{aligned} $$ K performance Estimations: evaluated on k validation sets . $$ \\begin{aligned} \\hat{\\mathcal{J}}_k = \\hat{\\mathcal{J}}(\\hat{h}_k : \\ D_k^{val}) \\end{aligned} $$ Cross Validation (CV) Score: averaging all k $\\ \\hat{\\mathcal{J}}_k \\ $ $$ \\begin{aligned} CV = \\frac{1}{K} \\sum_{k=1}^{K}\\hat{\\mathcal{J}}_k \\left(\\mathcal{A} (D_k^{train}: D_k^{val}) \\right) = \\hat{{\\bar{\\mathcal{J}}}} (\\mathcal{A}) \\end{aligned} $$ Estimate Variability of CV score $$ \\begin{aligned} Var(CV) \\approx \\frac{1}{k} Var(\\hat{\\mathcal{J}}_k), & & Var{\\hat{\\mathcal{J}}} = \\frac{1}{k} = (\\hat{\\mathcal{J}}_k - CV) ^2 \\end{aligned} $$ LOOCV (Leave-One-Out Cross-Validation): $\\ D^{val} \\ $ contains only one data point. Generally expensive, but for some problems, the computation can be done quickly. For a further discussion of the choice of K, see e.g. Section 7.10 in the textbook by Hastie, Tibshirani, and Friedman (2009). 4.3.2 Hyperparameters Selection and Performance Evaluation:Option 1 - Two Times Held-out Split off some testing data to evaluate the final performance., e.g. typically, $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Split remaining data into $\\ D^{train} \\ $, $\\ D^{val} \\ $, e.g. 80/20 ratio. Tuning parameters $\\ \\boldsymbol{\\lambda} \\ $ on $\\ D^{train} \\ $, return a set of $\\ \\hat{\\boldsymbol{\\lambda}} \\ $ . $$ \\begin{aligned} \\hat{h}_{\\boldsymbol{\\lambda}} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) \\end{aligned} $$ Compute prediction loss $\\ PL({\\boldsymbol{\\lambda}}) \\ $ on $\\ D^{val} \\ $. $$ \\begin{aligned} PL(\\boldsymbol{\\lambda}) = \\hat{\\mathcal{J}} (\\hat{h}_{\\boldsymbol{\\lambda}}: \\ D^{val}) \\end{aligned} $$ and choosing the $\\ \\boldsymbol{\\lambda} \\ $ by minimising $\\ PL(\\boldsymbol{\\lambda}) \\ $ $$ \\begin{aligned} \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }} PL(\\boldsymbol{\\lambda}) \\end{aligned} $$ Using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $, re-estimate $\\ \\boldsymbol{\\theta} \\ $ on the union of $\\ D^{train} \\ $ and $\\ D^{val} \\ $. $$ \\begin{aligned} \\hat{h} = \\mathcal{A}_{\\hat{\\boldsymbol{\\lambda}}} = \\left( D^{train} U D^{val} \\right) \\end{aligned} $$ Compute prediction loss on $\\ D^{test} \\ $. $$ \\begin{aligned} \\hat{\\mathcal{J}} = \\hat{\\mathcal{J}}(\\hat{h}:\\ D^{test}) \\end{aligned} $$ Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ Option 2 - Cross-validation + Held-out Split of $\\ D^{test} \\ $, e.g. $\\ D^{test} \\ $ = 20 % of $\\ D \\ $. Compute CV score on remaining data $\\ D^{train} \\ $. $$ EPL(\\boldsymbol{\\lambda}) = CV $$ Choose $\\ \\hat{\\boldsymbol{\\lambda}} = \\underset{\\boldsymbol{\\lambda}}{\\text{argmin }}EPL(\\boldsymbol{\\lambda}) \\ $ Re-estimate $\\ \\boldsymbol{\\theta} \\ $ on $\\ D^{train} \\ $ using $\\ \\hat{\\boldsymbol{\\lambda}} \\ $. $$ \\hat{h} = \\mathcal{A}_{\\boldsymbol{\\lambda}} (D^{train}) $$ Compute prediction loss on $\\ D^{test} \\ $. Re-estimate $\\ \\hat{h} \\ $ on all data $\\ D \\ $ 4.4 Loss Functions in Predictive Models.4.4.1 Regression $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = | \\hat{y} - y | & & \\text{(Absolute Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} \\frac{1}{2}\\left( \\hat{y} - y \\right)^2 & \\text{if } | \\hat{y} - y |< \\delta\\\\ \\delta | \\hat{y} - y | - \\frac{1}{2} \\delta^2 & \\text{otherwise} \\end{cases} & & \\text{(Huber Loss)} \\end{aligned} $$ 4.4.2 Classification4.4.2.1 Non-differentiable Loss Function Assume k different classes, loss function $\\ L(\\hat{y}, \\ y) \\ $ can be represented as $\\ k \\times k \\ $ matrix. $$ L(\\hat{y}, \\ y) = \\begin{bmatrix} L(1,1) & L(1,2) & \\dots & L(1,k) \\\\ L(2,1) & L(2,2) & \\dots & L(2,k) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ L(k,1) & L(k,2) & \\dots & L(k,k) \\end{bmatrix} $$ The diagonal $\\ L(i,i) \\ $ are zero as correct prediction. The off-diagonal $\\ L(i,j) \\ $ are positive: loss incurred when predicting ‘i’ instead of ‘j’ Zero-One loss: If $\\ L(i,\\ j) = 1 \\ $ for $\\ i \\neq j \\ $, and 0 otherwise $$ L(\\hat{y}, \\ y) = \\begin{cases} 1 & i \\neq j\\\\ 0 & otherwise \\end{cases} $$ The expected prediction loss: $$ \\begin{aligned} \\mathcal{J}(h) & = \\mathbb{E}_{\\boldsymbol{x}, \\ y} L \\left(h(\\boldsymbol{x}), \\ y) \\right)\\\\ & = \\mathbb{E}_{\\hat{y}, \\ y} L \\left(\\hat{y}, \\ y) \\right)\\\\ & = \\sum_{i, j} L(i,\\ j) p(i,\\ j)\\\\ & = \\sum_{i \\neq j} p(i, \\ j)\\\\ & = \\mathbb{P}(y \\neq \\hat{y}) \\end{aligned} $$ , where $\\ p(i,\\ j) = p(\\hat{y} = i, \\ y = j) \\ $ Known as ‘missclassification rate’ Binary Classification receiver operating characteristic curve (ROC curve) Minimising the false-positive (or false-negative) rate alone is not a very meaningful strategy: The reason is that the trivial classiﬁer $\\ h(x) = \\hat{y} = −1 \\ $ would be the optimal solution. But for such a classiﬁer the true-positive rate would be zero. ROC curve visualise a generally a trade-oﬀ between true-positive rate (TPR) and false-positive rates (FPR). 4.4.2.2 Diﬀerentiable Loss FunctionsFor simplicity, we consider here binary classiﬁcation only. Let us assume that $\\ \\hat{y} ∈{−1,1} \\ $ is given by $$ \\hat{y}(\\boldsymbol{x}) = sign(h(\\boldsymbol{x})) $$ , where $\\ h(\\boldsymbol{x})\\ $ is real-valued. $$ \\text{correct classiﬁcation of } \\boldsymbol{x} ⇐⇒ yh(\\boldsymbol{x}) > 0. $$ Loss Function: $$ \\begin{aligned} & L(\\hat{y},\\ y) = \\begin{cases} 1 & \\text{if } y h(\\boldsymbol{x} < 0)\\\\ 0 & \\text{otherwise.} \\end{cases} & & \\text{(Zero-One Loss)}\\\\ & L(\\hat{y},\\ y) = (h(\\boldsymbol{x}) - y)^2 = (1 - y h(\\boldsymbol{x}))^2 & & \\text{(Square Loss)}\\\\ & L(\\hat{y},\\ y) = log \\left( 1 + exp(- y h(\\boldsymbol{x})) \\right) & & \\text{(Log Loss)}\\\\ & L(\\hat{y},\\ y) = exp(- y h(\\boldsymbol{x})) & & \\text{(Exponential Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right) & & \\text{(Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & & \\text{(Square Hinge Loss)}\\\\ & L(\\hat{y},\\ y) = \\begin{cases} - 4 y h(\\boldsymbol{x}) & \\text{if } y h(\\boldsymbol{x}) < -1\\\\ max \\left( 0, \\ 1 - y h(\\boldsymbol{x}) \\right)^2 & \\text{otherwise} \\end{cases} & & \\text{(Huberised Square Hinge Loss)} \\end{aligned} $$ 4.5 Lab for Chapter.4 Appendix A$s_i^2$ related to eigen values $\\lambda_i$ of $\\Sigma$Assume $X$ centered, then, according the SVD of $X$, the covariance matrix is $$ \\begin{aligned} \\Sigma & = \\frac{1}{n}X X^T \\\\ & = \\frac{1}{n}U S V^T (U S V^T)^T = \\frac{1}{n} U (\\frac{1}{n}S S^T) U^T\\\\ & = U \\Lambda U^T \\end{aligned} $$ Reference[1]: Michael E Tipping and Christopher M Bishop. “Probabilistic principal component analysis”. In: Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61.3 (1999), pp. 611–622 [2]: T. Hastie, R. Tibshirani, and J.H. Friedman. The Elements of Statistical Learning. Springer, 2009.","link":"/2019/05/14/DME-Data-Mining-and-Exploration-Revision/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"CV,PoseEstimation","slug":"CV-PoseEstimation","link":"/tags/CV-PoseEstimation/"},{"name":"DataMining","slug":"DataMining","link":"/tags/DataMining/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"Image Segmentation","slug":"Image-Segmentation","link":"/tags/Image-Segmentation/"},{"name":"Medical Imaging","slug":"Medical-Imaging","link":"/tags/Medical-Imaging/"},{"name":"Image Classification","slug":"Image-Classification","link":"/tags/Image-Classification/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"CoursesRevision","slug":"CoursesRevision","link":"/tags/CoursesRevision/"}],"categories":[{"name":"DeepLearning","slug":"DeepLearning","link":"/categories/DeepLearning/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"CV","slug":"CV","link":"/categories/CV/"},{"name":"CheatSheet","slug":"Python/CheatSheet","link":"/categories/Python/CheatSheet/"},{"name":"DataMining","slug":"DataMining","link":"/categories/DataMining/"},{"name":"ReadingNote","slug":"ReadingNote","link":"/categories/ReadingNote/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"University of Edinburgh","slug":"University-of-Edinburgh","link":"/categories/University-of-Edinburgh/"}]}