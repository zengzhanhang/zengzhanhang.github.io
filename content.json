{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/05/10/hello-world/"},{"title":"DME - Data Mining and Exploration(INF11007) Revision","text":"Chapter 1. Exploratory Data AnalysisNumberical Data DescriptionLocation Non-robust Measure Sample Mean (arithmetic mean or average): $\\hat{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ for random variable: $\\mathbb{E}[x] = \\int xp(x) dx$ Robust Measure Median: $$ median(x) = \\begin{cases} x_{[(n+1)\\mathbin{/}2]}& \\text{; if $n$ is odd}\\\\ \\frac{1}{2}[x_{(n\\mathbin{/}2)}+x_{(n\\mathbin{/}2)+1}]& \\text{; if $n$ is even} \\end{cases} $$ Mode: Value that occurs most frequent $\\alpha_{th}$ Sample Quantile (rough data point, i.e. $q_{\\alpha} \\approx x_{([n\\alpha])}$) $Q_{1} = q_{0.25}$, $Q_{2} = q_{0.5}$, $Q_{3} = q_{0.75}$ ExampleData_1=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9]Data_2=[0, 1, 1, 1, 2, 3, 4, 4, 5, 9000] DataSet Mean Median $Q_{1}$ $Q_{3}$ Data 1 3.0 2.5 1.0 4.0 Data 2 902.1 2.5 1.0 4.0 Scale Non-robust Measure Sample Variance: $Var(x) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x})^2$ for random variable: $Var[x] = \\int [x-\\mathbb{E}[x]]^2 dx$ Standard Deviation: $Std(x) = \\sqrt{Var(x)}$ Robust Measure Median Absolute Deviation(MAD): $$MAD(x) = median[|x_{i} - median(x)|]$$ IQR(interquartile range): $$IQR = Q_{3} - Q_{1}$$ Shape: Non-robust Measure Skewness: measures the asymmetry of data $$skew(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{3}$$ Kurtosis: measures how heavy the tails of distribution are, in other word, measures how often x takes on values that are considerable larger or smaller than its standard deviation.$$kurt(x) = \\frac{1}{n} \\sum_{i=1}^{n}[\\frac{x_{i}-\\hat{x}}{std(x)}]^{4}$$ Robust Measure Galtons’s measure of skewness: $$skew(x) = \\frac{(Q_{3}-Q_{2})-(Q_{2}-Q_{1})}{Q_{3}-Q_{1}}$$ Robust kurtosis: $$kurt(x) = \\frac{(q_{7/8}-q_{5/8})-(q_{3/8}-q_{1/8})}{Q_{3}-Q_{1}}$$ Multivariate Measure: Sample Covariance: $$Cov(x, y) = \\frac{1}{n}\\sum_{i=1}^{n} (x_{i} - \\hat{x}) (y_{i} - \\hat{y})$$ for random variable: $Cov[x, y] = \\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])] = \\mathbb{E}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]$ Pearson’s Correlation Coefficient:$$\\rho(x,y) = \\frac{\\text{cov}(x,y)}{Std(x) Std(y)}$$ $\\rho=0$ doesn’t mean statistical independent, since it only measures linear correlation $-1 \\le \\rho \\le 1$ Simple way to measure non-linear correlation: $\\rho(g(x),g(y)) = \\frac{\\text{cov}(g(x),g(y))}{Std(g(x)) Std(g(y))}$ Covariance Matrix: $$Cov[X] = \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{T}]$$ Eigenvalue decomposition: $Cov[X] = U\\Lambda U^{T}$ $\\sum_{i=1}^{d}Var[x_{i}]=trace(Var[X])=\\sum_{i=1}^{d} \\lambda_{i}$ $cov[Ax+b] = Acov[x]A^{T}$ Correlation Matrix:$$\\rho(X) = diag\\left( \\frac{1}{std(X)} \\right) Cov[X]diag\\left( \\frac{1}{std(X)} \\right)$$ Rank Correlation - Kendall’s $\\tau$: $$\\tau(x,y) = \\frac{n_{c}(x,y) - n_{d}(x,y)}{n(n-1)/2}$$ $n_c$: total number of concordant pairs, $n_d$: total number of disconcordant pairs Data Preprocessing:Standardisation:Normalising data to have 0 (sample) mean and unit (sample) variance: Centering Matrix: $$C_n = I_{n} - \\frac{1}{n} 1_n 1_n^{T}$$ Where, $1_n = [1, 1, \\dots, 1]^T$ Multiplying it from right: removes sample mean of each row, i.e., $X = \\tilde{X}C_{n}$ left: removes sample mean of each column Outlier Detection: Tukey’s fences: $[Q_1 - k(Q_3 - Q_1), Q_3 + k(Q_3 - Q_1)] = [Q_1 - k \\times IQR, Q_3 + k \\times IQR]$ Typically, $k = 1.5$ for outlier removal Chapter 2. Principal Component Analysis(PCA)PCA by Variance MaximisationSequential ApproachPrincipal Component(PC) direction: $\\boldsymbol{w}$, projected data: $\\boldsymbol{w}^{T} \\boldsymbol{x}$ The First Principal Component Direction: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{1}}}{\\text{maximise}} & & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = Var(z_{1}) \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{1}} = 1|| \\end{aligned} $$ According to the eigenvalue decomposition of convariance matrix $\\Sigma$: $\\Sigma = U \\Lambda U^{T}$ Let $\\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i} \\boldsymbol{u_{i}} = U \\boldsymbol{a}$, then $$ \\begin{aligned} & \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & ||\\boldsymbol{w_{1}}|| = \\boldsymbol{w_{1}}^{T} \\boldsymbol{w_{1}} = \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ Thus, the optimisation problem can be written as: $$ \\begin{aligned} & {\\text{maximise}} & & \\sum_{i=1}^{n} a_{i}^{2} \\lambda_{i} \\\\ & \\text{subject to} & & \\sum_{i=1}^{n} a_{i}^{2} = 1 \\end{aligned} $$ $\\boldsymbol{a} = (1, 0, \\dots, 0)^T$ is the unique solution, if $lambda_{1} &gt; \\lambda{i}$. So the first PC direction is $$\\boldsymbol{w_{1}} = U \\boldsymbol{a} = \\boldsymbol{u_{1}}$$ , where the first PC direction given by the first eigen vector, $\\boldsymbol{u_{1}}$, of $\\Sigma$ corresponding to the first(largest) eigen value $\\lambda_{1}$. $Var(z_{1})= \\boldsymbol{w_{1}}^T \\Sigma \\boldsymbol{w_{1}} = \\lambda_{1}$ $\\mathbb{E}(z_{1}) = \\mathbb{E}(\\boldsymbol{w_{1}}^{T} \\boldsymbol{x}) = \\boldsymbol{w_{1}}^{T} \\mathbb{E}(\\boldsymbol{x}) = 0$ First PC scores: $\\boldsymbol{z_{1}}^{T} = \\boldsymbol{w_{1}}^{T} X_{d \\times n}$ Subsequent PC Direction \\boldsymbol{w_{m}}: $$ \\begin{aligned} & \\underset{\\boldsymbol{w_{m}}}{\\text{maximise}} & & \\boldsymbol{w_{m}}^T \\Sigma \\boldsymbol{w_{m}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{m}} = 1|| \\\\ & & & \\boldsymbol{w_{m}}^{T}\\boldsymbol{w_{i}} = 0 & & i = 1, 2, \\dots, m-1 \\end{aligned} $$ Solution: similar to the previous procedure $\\boldsymbol{w_{m}} = \\boldsymbol{u_{m}}$ is the m-th PC direction given by the m-th eigen vector of $\\Sigma$ corresponding to the m-th largest eigen value $\\lambda_{m}$. $Var(z_{m}) = \\lambda_{m}$, $\\mathbb{E}(z_{m}) = 0$ PCs (scores) uncorrelated: $$ \\begin{aligned} & Cov(z_i, z_j) & = & \\mathbb{E}(z_i z_j) - \\mathbb{E}(z_i) \\mathbb{E}(z_j)\\\\ & & = & \\mathbb{E}(\\boldsymbol{w_{i}}^{T} \\boldsymbol{x} \\boldsymbol{w_{j}}^{T} \\boldsymbol{x}) - 0\\\\ & & = & \\boldsymbol{w_{j}}^{T} \\mathbb{E}(\\boldsymbol{x} \\boldsymbol{x}) \\boldsymbol{w_{j}}^{T}\\\\ & & = & \\boldsymbol{w_{j}}^{T} \\Sigma \\boldsymbol{w_{j}}^{T} \\\\ & & = & \\boldsymbol{e_{i}}^T U^T U \\Lambda U^T U \\boldsymbol{e_{j}} \\\\ & & = & 0 \\end{aligned} $$ Fraction of variance explained: $$ \\frac{\\sum_{i}^{k}lambda_{i}}{\\sum_{i}^{d}lambda_{i}} $$ how much variability in data is captured by the first k principal components. Simultaneous Approach $$ \\begin{aligned} & \\text{maximise} & & \\sum_{i=1}^{k}\\boldsymbol{w_{i}}^T \\Sigma \\boldsymbol{w_{i}} \\\\ & \\text{subject to} & & ||\\boldsymbol{w_{i}} = 1|| & & i = 1, 2, \\dots, m-1\\\\ & & & \\boldsymbol{w_{i}}^{T}\\boldsymbol{w_{j}} = 0 & & i \\neq j \\end{aligned} $$ Subtle technical point: the sequential approach corresponds to solving this optimisation problem in greedy manner(algorithm), which doesn’t guarantee to yield optimal solution. However, sequential approach and simultaneous yield same results. PCA by Minimisation of Approximation Error","link":"/2019/05/10/DME-Data-Mining-and-Exploration-Revision/"},{"title":"markdown example","text":"Advertisement :) pica - high quality and fast imageresize in browser. babelfish - developer friendlyi18n with plurals support and easy syntax. You will like those projects! h1 Heading 8-)h2 Headingh3 Headingh4 Headingh5 Headingh6 Heading Horizontal Rules Typographic replacementsEnable typographer option to see result. (c) (C) (r) (R) (tm) (TM) (p) (P) +- test.. test… test….. test?….. test!…. !!!!!! ???? ,, – — “Smartypants, double quotes” and ‘single quotes’ EmphasisThis is bold text This is bold text This is italic text This is italic text Strikethrough Blockquotes Blockquotes can also be nested… …by using additional greater-than signs right next to each other… …or with spaces between arrows. ListsUnordered Create a list by starting a line with +, -, or * Sub-lists are made by indenting 2 spaces: Marker character change forces new list start: Ac tristique libero volutpat at Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Very easy! Ordered Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential numbers… …or keep all the numbers as 1. Start numbering with offset: foo bar CodeInline code Indented code // Some comments line 1 of code line 2 of code line 3 of code Block code “fences” 1Sample text here... Syntax highlighting 12345var foo = function (bar) { return bar++;};console.log(foo(5)); Tables Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Linkslink text link with title Autoconverted link https://github.com/nodeca/pica (enable linkify to see) Images Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location: PluginsThe killer feature of markdown-it is very effective support ofsyntax plugins. Emojies Classic markup: :wink: :crush: :cry: :tear: :laughing: :yum: Shortcuts (emoticons): :-) :-( 8-) ;) see how to change output with twemoji. Subscript / Superscript 19^th^ H~2~O \\++Inserted text++ \\==Marked text== FootnotesFootnote 1 link[^first]. Footnote 2 link[^second]. Inline footnote^[Text of inline footnote] definition. Duplicated footnote reference[^second]. [^first]: Footnote can have markup and multiple paragraphs. [^second]: Footnote text. Definition listsTerm 1 : Definition 1with lazy continuation. Term 2 with inline markup : Definition 2 { some code, part of Definition 2 } Third paragraph of definition 2. Compact style: Term 1 ~ Definition 1 Term 2 ~ Definition 2a ~ Definition 2b AbbreviationsThis is HTML abbreviation example. It converts “HTML”, but keep intact partial entries like “xxxHTMLyyy” and so on. *[HTML]: Hyper Text Markup Language Custom containers::: warninghere be dragons:::","link":"/2019/05/11/markdown-example/"}],"tags":[{"name":"CoursesRevision","slug":"CoursesRevision","link":"/tags/CoursesRevision/"}],"categories":[{"name":"University of Edinburgh","slug":"University-of-Edinburgh","link":"/categories/University-of-Edinburgh/"}]}